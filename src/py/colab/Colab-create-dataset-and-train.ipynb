{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Colab-create-dataset-and-train.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koukyo1994/iOS-note-v2/blob/master/src/py/colab/Colab-create-dataset-and-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpTI0ZUNM0t_",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB3gkaOoM0uB",
        "colab_type": "code",
        "outputId": "102e0671-b153-4abb-fdad-59d1484e12c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%sh\n",
        "apt -qq -y update >> /dev/null\n",
        "apt -qq -y install fonts-ipafont wamerican >> /dev/null\n",
        "pip install tensorflow-gpu==2.0.0 imgaug==0.2.6 coremltools==3.1 >> /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "ERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.2 which is incompatible.\n",
            "ERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\n",
            "ERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\n",
            "ERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpTZf0ZeM0uF",
        "colab_type": "code",
        "outputId": "9342d8b7-c513-45f2-c82e-b46009badaca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/koukyo1994/iOS-note-v2.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'iOS-note-v2'...\n",
            "remote: Enumerating objects: 302, done.\u001b[K\n",
            "remote: Counting objects: 100% (302/302), done.\u001b[K\n",
            "remote: Compressing objects: 100% (235/235), done.\u001b[K\n",
            "remote: Total 302 (delta 132), reused 216 (delta 61), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (302/302), 11.78 MiB | 7.58 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cldi5yNjcCKf",
        "colab_type": "text"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klFTCJwfM0uI",
        "colab_type": "code",
        "outputId": "90081497-ffe2-402e-d60d-2b3e59f52bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "%cd /content/iOS-note-v2/src/py\n",
        "!make create-dataset NSAMPLES=20000"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/iOS-note-v2/src/py\n",
            "chmod +x setup.sh\n",
            "make font-setup\n",
            "make[1]: Entering directory '/content/iOS-note-v2/src/py'\n",
            "./setup.sh\n",
            "alanis-hand\n",
            "architext\n",
            "ashcan-bb\n",
            "./setup.sh: 7: [: ashcanbb_bold.ttf: unexpected operator\n",
            "attack-of-the-cucumbers\n",
            "./setup.sh: 7: [: attack: unexpected operator\n",
            "attract-more-women\n",
            "blzee\n",
            "calligravity\n",
            "domestic-manners\n",
            "FH-GoodDogPlain-WTT\n",
            "james-almacen\n",
            "james-fajardo\n",
            "./setup.sh: 7: [: James: unexpected operator\n",
            "khand\n",
            "ladylike-bb\n",
            "mulders-handwriting\n",
            "mumsies\n",
            "Otto\n",
            "pecita\n",
            "quikhand\n",
            "Sophia\n",
            "two-turtle-doves\n",
            "make[1]: Leaving directory '/content/iOS-note-v2/src/py'\n",
            "python create_dataset.py --n_samples 20000\n",
            "100% 20000/20000 [08:23<00:00, 39.69it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muqkdWAPcFYN",
        "colab_type": "text"
      },
      "source": [
        "## Save Dataset in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTQ5ZsJwM0uK",
        "colab_type": "code",
        "outputId": "e517b28f-fa1c-4ab9-bb71-9faa3931da4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sL6bYdfM0uM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r data/images /content/gdrive/My\\ Drive/AppliedML/\n",
        "!cp data/labels.csv /content/gdrive/My\\ Drive/AppliedML/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juvw9y15cJbr",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKAcedl7UfFE",
        "colab_type": "code",
        "outputId": "6d454138-fca1-4757-9b1e-55dbea0fb2f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --config config/resnet.2blocks.json"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Step 10 : Loss : 59.6284866\n",
            "Step 20 : Loss : 48.975502\n",
            "Step 30 : Loss : 43.4608192\n",
            "Step 40 : Loss : 40.2984\n",
            "Step 50 : Loss : 38.1255035\n",
            "Step 60 : Loss : 36.6866074\n",
            "Step 70 : Loss : 35.5437469\n",
            "Step 80 : Loss : 34.516098\n",
            "Step 90 : Loss : 33.7528801\n",
            "Step 100 : Loss : 33.1648369\n",
            "Step 110 : Loss : 32.5995216\n",
            "Step 120 : Loss : 32.1297455\n",
            "Step 130 : Loss : 31.5976486\n",
            "Step 140 : Loss : 31.2156982\n",
            "Step 150 : Loss : 30.8617115\n",
            "Step 160 : Loss : 30.5397224\n",
            "Step 170 : Loss : 30.2848835\n",
            "Step 180 : Loss : 30.0599098\n",
            "Step 190 : Loss : 29.8216095\n",
            "Step 200 : Loss : 29.590107\n",
            "Step 210 : Loss : 29.3774014\n",
            "Step 220 : Loss : 29.1248798\n",
            "Step 230 : Loss : 28.9601154\n",
            "Step 240 : Loss : 28.817131\n",
            "Step 250 : Loss : 28.6118469\n",
            "Step 260 : Loss : 28.4574852\n",
            "Step 270 : Loss : 28.2847137\n",
            "Step 280 : Loss : 28.1406956\n",
            "Step 290 : Loss : 27.9867344\n",
            "Step 300 : Loss : 27.8429565\n",
            "Step 310 : Loss : 27.702404\n",
            "Step 320 : Loss : 27.5794125\n",
            "Step 330 : Loss : 27.445528\n",
            "Step 340 : Loss : 27.3295155\n",
            "Step 350 : Loss : 27.2167797\n",
            "Step 360 : Loss : 27.0995464\n",
            "Step 370 : Loss : 26.946722\n",
            "Step 380 : Loss : 26.8283329\n",
            "Step 390 : Loss : 26.7341118\n",
            "Step 400 : Loss : 26.6298771\n",
            "Step 410 : Loss : 26.5291462\n",
            "Step 420 : Loss : 26.4179707\n",
            "Step 430 : Loss : 26.3177032\n",
            "Step 440 : Loss : 26.1958656\n",
            "Step 450 : Loss : 26.0795269\n",
            "Step 460 : Loss : 25.9666958\n",
            "Step 470 : Loss : 25.8568897\n",
            "Step 480 : Loss : 25.7481766\n",
            "Step 490 : Loss : 25.6462231\n",
            "Step 500 : Loss : 25.5251637\n",
            "Step 510 : Loss : 25.4145908\n",
            "Step 520 : Loss : 25.3027821\n",
            "Step 530 : Loss : 25.1968231\n",
            "Step 540 : Loss : 25.073679\n",
            "Step 550 : Loss : 24.9696827\n",
            "Step 560 : Loss : 24.8479233\n",
            "Step 570 : Loss : 24.7394295\n",
            "Step 580 : Loss : 24.6236744\n",
            "Step 590 : Loss : 24.5186234\n",
            "Step 590 : Loss :  24.5186234\n",
            "Save weights at epoch: 0\n",
            "Epoch 2\n",
            "Step 600 : Loss : 24.4024086\n",
            "Step 610 : Loss : 24.2897034\n",
            "Step 620 : Loss : 24.1762772\n",
            "Step 630 : Loss : 24.0592957\n",
            "Step 640 : Loss : 23.9492092\n",
            "Step 650 : Loss : 23.8474026\n",
            "Step 660 : Loss : 23.7431412\n",
            "Step 670 : Loss : 23.6257191\n",
            "Step 680 : Loss : 23.5176449\n",
            "Step 690 : Loss : 23.4049625\n",
            "Step 700 : Loss : 23.3081894\n",
            "Step 710 : Loss : 23.20397\n",
            "Step 720 : Loss : 23.0867081\n",
            "Step 730 : Loss : 22.9793282\n",
            "Step 740 : Loss : 22.8726463\n",
            "Step 750 : Loss : 22.7623463\n",
            "Step 760 : Loss : 22.6641045\n",
            "Step 770 : Loss : 22.5652828\n",
            "Step 780 : Loss : 22.462162\n",
            "Step 790 : Loss : 22.3582077\n",
            "Step 800 : Loss : 22.2656746\n",
            "Step 810 : Loss : 22.1564655\n",
            "Step 820 : Loss : 22.0557766\n",
            "Step 830 : Loss : 21.9629688\n",
            "Step 840 : Loss : 21.8581982\n",
            "Step 850 : Loss : 21.7709599\n",
            "Step 860 : Loss : 21.6790085\n",
            "Step 870 : Loss : 21.5810585\n",
            "Step 880 : Loss : 21.4842758\n",
            "Step 890 : Loss : 21.3839054\n",
            "Step 900 : Loss : 21.2914772\n",
            "Step 910 : Loss : 21.1929302\n",
            "Step 920 : Loss : 21.0967541\n",
            "Step 930 : Loss : 21.0077438\n",
            "Step 940 : Loss : 20.924181\n",
            "Step 950 : Loss : 20.8408546\n",
            "Step 960 : Loss : 20.7478676\n",
            "Step 970 : Loss : 20.659\n",
            "Step 980 : Loss : 20.5836639\n",
            "Step 990 : Loss : 20.4988174\n",
            "Step 1000 : Loss : 20.4197807\n",
            "Step 1010 : Loss : 20.3395901\n",
            "Step 1020 : Loss : 20.2599106\n",
            "Step 1030 : Loss : 20.1720219\n",
            "Step 1040 : Loss : 20.0857258\n",
            "Step 1050 : Loss : 20.0005703\n",
            "Step 1060 : Loss : 19.9160843\n",
            "Step 1070 : Loss : 19.8394432\n",
            "Step 1080 : Loss : 19.7633286\n",
            "Step 1090 : Loss : 19.6860809\n",
            "Step 1100 : Loss : 19.6150227\n",
            "Step 1110 : Loss : 19.5392799\n",
            "Step 1120 : Loss : 19.4654\n",
            "Step 1130 : Loss : 19.3896427\n",
            "Step 1140 : Loss : 19.3228302\n",
            "Step 1150 : Loss : 19.2462\n",
            "Step 1160 : Loss : 19.1732616\n",
            "Step 1170 : Loss : 19.0976353\n",
            "Step 1180 : Loss : 19.0274544\n",
            "Step 1180 : Loss :  19.0274544\n",
            "Save weights at epoch: 1\n",
            "Epoch 3\n",
            "Step 1190 : Loss : 18.9559345\n",
            "Step 1200 : Loss : 18.8832321\n",
            "Step 1210 : Loss : 18.8147869\n",
            "Step 1220 : Loss : 18.7406368\n",
            "Step 1230 : Loss : 18.6729317\n",
            "Step 1240 : Loss : 18.6071377\n",
            "Step 1250 : Loss : 18.5414467\n",
            "Step 1260 : Loss : 18.4732857\n",
            "Step 1270 : Loss : 18.4086342\n",
            "Step 1280 : Loss : 18.3392525\n",
            "Step 1290 : Loss : 18.2806072\n",
            "Step 1300 : Loss : 18.2144318\n",
            "Step 1310 : Loss : 18.1430264\n",
            "Step 1320 : Loss : 18.0763474\n",
            "Step 1330 : Loss : 18.0116673\n",
            "Step 1340 : Loss : 17.9455814\n",
            "Step 1350 : Loss : 17.8846893\n",
            "Step 1360 : Loss : 17.8249359\n",
            "Step 1370 : Loss : 17.7620201\n",
            "Step 1380 : Loss : 17.6988373\n",
            "Step 1390 : Loss : 17.6420727\n",
            "Step 1400 : Loss : 17.5787354\n",
            "Step 1410 : Loss : 17.5183525\n",
            "Step 1420 : Loss : 17.4606876\n",
            "Step 1430 : Loss : 17.3982735\n",
            "Step 1440 : Loss : 17.3429909\n",
            "Step 1450 : Loss : 17.2881966\n",
            "Step 1460 : Loss : 17.2292614\n",
            "Step 1470 : Loss : 17.1720486\n",
            "Step 1480 : Loss : 17.1111946\n",
            "Step 1490 : Loss : 17.0555096\n",
            "Step 1500 : Loss : 16.9952755\n",
            "Step 1510 : Loss : 16.9364834\n",
            "Step 1520 : Loss : 16.8821926\n",
            "Step 1530 : Loss : 16.8293533\n",
            "Step 1540 : Loss : 16.7777576\n",
            "Step 1550 : Loss : 16.7218037\n",
            "Step 1560 : Loss : 16.6677551\n",
            "Step 1570 : Loss : 16.6205883\n",
            "Step 1580 : Loss : 16.5686207\n",
            "Step 1590 : Loss : 16.5201721\n",
            "Step 1600 : Loss : 16.4696369\n",
            "Step 1610 : Loss : 16.4193153\n",
            "Step 1620 : Loss : 16.3652306\n",
            "Step 1630 : Loss : 16.3118553\n",
            "Step 1640 : Loss : 16.2603683\n",
            "Step 1650 : Loss : 16.2079544\n",
            "Step 1660 : Loss : 16.1603432\n",
            "Step 1670 : Loss : 16.111927\n",
            "Step 1680 : Loss : 16.0640564\n",
            "Step 1690 : Loss : 16.0198841\n",
            "Step 1700 : Loss : 15.9732113\n",
            "Step 1710 : Loss : 15.9267454\n",
            "Step 1720 : Loss : 15.8792753\n",
            "Step 1730 : Loss : 15.8365555\n",
            "Step 1740 : Loss : 15.7884521\n",
            "Step 1750 : Loss : 15.7414932\n",
            "Step 1760 : Loss : 15.6934214\n",
            "Step 1770 : Loss : 15.648283\n",
            "Step 1770 : Loss :  15.648283\n",
            "Save weights at epoch: 2\n",
            "Epoch 4\n",
            "Step 1780 : Loss : 15.6020737\n",
            "Step 1790 : Loss : 15.555213\n",
            "Step 1800 : Loss : 15.5114689\n",
            "Step 1810 : Loss : 15.4637804\n",
            "Step 1820 : Loss : 15.4203815\n",
            "Step 1830 : Loss : 15.3778839\n",
            "Step 1840 : Loss : 15.3357706\n",
            "Step 1850 : Loss : 15.2922668\n",
            "Step 1860 : Loss : 15.2509632\n",
            "Step 1870 : Loss : 15.2070522\n",
            "Step 1880 : Loss : 15.1694212\n",
            "Step 1890 : Loss : 15.1264114\n",
            "Step 1900 : Loss : 15.0802498\n",
            "Step 1910 : Loss : 15.0372257\n",
            "Step 1920 : Loss : 14.9946842\n",
            "Step 1930 : Loss : 14.9514112\n",
            "Step 1940 : Loss : 14.9115057\n",
            "Step 1950 : Loss : 14.8725901\n",
            "Step 1960 : Loss : 14.8312387\n",
            "Step 1970 : Loss : 14.7895899\n",
            "Step 1980 : Loss : 14.7516937\n",
            "Step 1990 : Loss : 14.7104731\n",
            "Step 2000 : Loss : 14.6703444\n",
            "Step 2010 : Loss : 14.6318874\n",
            "Step 2020 : Loss : 14.5899897\n",
            "Step 2030 : Loss : 14.5531931\n",
            "Step 2040 : Loss : 14.517107\n",
            "Step 2050 : Loss : 14.4784298\n",
            "Step 2060 : Loss : 14.4404364\n",
            "Step 2070 : Loss : 14.3997269\n",
            "Step 2080 : Loss : 14.3627777\n",
            "Step 2090 : Loss : 14.322669\n",
            "Step 2100 : Loss : 14.2827988\n",
            "Step 2110 : Loss : 14.2464762\n",
            "Step 2120 : Loss : 14.2099752\n",
            "Step 2130 : Loss : 14.1744747\n",
            "Step 2140 : Loss : 14.136342\n",
            "Step 2150 : Loss : 14.0993919\n",
            "Step 2160 : Loss : 14.0665045\n",
            "Step 2170 : Loss : 14.0309982\n",
            "Step 2180 : Loss : 13.9984179\n",
            "Step 2190 : Loss : 13.963294\n",
            "Step 2200 : Loss : 13.9286737\n",
            "Step 2210 : Loss : 13.8917103\n",
            "Step 2220 : Loss : 13.8551064\n",
            "Step 2230 : Loss : 13.819418\n",
            "Step 2240 : Loss : 13.7832766\n",
            "Step 2250 : Loss : 13.7501125\n",
            "Step 2260 : Loss : 13.7163601\n",
            "Step 2270 : Loss : 13.6829681\n",
            "Step 2280 : Loss : 13.6521139\n",
            "Step 2290 : Loss : 13.619875\n",
            "Step 2300 : Loss : 13.587431\n",
            "Step 2310 : Loss : 13.5539465\n",
            "Step 2320 : Loss : 13.5239248\n",
            "Step 2330 : Loss : 13.4905701\n",
            "Step 2340 : Loss : 13.4573698\n",
            "Step 2350 : Loss : 13.4235363\n",
            "Step 2360 : Loss : 13.3913765\n",
            "Step 2360 : Loss :  13.3913765\n",
            "Save weights at epoch: 3\n",
            "Epoch 5\n",
            "Step 2370 : Loss : 13.3578215\n",
            "Step 2380 : Loss : 13.3246393\n",
            "Step 2390 : Loss : 13.2931156\n",
            "Step 2400 : Loss : 13.2593527\n",
            "Step 2410 : Loss : 13.2283897\n",
            "Step 2420 : Loss : 13.1980572\n",
            "Step 2430 : Loss : 13.1679735\n",
            "Step 2440 : Loss : 13.1368771\n",
            "Step 2450 : Loss : 13.1071672\n",
            "Step 2460 : Loss : 13.0760641\n",
            "Step 2470 : Loss : 13.0488586\n",
            "Step 2480 : Loss : 13.0178881\n",
            "Step 2490 : Loss : 12.9847\n",
            "Step 2500 : Loss : 12.9539137\n",
            "Step 2510 : Loss : 12.9230566\n",
            "Step 2520 : Loss : 12.8919277\n",
            "Step 2530 : Loss : 12.8628588\n",
            "Step 2540 : Loss : 12.8346338\n",
            "Step 2550 : Loss : 12.8047705\n",
            "Step 2560 : Loss : 12.7747068\n",
            "Step 2570 : Loss : 12.7469082\n",
            "Step 2580 : Loss : 12.7168503\n",
            "Step 2590 : Loss : 12.6876602\n",
            "Step 2600 : Loss : 12.6591825\n",
            "Step 2610 : Loss : 12.6283703\n",
            "Step 2620 : Loss : 12.6013699\n",
            "Step 2630 : Loss : 12.5751381\n",
            "Step 2640 : Loss : 12.5469847\n",
            "Step 2650 : Loss : 12.5192614\n",
            "Step 2660 : Loss : 12.489336\n",
            "Step 2670 : Loss : 12.4621229\n",
            "Step 2680 : Loss : 12.4327545\n",
            "Step 2690 : Loss : 12.4031334\n",
            "Step 2700 : Loss : 12.376503\n",
            "Step 2710 : Loss : 12.3491468\n",
            "Step 2720 : Loss : 12.3225813\n",
            "Step 2730 : Loss : 12.2939205\n",
            "Step 2740 : Loss : 12.2662754\n",
            "Step 2750 : Loss : 12.2415314\n",
            "Step 2760 : Loss : 12.2151804\n",
            "Step 2770 : Loss : 12.1909819\n",
            "Step 2780 : Loss : 12.1643324\n",
            "Step 2790 : Loss : 12.1382742\n",
            "Step 2800 : Loss : 12.1107159\n",
            "Step 2810 : Loss : 12.0831528\n",
            "Step 2820 : Loss : 12.0561638\n",
            "Step 2830 : Loss : 12.0289583\n",
            "Step 2840 : Loss : 12.0037622\n",
            "Step 2850 : Loss : 11.9779081\n",
            "Step 2860 : Loss : 11.9524612\n",
            "Step 2870 : Loss : 11.9289732\n",
            "Step 2880 : Loss : 11.9045019\n",
            "Step 2890 : Loss : 11.8796377\n",
            "Step 2900 : Loss : 11.8539143\n",
            "Step 2910 : Loss : 11.8309708\n",
            "Step 2920 : Loss : 11.8057213\n",
            "Step 2930 : Loss : 11.7804298\n",
            "Step 2940 : Loss : 11.7546806\n",
            "Step 2950 : Loss : 11.7300158\n",
            "Step 2950 : Loss :  11.7300158\n",
            "Save weights at epoch: 4\n",
            "Epoch 6\n",
            "Step 2960 : Loss : 11.7038574\n",
            "Step 2970 : Loss : 11.6782923\n",
            "Step 2980 : Loss : 11.6537638\n",
            "Step 2990 : Loss : 11.6278582\n",
            "Step 3000 : Loss : 11.6038618\n",
            "Step 3010 : Loss : 11.5804386\n",
            "Step 3020 : Loss : 11.5569391\n",
            "Step 3030 : Loss : 11.5327892\n",
            "Step 3040 : Loss : 11.5096331\n",
            "Step 3050 : Loss : 11.4857464\n",
            "Step 3060 : Loss : 11.4643793\n",
            "Step 3070 : Loss : 11.440362\n",
            "Step 3080 : Loss : 11.4149656\n",
            "Step 3090 : Loss : 11.3912783\n",
            "Step 3100 : Loss : 11.3673038\n",
            "Step 3110 : Loss : 11.3432617\n",
            "Step 3120 : Loss : 11.3206339\n",
            "Step 3130 : Loss : 11.2983418\n",
            "Step 3140 : Loss : 11.2753487\n",
            "Step 3150 : Loss : 11.2519722\n",
            "Step 3160 : Loss : 11.2298965\n",
            "Step 3170 : Loss : 11.2063513\n",
            "Step 3180 : Loss : 11.1838112\n",
            "Step 3190 : Loss : 11.1612196\n",
            "Step 3200 : Loss : 11.1368837\n",
            "Step 3210 : Loss : 11.1154852\n",
            "Step 3220 : Loss : 11.094696\n",
            "Step 3230 : Loss : 11.0723476\n",
            "Step 3240 : Loss : 11.0506706\n",
            "Step 3250 : Loss : 11.0273743\n",
            "Step 3260 : Loss : 11.0058136\n",
            "Step 3270 : Loss : 10.9828615\n",
            "Step 3280 : Loss : 10.9595442\n",
            "Step 3290 : Loss : 10.9384699\n",
            "Step 3300 : Loss : 10.9166346\n",
            "Step 3310 : Loss : 10.8953066\n",
            "Step 3320 : Loss : 10.8723068\n",
            "Step 3330 : Loss : 10.8502264\n",
            "Step 3340 : Loss : 10.8303795\n",
            "Step 3350 : Loss : 10.8093309\n",
            "Step 3360 : Loss : 10.7899971\n",
            "Step 3370 : Loss : 10.7685232\n",
            "Step 3380 : Loss : 10.747592\n",
            "Step 3390 : Loss : 10.7257347\n",
            "Step 3400 : Loss : 10.7037268\n",
            "Step 3410 : Loss : 10.6820908\n",
            "Step 3420 : Loss : 10.6603365\n",
            "Step 3430 : Loss : 10.6399345\n",
            "Step 3440 : Loss : 10.6190643\n",
            "Step 3450 : Loss : 10.5984421\n",
            "Step 3460 : Loss : 10.5791216\n",
            "Step 3470 : Loss : 10.5593348\n",
            "Step 3480 : Loss : 10.53897\n",
            "Step 3490 : Loss : 10.5181684\n",
            "Step 3500 : Loss : 10.499402\n",
            "Step 3510 : Loss : 10.4790077\n",
            "Step 3520 : Loss : 10.4586601\n",
            "Step 3530 : Loss : 10.4379816\n",
            "Step 3540 : Loss : 10.4180994\n",
            "Step 3540 : Loss :  10.4180994\n",
            "Save weights at epoch: 5\n",
            "Epoch 7\n",
            "Step 3550 : Loss : 10.3967905\n",
            "Step 3560 : Loss : 10.3757973\n",
            "Step 3570 : Loss : 10.3558283\n",
            "Step 3580 : Loss : 10.334796\n",
            "Step 3590 : Loss : 10.3152714\n",
            "Step 3600 : Loss : 10.2960186\n",
            "Step 3610 : Loss : 10.2766523\n",
            "Step 3620 : Loss : 10.2567863\n",
            "Step 3630 : Loss : 10.2375498\n",
            "Step 3640 : Loss : 10.2179689\n",
            "Step 3650 : Loss : 10.2000856\n",
            "Step 3660 : Loss : 10.1805553\n",
            "Step 3670 : Loss : 10.1600103\n",
            "Step 3680 : Loss : 10.1405573\n",
            "Step 3690 : Loss : 10.1210194\n",
            "Step 3700 : Loss : 10.101428\n",
            "Step 3710 : Loss : 10.0827065\n",
            "Step 3720 : Loss : 10.0642052\n",
            "Step 3730 : Loss : 10.0453634\n",
            "Step 3740 : Loss : 10.0262022\n",
            "Step 3750 : Loss : 10.0077171\n",
            "Step 3760 : Loss : 9.98837376\n",
            "Step 3770 : Loss : 9.96999454\n",
            "Step 3780 : Loss : 9.95093155\n",
            "Step 3790 : Loss : 9.93079\n",
            "Step 3800 : Loss : 9.91287804\n",
            "Step 3810 : Loss : 9.89551258\n",
            "Step 3820 : Loss : 9.87697\n",
            "Step 3830 : Loss : 9.85906\n",
            "Step 3840 : Loss : 9.84011364\n",
            "Step 3850 : Loss : 9.82226753\n",
            "Step 3860 : Loss : 9.80353451\n",
            "Step 3870 : Loss : 9.78434\n",
            "Step 3880 : Loss : 9.7668066\n",
            "Step 3890 : Loss : 9.74869633\n",
            "Step 3900 : Loss : 9.73070145\n",
            "Step 3910 : Loss : 9.71158791\n",
            "Step 3920 : Loss : 9.69326591\n",
            "Step 3930 : Loss : 9.67641544\n",
            "Step 3940 : Loss : 9.65881729\n",
            "Step 3950 : Loss : 9.64251709\n",
            "Step 3960 : Loss : 9.62454224\n",
            "Step 3970 : Loss : 9.60709095\n",
            "Step 3980 : Loss : 9.58899498\n",
            "Step 3990 : Loss : 9.57063866\n",
            "Step 4000 : Loss : 9.5525856\n",
            "Step 4010 : Loss : 9.53454208\n",
            "Step 4020 : Loss : 9.51729298\n",
            "Step 4030 : Loss : 9.49988174\n",
            "Step 4040 : Loss : 9.48253727\n",
            "Step 4050 : Loss : 9.4661932\n",
            "Step 4060 : Loss : 9.44948196\n",
            "Step 4070 : Loss : 9.43218136\n",
            "Step 4080 : Loss : 9.41469288\n",
            "Step 4090 : Loss : 9.39878559\n",
            "Step 4100 : Loss : 9.38158512\n",
            "Step 4110 : Loss : 9.36447811\n",
            "Step 4120 : Loss : 9.34719372\n",
            "Step 4130 : Loss : 9.33056\n",
            "Step 4130 : Loss :  9.33056\n",
            "Save weights at epoch: 6\n",
            "Epoch 8\n",
            "Step 4140 : Loss : 9.31290531\n",
            "Step 4150 : Loss : 9.29524517\n",
            "Step 4160 : Loss : 9.27824497\n",
            "Step 4170 : Loss : 9.26058\n",
            "Step 4180 : Loss : 9.24412632\n",
            "Step 4190 : Loss : 9.22762203\n",
            "Step 4200 : Loss : 9.21117\n",
            "Step 4210 : Loss : 9.19432354\n",
            "Step 4220 : Loss : 9.17789936\n",
            "Step 4230 : Loss : 9.16117477\n",
            "Step 4240 : Loss : 9.14565372\n",
            "Step 4250 : Loss : 9.12914658\n",
            "Step 4260 : Loss : 9.11194801\n",
            "Step 4270 : Loss : 9.09539223\n",
            "Step 4280 : Loss : 9.07896137\n",
            "Step 4290 : Loss : 9.06240749\n",
            "Step 4300 : Loss : 9.04657364\n",
            "Step 4310 : Loss : 9.03082752\n",
            "Step 4320 : Loss : 9.01494884\n",
            "Step 4330 : Loss : 8.99867249\n",
            "Step 4340 : Loss : 8.98281288\n",
            "Step 4350 : Loss : 8.96645069\n",
            "Step 4360 : Loss : 8.95085049\n",
            "Step 4370 : Loss : 8.93448925\n",
            "Step 4380 : Loss : 8.91761684\n",
            "Step 4390 : Loss : 8.90227604\n",
            "Step 4400 : Loss : 8.88720703\n",
            "Step 4410 : Loss : 8.87138653\n",
            "Step 4420 : Loss : 8.85610771\n",
            "Step 4430 : Loss : 8.84017\n",
            "Step 4440 : Loss : 8.82499\n",
            "Step 4450 : Loss : 8.80909348\n",
            "Step 4460 : Loss : 8.7928648\n",
            "Step 4470 : Loss : 8.77785683\n",
            "Step 4480 : Loss : 8.7624588\n",
            "Step 4490 : Loss : 8.74716663\n",
            "Step 4500 : Loss : 8.73113346\n",
            "Step 4510 : Loss : 8.71561432\n",
            "Step 4520 : Loss : 8.70093727\n",
            "Step 4530 : Loss : 8.68590069\n",
            "Step 4540 : Loss : 8.67165661\n",
            "Step 4550 : Loss : 8.65629196\n",
            "Step 4560 : Loss : 8.6414\n",
            "Step 4570 : Loss : 8.62614822\n",
            "Step 4580 : Loss : 8.61085701\n",
            "Step 4590 : Loss : 8.59569168\n",
            "Step 4600 : Loss : 8.58053684\n",
            "Step 4610 : Loss : 8.5656395\n",
            "Step 4620 : Loss : 8.55072212\n",
            "Step 4630 : Loss : 8.5357523\n",
            "Step 4640 : Loss : 8.52158165\n",
            "Step 4650 : Loss : 8.50722\n",
            "Step 4660 : Loss : 8.49243259\n",
            "Step 4670 : Loss : 8.47749138\n",
            "Step 4680 : Loss : 8.46376133\n",
            "Step 4690 : Loss : 8.44913483\n",
            "Step 4700 : Loss : 8.43467903\n",
            "Step 4710 : Loss : 8.42007542\n",
            "Step 4720 : Loss : 8.40591526\n",
            "Step 4720 : Loss :  8.40591526\n",
            "Save weights at epoch: 7\n",
            "Epoch 9\n",
            "Step 4730 : Loss : 8.39119244\n",
            "Step 4740 : Loss : 8.37629318\n",
            "Step 4750 : Loss : 8.36169529\n",
            "Step 4760 : Loss : 8.34660721\n",
            "Step 4770 : Loss : 8.33245277\n",
            "Step 4780 : Loss : 8.31827641\n",
            "Step 4790 : Loss : 8.30417728\n",
            "Step 4800 : Loss : 8.28983402\n",
            "Step 4810 : Loss : 8.2756567\n",
            "Step 4820 : Loss : 8.26130104\n",
            "Step 4830 : Loss : 8.24782562\n",
            "Step 4840 : Loss : 8.23382187\n",
            "Step 4850 : Loss : 8.21936321\n",
            "Step 4860 : Loss : 8.2050705\n",
            "Step 4870 : Loss : 8.19113159\n",
            "Step 4880 : Loss : 8.17708111\n",
            "Step 4890 : Loss : 8.16363525\n",
            "Step 4900 : Loss : 8.15014553\n",
            "Step 4910 : Loss : 8.13674164\n",
            "Step 4920 : Loss : 8.12288475\n",
            "Step 4930 : Loss : 8.10926247\n",
            "Step 4940 : Loss : 8.09535599\n",
            "Step 4950 : Loss : 8.08198929\n",
            "Step 4960 : Loss : 8.06807709\n",
            "Step 4970 : Loss : 8.05398369\n",
            "Step 4980 : Loss : 8.04068947\n",
            "Step 4990 : Loss : 8.02745819\n",
            "Step 5000 : Loss : 8.01395\n",
            "Step 5010 : Loss : 8.00083447\n",
            "Step 5020 : Loss : 7.98722\n",
            "Step 5030 : Loss : 7.97404623\n",
            "Step 5040 : Loss : 7.96050882\n",
            "Step 5050 : Loss : 7.9467845\n",
            "Step 5060 : Loss : 7.93378973\n",
            "Step 5070 : Loss : 7.92057896\n",
            "Step 5080 : Loss : 7.90742\n",
            "Step 5090 : Loss : 7.89390516\n",
            "Step 5100 : Loss : 7.8808403\n",
            "Step 5110 : Loss : 7.86823797\n",
            "Step 5120 : Loss : 7.85539532\n",
            "Step 5130 : Loss : 7.84293\n",
            "Step 5140 : Loss : 7.82988071\n",
            "Step 5150 : Loss : 7.81715393\n",
            "Step 5160 : Loss : 7.80419779\n",
            "Step 5170 : Loss : 7.79129457\n",
            "Step 5180 : Loss : 7.77860546\n",
            "Step 5190 : Loss : 7.76621151\n",
            "Step 5200 : Loss : 7.75377941\n",
            "Step 5210 : Loss : 7.74112368\n",
            "Step 5220 : Loss : 7.72832918\n",
            "Step 5230 : Loss : 7.71621227\n",
            "Step 5240 : Loss : 7.7039361\n",
            "Step 5250 : Loss : 7.69156694\n",
            "Step 5260 : Loss : 7.67898512\n",
            "Step 5270 : Loss : 7.66727781\n",
            "Step 5280 : Loss : 7.65493965\n",
            "Step 5290 : Loss : 7.64271641\n",
            "Step 5300 : Loss : 7.6303072\n",
            "Step 5310 : Loss : 7.61822748\n",
            "Step 5310 : Loss :  7.61822748\n",
            "Save weights at epoch: 8\n",
            "Epoch 10\n",
            "Step 5320 : Loss : 7.60605764\n",
            "Step 5330 : Loss : 7.59365463\n",
            "Step 5340 : Loss : 7.58142138\n",
            "Step 5350 : Loss : 7.56886482\n",
            "Step 5360 : Loss : 7.55689144\n",
            "Step 5370 : Loss : 7.54487085\n",
            "Step 5380 : Loss : 7.5327611\n",
            "Step 5390 : Loss : 7.52053118\n",
            "Step 5400 : Loss : 7.50843048\n",
            "Step 5410 : Loss : 7.49618912\n",
            "Step 5420 : Loss : 7.4846\n",
            "Step 5430 : Loss : 7.47273\n",
            "Step 5440 : Loss : 7.46045208\n",
            "Step 5450 : Loss : 7.44823551\n",
            "Step 5460 : Loss : 7.43630028\n",
            "Step 5470 : Loss : 7.42446136\n",
            "Step 5480 : Loss : 7.41319513\n",
            "Step 5490 : Loss : 7.40175056\n",
            "Step 5500 : Loss : 7.39047\n",
            "Step 5510 : Loss : 7.37888765\n",
            "Step 5520 : Loss : 7.36749268\n",
            "Step 5530 : Loss : 7.35583544\n",
            "Step 5540 : Loss : 7.3443656\n",
            "Step 5550 : Loss : 7.33264303\n",
            "Step 5560 : Loss : 7.32087469\n",
            "Step 5570 : Loss : 7.30951452\n",
            "Step 5580 : Loss : 7.29826164\n",
            "Step 5590 : Loss : 7.28683424\n",
            "Step 5600 : Loss : 7.27553844\n",
            "Step 5610 : Loss : 7.26412106\n",
            "Step 5620 : Loss : 7.25289345\n",
            "Step 5630 : Loss : 7.24150562\n",
            "Step 5640 : Loss : 7.22984266\n",
            "Step 5650 : Loss : 7.21868515\n",
            "Step 5660 : Loss : 7.20739412\n",
            "Step 5670 : Loss : 7.19601727\n",
            "Step 5680 : Loss : 7.18462658\n",
            "Step 5690 : Loss : 7.17350388\n",
            "Step 5700 : Loss : 7.16270924\n",
            "Step 5710 : Loss : 7.15163422\n",
            "Step 5720 : Loss : 7.14095259\n",
            "Step 5730 : Loss : 7.13015366\n",
            "Step 5740 : Loss : 7.11966133\n",
            "Step 5750 : Loss : 7.10869884\n",
            "Step 5760 : Loss : 7.09776974\n",
            "Step 5770 : Loss : 7.08729076\n",
            "Step 5780 : Loss : 7.07704449\n",
            "Step 5790 : Loss : 7.06689119\n",
            "Step 5800 : Loss : 7.05663395\n",
            "Step 5810 : Loss : 7.04605722\n",
            "Step 5820 : Loss : 7.035748\n",
            "Step 5830 : Loss : 7.02539396\n",
            "Step 5840 : Loss : 7.01497364\n",
            "Step 5850 : Loss : 7.0044713\n",
            "Step 5860 : Loss : 6.9946804\n",
            "Step 5870 : Loss : 6.98435163\n",
            "Step 5880 : Loss : 6.97402763\n",
            "Step 5890 : Loss : 6.9637742\n",
            "Step 5900 : Loss : 6.95367765\n",
            "Step 5900 : Loss :  6.95367765\n",
            "Save weights at epoch: 9\n",
            "Epoch 11\n",
            "Step 5910 : Loss : 6.94359589\n",
            "Step 5920 : Loss : 6.93323326\n",
            "Step 5930 : Loss : 6.92306423\n",
            "Step 5940 : Loss : 6.91265535\n",
            "Step 5950 : Loss : 6.90261889\n",
            "Step 5960 : Loss : 6.89250088\n",
            "Step 5970 : Loss : 6.88230848\n",
            "Step 5980 : Loss : 6.87212467\n",
            "Step 5990 : Loss : 6.86199903\n",
            "Step 6000 : Loss : 6.85173178\n",
            "Step 6010 : Loss : 6.84183598\n",
            "Step 6020 : Loss : 6.83181953\n",
            "Step 6030 : Loss : 6.82160044\n",
            "Step 6040 : Loss : 6.81133223\n",
            "Step 6050 : Loss : 6.80125475\n",
            "Step 6060 : Loss : 6.79128742\n",
            "Step 6070 : Loss : 6.78171778\n",
            "Step 6080 : Loss : 6.77212667\n",
            "Step 6090 : Loss : 6.76254702\n",
            "Step 6100 : Loss : 6.75289726\n",
            "Step 6110 : Loss : 6.74349\n",
            "Step 6120 : Loss : 6.73368073\n",
            "Step 6130 : Loss : 6.7240696\n",
            "Step 6140 : Loss : 6.71418476\n",
            "Step 6150 : Loss : 6.70430326\n",
            "Step 6160 : Loss : 6.6946826\n",
            "Step 6170 : Loss : 6.68517637\n",
            "Step 6180 : Loss : 6.67565155\n",
            "Step 6190 : Loss : 6.66606665\n",
            "Step 6200 : Loss : 6.6564846\n",
            "Step 6210 : Loss : 6.64704609\n",
            "Step 6220 : Loss : 6.63736629\n",
            "Step 6230 : Loss : 6.62761259\n",
            "Step 6240 : Loss : 6.61815071\n",
            "Step 6250 : Loss : 6.60856676\n",
            "Step 6260 : Loss : 6.59891129\n",
            "Step 6270 : Loss : 6.58922958\n",
            "Step 6280 : Loss : 6.57970047\n",
            "Step 6290 : Loss : 6.57043\n",
            "Step 6300 : Loss : 6.56106281\n",
            "Step 6310 : Loss : 6.55193329\n",
            "Step 6320 : Loss : 6.54266453\n",
            "Step 6330 : Loss : 6.53355837\n",
            "Step 6340 : Loss : 6.5242281\n",
            "Step 6350 : Loss : 6.5149827\n",
            "Step 6360 : Loss : 6.50609\n",
            "Step 6370 : Loss : 6.49722624\n",
            "Step 6380 : Loss : 6.4883647\n",
            "Step 6390 : Loss : 6.48001623\n",
            "Step 6400 : Loss : 6.47134972\n",
            "Step 6410 : Loss : 6.46268892\n",
            "Step 6420 : Loss : 6.4539113\n",
            "Step 6430 : Loss : 6.44517803\n",
            "Step 6440 : Loss : 6.43642044\n",
            "Step 6450 : Loss : 6.42786\n",
            "Step 6460 : Loss : 6.41913033\n",
            "Step 6470 : Loss : 6.41045284\n",
            "Step 6480 : Loss : 6.40159321\n",
            "Step 6490 : Loss : 6.39277315\n",
            "Step 6490 : Loss :  6.39277315\n",
            "Save weights at epoch: 10\n",
            "Epoch 12\n",
            "Step 6500 : Loss : 6.38402462\n",
            "Step 6510 : Loss : 6.37512255\n",
            "Step 6520 : Loss : 6.36633444\n",
            "Step 6530 : Loss : 6.35742807\n",
            "Step 6540 : Loss : 6.34880733\n",
            "Step 6550 : Loss : 6.34012127\n",
            "Step 6560 : Loss : 6.33154964\n",
            "Step 6570 : Loss : 6.32291031\n",
            "Step 6580 : Loss : 6.31427193\n",
            "Step 6590 : Loss : 6.30559158\n",
            "Step 6600 : Loss : 6.2970252\n",
            "Step 6610 : Loss : 6.28846741\n",
            "Step 6620 : Loss : 6.27963448\n",
            "Step 6630 : Loss : 6.27088213\n",
            "Step 6640 : Loss : 6.26224136\n",
            "Step 6650 : Loss : 6.25362825\n",
            "Step 6660 : Loss : 6.24515629\n",
            "Step 6670 : Loss : 6.23669338\n",
            "Step 6680 : Loss : 6.2283144\n",
            "Step 6690 : Loss : 6.21994114\n",
            "Step 6700 : Loss : 6.21174717\n",
            "Step 6710 : Loss : 6.20341682\n",
            "Step 6720 : Loss : 6.19509935\n",
            "Step 6730 : Loss : 6.1867733\n",
            "Step 6740 : Loss : 6.17840242\n",
            "Step 6750 : Loss : 6.17017078\n",
            "Step 6760 : Loss : 6.16194105\n",
            "Step 6770 : Loss : 6.15372515\n",
            "Step 6780 : Loss : 6.14542818\n",
            "Step 6790 : Loss : 6.13718033\n",
            "Step 6800 : Loss : 6.12920761\n",
            "Step 6810 : Loss : 6.12098\n",
            "Step 6820 : Loss : 6.11258936\n",
            "Step 6830 : Loss : 6.10440254\n",
            "Step 6840 : Loss : 6.0961442\n",
            "Step 6850 : Loss : 6.08787298\n",
            "Step 6860 : Loss : 6.07957411\n",
            "Step 6870 : Loss : 6.07147455\n",
            "Step 6880 : Loss : 6.06347847\n",
            "Step 6890 : Loss : 6.05535364\n",
            "Step 6900 : Loss : 6.04742527\n",
            "Step 6910 : Loss : 6.03935623\n",
            "Step 6920 : Loss : 6.03142881\n",
            "Step 6930 : Loss : 6.02343178\n",
            "Step 6940 : Loss : 6.01537037\n",
            "Step 6950 : Loss : 6.00759029\n",
            "Step 6960 : Loss : 5.99971771\n",
            "Step 6970 : Loss : 5.99209404\n",
            "Step 6980 : Loss : 5.98457575\n",
            "Step 6990 : Loss : 5.97715425\n",
            "Step 7000 : Loss : 5.96980715\n",
            "Step 7010 : Loss : 5.96239185\n",
            "Step 7020 : Loss : 5.95493078\n",
            "Step 7030 : Loss : 5.94723415\n",
            "Step 7040 : Loss : 5.93988085\n",
            "Step 7050 : Loss : 5.93224049\n",
            "Step 7060 : Loss : 5.92459488\n",
            "Step 7070 : Loss : 5.91697311\n",
            "Step 7080 : Loss : 5.90927744\n",
            "Step 7080 : Loss :  5.90927744\n",
            "Save weights at epoch: 11\n",
            "Epoch 13\n",
            "Step 7090 : Loss : 5.90158224\n",
            "Step 7100 : Loss : 5.89383793\n",
            "Step 7110 : Loss : 5.88613462\n",
            "Step 7120 : Loss : 5.8783803\n",
            "Step 7130 : Loss : 5.87076235\n",
            "Step 7140 : Loss : 5.86316252\n",
            "Step 7150 : Loss : 5.85564518\n",
            "Step 7160 : Loss : 5.8480835\n",
            "Step 7170 : Loss : 5.8406148\n",
            "Step 7180 : Loss : 5.83307362\n",
            "Step 7190 : Loss : 5.82562065\n",
            "Step 7200 : Loss : 5.81819439\n",
            "Step 7210 : Loss : 5.81057882\n",
            "Step 7220 : Loss : 5.80300426\n",
            "Step 7230 : Loss : 5.79558754\n",
            "Step 7240 : Loss : 5.78814888\n",
            "Step 7250 : Loss : 5.78081512\n",
            "Step 7260 : Loss : 5.7734108\n",
            "Step 7270 : Loss : 5.76595306\n",
            "Step 7280 : Loss : 5.75853205\n",
            "Step 7290 : Loss : 5.75119162\n",
            "Step 7300 : Loss : 5.74383163\n",
            "Step 7310 : Loss : 5.73656368\n",
            "Step 7320 : Loss : 5.72931194\n",
            "Step 7330 : Loss : 5.7219944\n",
            "Step 7340 : Loss : 5.71479034\n",
            "Step 7350 : Loss : 5.70764971\n",
            "Step 7360 : Loss : 5.70045376\n",
            "Step 7370 : Loss : 5.69326544\n",
            "Step 7380 : Loss : 5.68611336\n",
            "Step 7390 : Loss : 5.67903614\n",
            "Step 7400 : Loss : 5.67183971\n",
            "Step 7410 : Loss : 5.66467285\n",
            "Step 7420 : Loss : 5.65761757\n",
            "Step 7430 : Loss : 5.6505475\n",
            "Step 7440 : Loss : 5.64342\n",
            "Step 7450 : Loss : 5.63632488\n",
            "Step 7460 : Loss : 5.62922764\n",
            "Step 7470 : Loss : 5.62224722\n",
            "Step 7480 : Loss : 5.61513901\n",
            "Step 7490 : Loss : 5.60812521\n",
            "Step 7500 : Loss : 5.60120344\n",
            "Step 7510 : Loss : 5.59433651\n",
            "Step 7520 : Loss : 5.58742619\n",
            "Step 7530 : Loss : 5.58052778\n",
            "Step 7540 : Loss : 5.57366228\n",
            "Step 7550 : Loss : 5.56694174\n",
            "Step 7560 : Loss : 5.56025219\n",
            "Step 7570 : Loss : 5.55352688\n",
            "Step 7580 : Loss : 5.54686451\n",
            "Step 7590 : Loss : 5.54046297\n",
            "Step 7600 : Loss : 5.53399849\n",
            "Step 7610 : Loss : 5.52747107\n",
            "Step 7620 : Loss : 5.52087\n",
            "Step 7630 : Loss : 5.5145216\n",
            "Step 7640 : Loss : 5.50813723\n",
            "Step 7650 : Loss : 5.50155926\n",
            "Step 7660 : Loss : 5.49496603\n",
            "Step 7670 : Loss : 5.48830175\n",
            "Step 7670 : Loss :  5.48830175\n",
            "Save weights at epoch: 12\n",
            "Epoch 14\n",
            "Step 7680 : Loss : 5.48165321\n",
            "Step 7690 : Loss : 5.47488976\n",
            "Step 7700 : Loss : 5.4681716\n",
            "Step 7710 : Loss : 5.46149111\n",
            "Step 7720 : Loss : 5.45481205\n",
            "Step 7730 : Loss : 5.44821596\n",
            "Step 7740 : Loss : 5.44162941\n",
            "Step 7750 : Loss : 5.43507767\n",
            "Step 7760 : Loss : 5.4285531\n",
            "Step 7770 : Loss : 5.42196655\n",
            "Step 7780 : Loss : 5.41555166\n",
            "Step 7790 : Loss : 5.40907383\n",
            "Step 7800 : Loss : 5.40246439\n",
            "Step 7810 : Loss : 5.39597893\n",
            "Step 7820 : Loss : 5.38948822\n",
            "Step 7830 : Loss : 5.3829627\n",
            "Step 7840 : Loss : 5.37649345\n",
            "Step 7850 : Loss : 5.3700614\n",
            "Step 7860 : Loss : 5.36358356\n",
            "Step 7870 : Loss : 5.35711145\n",
            "Step 7880 : Loss : 5.3507328\n",
            "Step 7890 : Loss : 5.34427118\n",
            "Step 7900 : Loss : 5.3378849\n",
            "Step 7910 : Loss : 5.33145094\n",
            "Step 7920 : Loss : 5.32507277\n",
            "Step 7930 : Loss : 5.31885242\n",
            "Step 7940 : Loss : 5.31260777\n",
            "Step 7950 : Loss : 5.30640221\n",
            "Step 7960 : Loss : 5.300138\n",
            "Step 7970 : Loss : 5.29380274\n",
            "Step 7980 : Loss : 5.28757095\n",
            "Step 7990 : Loss : 5.28131485\n",
            "Step 8000 : Loss : 5.27512503\n",
            "Step 8010 : Loss : 5.26896048\n",
            "Step 8020 : Loss : 5.26277161\n",
            "Step 8030 : Loss : 5.25661\n",
            "Step 8040 : Loss : 5.25037766\n",
            "Step 8050 : Loss : 5.24423456\n",
            "Step 8060 : Loss : 5.23814917\n",
            "Step 8070 : Loss : 5.23204756\n",
            "Step 8080 : Loss : 5.22597122\n",
            "Step 8090 : Loss : 5.21977425\n",
            "Step 8100 : Loss : 5.21376419\n",
            "Step 8110 : Loss : 5.20763063\n",
            "Step 8120 : Loss : 5.20150471\n",
            "Step 8130 : Loss : 5.19551\n",
            "Step 8140 : Loss : 5.18945026\n",
            "Step 8150 : Loss : 5.18339\n",
            "Step 8160 : Loss : 5.17736673\n",
            "Step 8170 : Loss : 5.17140198\n",
            "Step 8180 : Loss : 5.16545248\n",
            "Step 8190 : Loss : 5.15956831\n",
            "Step 8200 : Loss : 5.15370464\n",
            "Step 8210 : Loss : 5.14783144\n",
            "Step 8220 : Loss : 5.14215469\n",
            "Step 8230 : Loss : 5.13635\n",
            "Step 8240 : Loss : 5.13059235\n",
            "Step 8250 : Loss : 5.12478256\n",
            "Step 8260 : Loss : 5.11899519\n",
            "Step 8260 : Loss :  5.11899519\n",
            "Save weights at epoch: 13\n",
            "Epoch 15\n",
            "Step 8270 : Loss : 5.11316633\n",
            "Step 8280 : Loss : 5.10730553\n",
            "Step 8290 : Loss : 5.10147047\n",
            "Step 8300 : Loss : 5.09555292\n",
            "Step 8310 : Loss : 5.08974695\n",
            "Step 8320 : Loss : 5.08393383\n",
            "Step 8330 : Loss : 5.07813168\n",
            "Step 8340 : Loss : 5.07231379\n",
            "Step 8350 : Loss : 5.06653118\n",
            "Step 8360 : Loss : 5.06076193\n",
            "Step 8370 : Loss : 5.05503416\n",
            "Step 8380 : Loss : 5.04936266\n",
            "Step 8390 : Loss : 5.04362345\n",
            "Step 8400 : Loss : 5.03791809\n",
            "Step 8410 : Loss : 5.03218651\n",
            "Step 8420 : Loss : 5.02643347\n",
            "Step 8430 : Loss : 5.02078581\n",
            "Step 8440 : Loss : 5.01513243\n",
            "Step 8450 : Loss : 5.00947142\n",
            "Step 8460 : Loss : 5.00376368\n",
            "Step 8470 : Loss : 4.99814034\n",
            "Step 8480 : Loss : 4.9924612\n",
            "Step 8490 : Loss : 4.9868269\n",
            "Step 8500 : Loss : 4.98115349\n",
            "Step 8510 : Loss : 4.97553205\n",
            "Step 8520 : Loss : 4.96992922\n",
            "Step 8530 : Loss : 4.96438742\n",
            "Step 8540 : Loss : 4.95887804\n",
            "Step 8550 : Loss : 4.953372\n",
            "Step 8560 : Loss : 4.94785213\n",
            "Step 8570 : Loss : 4.94235802\n",
            "Step 8580 : Loss : 4.93684816\n",
            "Step 8590 : Loss : 4.93132496\n",
            "Step 8600 : Loss : 4.92589378\n",
            "Step 8610 : Loss : 4.92046738\n",
            "Step 8620 : Loss : 4.91503191\n",
            "Step 8630 : Loss : 4.90957117\n",
            "Step 8640 : Loss : 4.9041791\n",
            "Step 8650 : Loss : 4.8987956\n",
            "Step 8660 : Loss : 4.89341497\n",
            "Step 8670 : Loss : 4.88807583\n",
            "Step 8680 : Loss : 4.88264704\n",
            "Step 8690 : Loss : 4.87727976\n",
            "Step 8700 : Loss : 4.87185812\n",
            "Step 8710 : Loss : 4.86641741\n",
            "Step 8720 : Loss : 4.86105728\n",
            "Step 8730 : Loss : 4.85570478\n",
            "Step 8740 : Loss : 4.85037518\n",
            "Step 8750 : Loss : 4.84507322\n",
            "Step 8760 : Loss : 4.83978271\n",
            "Step 8770 : Loss : 4.83452702\n",
            "Step 8780 : Loss : 4.82929325\n",
            "Step 8790 : Loss : 4.82407\n",
            "Step 8800 : Loss : 4.81885099\n",
            "Step 8810 : Loss : 4.81370974\n",
            "Step 8820 : Loss : 4.80848503\n",
            "Step 8830 : Loss : 4.80338335\n",
            "Step 8840 : Loss : 4.79826975\n",
            "Step 8850 : Loss : 4.79313421\n",
            "Step 8850 : Loss :  4.79313421\n",
            "Save weights at epoch: 14\n",
            "Epoch 16\n",
            "Step 8860 : Loss : 4.78809071\n",
            "Step 8870 : Loss : 4.78303671\n",
            "Step 8880 : Loss : 4.77794933\n",
            "Step 8890 : Loss : 4.77281523\n",
            "Step 8900 : Loss : 4.76772881\n",
            "Step 8910 : Loss : 4.76271\n",
            "Step 8920 : Loss : 4.75769186\n",
            "Step 8930 : Loss : 4.75264359\n",
            "Step 8940 : Loss : 4.74762392\n",
            "Step 8950 : Loss : 4.7425642\n",
            "Step 8960 : Loss : 4.73759508\n",
            "Step 8970 : Loss : 4.73260403\n",
            "Step 8980 : Loss : 4.72757769\n",
            "Step 8990 : Loss : 4.72254848\n",
            "Step 9000 : Loss : 4.71754599\n",
            "Step 9010 : Loss : 4.71256447\n",
            "Step 9020 : Loss : 4.70760918\n",
            "Step 9030 : Loss : 4.70265627\n",
            "Step 9040 : Loss : 4.69764757\n",
            "Step 9050 : Loss : 4.69272\n",
            "Step 9060 : Loss : 4.68779469\n",
            "Step 9070 : Loss : 4.68284559\n",
            "Step 9080 : Loss : 4.67790937\n",
            "Step 9090 : Loss : 4.67296267\n",
            "Step 9100 : Loss : 4.66800976\n",
            "Step 9110 : Loss : 4.66309071\n",
            "Step 9120 : Loss : 4.65819407\n",
            "Step 9130 : Loss : 4.65335751\n",
            "Step 9140 : Loss : 4.64848709\n",
            "Step 9150 : Loss : 4.6436286\n",
            "Step 9160 : Loss : 4.63881683\n",
            "Step 9170 : Loss : 4.63399506\n",
            "Step 9180 : Loss : 4.6291194\n",
            "Step 9190 : Loss : 4.62428427\n",
            "Step 9200 : Loss : 4.61949\n",
            "Step 9210 : Loss : 4.61475086\n",
            "Step 9220 : Loss : 4.60996199\n",
            "Step 9230 : Loss : 4.60526657\n",
            "Step 9240 : Loss : 4.60054922\n",
            "Step 9250 : Loss : 4.59580851\n",
            "Step 9260 : Loss : 4.59107828\n",
            "Step 9270 : Loss : 4.58636332\n",
            "Step 9280 : Loss : 4.58166218\n",
            "Step 9290 : Loss : 4.57692909\n",
            "Step 9300 : Loss : 4.5721693\n",
            "Step 9310 : Loss : 4.56743574\n",
            "Step 9320 : Loss : 4.5626955\n",
            "Step 9330 : Loss : 4.55801249\n",
            "Step 9340 : Loss : 4.55332518\n",
            "Step 9350 : Loss : 4.54863691\n",
            "Step 9360 : Loss : 4.5439868\n",
            "Step 9370 : Loss : 4.5393734\n",
            "Step 9380 : Loss : 4.53478956\n",
            "Step 9390 : Loss : 4.53022194\n",
            "Step 9400 : Loss : 4.52578974\n",
            "Step 9410 : Loss : 4.52125931\n",
            "Step 9420 : Loss : 4.51677513\n",
            "Step 9430 : Loss : 4.51220179\n",
            "Step 9440 : Loss : 4.50772\n",
            "Step 9440 : Loss :  4.50772\n",
            "Save weights at epoch: 15\n",
            "Epoch 17\n",
            "Step 9450 : Loss : 4.5031991\n",
            "Step 9460 : Loss : 4.49873829\n",
            "Step 9470 : Loss : 4.49430466\n",
            "Step 9480 : Loss : 4.4898386\n",
            "Step 9490 : Loss : 4.48541927\n",
            "Step 9500 : Loss : 4.48101616\n",
            "Step 9510 : Loss : 4.47662\n",
            "Step 9520 : Loss : 4.47217846\n",
            "Step 9530 : Loss : 4.4678\n",
            "Step 9540 : Loss : 4.46343517\n",
            "Step 9550 : Loss : 4.45907497\n",
            "Step 9560 : Loss : 4.4546771\n",
            "Step 9570 : Loss : 4.45024109\n",
            "Step 9580 : Loss : 4.4458766\n",
            "Step 9590 : Loss : 4.44152594\n",
            "Step 9600 : Loss : 4.43716717\n",
            "Step 9610 : Loss : 4.43283606\n",
            "Step 9620 : Loss : 4.42851639\n",
            "Step 9630 : Loss : 4.42416096\n",
            "Step 9640 : Loss : 4.41980028\n",
            "Step 9650 : Loss : 4.4155\n",
            "Step 9660 : Loss : 4.41115379\n",
            "Step 9670 : Loss : 4.40684509\n",
            "Step 9680 : Loss : 4.40248775\n",
            "Step 9690 : Loss : 4.39816523\n",
            "Step 9700 : Loss : 4.39388037\n",
            "Step 9710 : Loss : 4.38965082\n",
            "Step 9720 : Loss : 4.3853941\n",
            "Step 9730 : Loss : 4.38110256\n",
            "Step 9740 : Loss : 4.37680197\n",
            "Step 9750 : Loss : 4.37253666\n",
            "Step 9760 : Loss : 4.36823177\n",
            "Step 9770 : Loss : 4.36392\n",
            "Step 9780 : Loss : 4.35969591\n",
            "Step 9790 : Loss : 4.35548639\n",
            "Step 9800 : Loss : 4.35125399\n",
            "Step 9810 : Loss : 4.34699535\n",
            "Step 9820 : Loss : 4.34273243\n",
            "Step 9830 : Loss : 4.33856773\n",
            "Step 9840 : Loss : 4.33442926\n",
            "Step 9850 : Loss : 4.33026266\n",
            "Step 9860 : Loss : 4.3260746\n",
            "Step 9870 : Loss : 4.32189178\n",
            "Step 9880 : Loss : 4.31768084\n",
            "Step 9890 : Loss : 4.31348276\n",
            "Step 9900 : Loss : 4.30926704\n",
            "Step 9910 : Loss : 4.30511427\n",
            "Step 9920 : Loss : 4.30096817\n",
            "Step 9930 : Loss : 4.29683352\n",
            "Step 9940 : Loss : 4.29270172\n",
            "Step 9950 : Loss : 4.28858137\n",
            "Step 9960 : Loss : 4.28448057\n",
            "Step 9970 : Loss : 4.28035259\n",
            "Step 9980 : Loss : 4.27623749\n",
            "Step 9990 : Loss : 4.27217531\n",
            "Step 10000 : Loss : 4.26810455\n",
            "Step 10010 : Loss : 4.26402521\n",
            "Step 10020 : Loss : 4.25994825\n",
            "Step 10030 : Loss : 4.25591707\n",
            "Step 10030 : Loss :  4.25591707\n",
            "Save weights at epoch: 16\n",
            "Epoch 18\n",
            "Step 10040 : Loss : 4.25196648\n",
            "Step 10050 : Loss : 4.24801\n",
            "Step 10060 : Loss : 4.24406624\n",
            "Step 10070 : Loss : 4.24016762\n",
            "Step 10080 : Loss : 4.23627281\n",
            "Step 10090 : Loss : 4.2324357\n",
            "Step 10100 : Loss : 4.22852898\n",
            "Step 10110 : Loss : 4.22458935\n",
            "Step 10120 : Loss : 4.22063065\n",
            "Step 10130 : Loss : 4.21671724\n",
            "Step 10140 : Loss : 4.21283388\n",
            "Step 10150 : Loss : 4.20893097\n",
            "Step 10160 : Loss : 4.20497942\n",
            "Step 10170 : Loss : 4.20108938\n",
            "Step 10180 : Loss : 4.19717121\n",
            "Step 10190 : Loss : 4.19325495\n",
            "Step 10200 : Loss : 4.18935871\n",
            "Step 10210 : Loss : 4.18551922\n",
            "Step 10220 : Loss : 4.18168068\n",
            "Step 10230 : Loss : 4.17782593\n",
            "Step 10240 : Loss : 4.17399597\n",
            "Step 10250 : Loss : 4.17015362\n",
            "Step 10260 : Loss : 4.16632\n",
            "Step 10270 : Loss : 4.16248846\n",
            "Step 10280 : Loss : 4.15864134\n",
            "Step 10290 : Loss : 4.15484285\n",
            "Step 10300 : Loss : 4.15104628\n",
            "Step 10310 : Loss : 4.1472435\n",
            "Step 10320 : Loss : 4.14343596\n",
            "Step 10330 : Loss : 4.13960075\n",
            "Step 10340 : Loss : 4.1358285\n",
            "Step 10350 : Loss : 4.13204575\n",
            "Step 10360 : Loss : 4.12820387\n",
            "Step 10370 : Loss : 4.1244278\n",
            "Step 10380 : Loss : 4.12063217\n",
            "Step 10390 : Loss : 4.11688232\n",
            "Step 10400 : Loss : 4.11312771\n",
            "Step 10410 : Loss : 4.10934782\n",
            "Step 10420 : Loss : 4.10562277\n",
            "Step 10430 : Loss : 4.10186\n",
            "Step 10440 : Loss : 4.0981369\n",
            "Step 10450 : Loss : 4.09440947\n",
            "Step 10460 : Loss : 4.09066772\n",
            "Step 10470 : Loss : 4.08690882\n",
            "Step 10480 : Loss : 4.0831542\n",
            "Step 10490 : Loss : 4.0793972\n",
            "Step 10500 : Loss : 4.07565498\n",
            "Step 10510 : Loss : 4.07192659\n",
            "Step 10520 : Loss : 4.06822491\n",
            "Step 10530 : Loss : 4.06448364\n",
            "Step 10540 : Loss : 4.06078911\n",
            "Step 10550 : Loss : 4.05707932\n",
            "Step 10560 : Loss : 4.05340099\n",
            "Step 10570 : Loss : 4.04970074\n",
            "Step 10580 : Loss : 4.04602385\n",
            "Step 10590 : Loss : 4.04235268\n",
            "Step 10600 : Loss : 4.03865433\n",
            "Step 10610 : Loss : 4.03497362\n",
            "Step 10620 : Loss : 4.03135109\n",
            "Step 10620 : Loss :  4.03135109\n",
            "Save weights at epoch: 17\n",
            "Epoch 19\n",
            "Step 10630 : Loss : 4.02771759\n",
            "Step 10640 : Loss : 4.02410936\n",
            "Step 10650 : Loss : 4.02056026\n",
            "Step 10660 : Loss : 4.01694679\n",
            "Step 10670 : Loss : 4.01337767\n",
            "Step 10680 : Loss : 4.00984335\n",
            "Step 10690 : Loss : 4.00632238\n",
            "Step 10700 : Loss : 4.00277615\n",
            "Step 10710 : Loss : 3.99925661\n",
            "Step 10720 : Loss : 3.99575734\n",
            "Step 10730 : Loss : 3.99228168\n",
            "Step 10740 : Loss : 3.98886037\n",
            "Step 10750 : Loss : 3.98538232\n",
            "Step 10760 : Loss : 3.98196435\n",
            "Step 10770 : Loss : 3.9784863\n",
            "Step 10780 : Loss : 3.97499275\n",
            "Step 10790 : Loss : 3.9715662\n",
            "Step 10800 : Loss : 3.96815252\n",
            "Step 10810 : Loss : 3.96470928\n",
            "Step 10820 : Loss : 3.96124649\n",
            "Step 10830 : Loss : 3.95786166\n",
            "Step 10840 : Loss : 3.95441031\n",
            "Step 10850 : Loss : 3.95100307\n",
            "Step 10860 : Loss : 3.94760346\n",
            "Step 10870 : Loss : 3.94420218\n",
            "Step 10880 : Loss : 3.9407897\n",
            "Step 10890 : Loss : 3.93744969\n",
            "Step 10900 : Loss : 3.93411565\n",
            "Step 10910 : Loss : 3.93072653\n",
            "Step 10920 : Loss : 3.92733598\n",
            "Step 10930 : Loss : 3.92392397\n",
            "Step 10940 : Loss : 3.9205029\n",
            "Step 10950 : Loss : 3.91708803\n",
            "Step 10960 : Loss : 3.91369653\n",
            "Step 10970 : Loss : 3.91038775\n",
            "Step 10980 : Loss : 3.90699887\n",
            "Step 10990 : Loss : 3.90365362\n",
            "Step 11000 : Loss : 3.90029836\n",
            "Step 11010 : Loss : 3.89698076\n",
            "Step 11020 : Loss : 3.8936255\n",
            "Step 11030 : Loss : 3.8902719\n",
            "Step 11040 : Loss : 3.88696122\n",
            "Step 11050 : Loss : 3.88361526\n",
            "Step 11060 : Loss : 3.88029146\n",
            "Step 11070 : Loss : 3.87696266\n",
            "Step 11080 : Loss : 3.87363076\n",
            "Step 11090 : Loss : 3.87031698\n",
            "Step 11100 : Loss : 3.86700654\n",
            "Step 11110 : Loss : 3.86368585\n",
            "Step 11120 : Loss : 3.86036706\n",
            "Step 11130 : Loss : 3.85706663\n",
            "Step 11140 : Loss : 3.8537972\n",
            "Step 11150 : Loss : 3.85053706\n",
            "Step 11160 : Loss : 3.84723449\n",
            "Step 11170 : Loss : 3.84394717\n",
            "Step 11180 : Loss : 3.84068966\n",
            "Step 11190 : Loss : 3.83743238\n",
            "Step 11200 : Loss : 3.83419847\n",
            "Step 11210 : Loss : 3.83093858\n",
            "Step 11210 : Loss :  3.83093858\n",
            "Save weights at epoch: 18\n",
            "Epoch 20\n",
            "Step 11220 : Loss : 3.82768631\n",
            "Step 11230 : Loss : 3.82445598\n",
            "Step 11240 : Loss : 3.82122397\n",
            "Step 11250 : Loss : 3.81802249\n",
            "Step 11260 : Loss : 3.81481\n",
            "Step 11270 : Loss : 3.81163836\n",
            "Step 11280 : Loss : 3.80849671\n",
            "Step 11290 : Loss : 3.80532694\n",
            "Step 11300 : Loss : 3.80218434\n",
            "Step 11310 : Loss : 3.79899573\n",
            "Step 11320 : Loss : 3.79585052\n",
            "Step 11330 : Loss : 3.79272223\n",
            "Step 11340 : Loss : 3.78953099\n",
            "Step 11350 : Loss : 3.78635335\n",
            "Step 11360 : Loss : 3.78321457\n",
            "Step 11370 : Loss : 3.78008389\n",
            "Step 11380 : Loss : 3.7769618\n",
            "Step 11390 : Loss : 3.77389812\n",
            "Step 11400 : Loss : 3.77077198\n",
            "Step 11410 : Loss : 3.76765895\n",
            "Step 11420 : Loss : 3.76454782\n",
            "Step 11430 : Loss : 3.76141286\n",
            "Step 11440 : Loss : 3.75830054\n",
            "Step 11450 : Loss : 3.75519967\n",
            "Step 11460 : Loss : 3.75211954\n",
            "Step 11470 : Loss : 3.74910593\n",
            "Step 11480 : Loss : 3.74610162\n",
            "Step 11490 : Loss : 3.7430687\n",
            "Step 11500 : Loss : 3.74002314\n",
            "Step 11510 : Loss : 3.73693323\n",
            "Step 11520 : Loss : 3.73390031\n",
            "Step 11530 : Loss : 3.73082924\n",
            "Step 11540 : Loss : 3.72772455\n",
            "Step 11550 : Loss : 3.72467828\n",
            "Step 11560 : Loss : 3.72166443\n",
            "Step 11570 : Loss : 3.71863294\n",
            "Step 11580 : Loss : 3.71559763\n",
            "Step 11590 : Loss : 3.71257472\n",
            "Step 11600 : Loss : 3.7096\n",
            "Step 11610 : Loss : 3.70660186\n",
            "Step 11620 : Loss : 3.70363164\n",
            "Step 11630 : Loss : 3.70064855\n",
            "Step 11640 : Loss : 3.69766641\n",
            "Step 11650 : Loss : 3.6946485\n",
            "Step 11660 : Loss : 3.69168425\n",
            "Step 11670 : Loss : 3.68868637\n",
            "Step 11680 : Loss : 3.68568373\n",
            "Step 11690 : Loss : 3.68270063\n",
            "Step 11700 : Loss : 3.67971826\n",
            "Step 11710 : Loss : 3.67676806\n",
            "Step 11720 : Loss : 3.67377424\n",
            "Step 11730 : Loss : 3.67082524\n",
            "Step 11740 : Loss : 3.66786623\n",
            "Step 11750 : Loss : 3.66487956\n",
            "Step 11760 : Loss : 3.66191363\n",
            "Step 11770 : Loss : 3.6589489\n",
            "Step 11780 : Loss : 3.65597916\n",
            "Step 11790 : Loss : 3.65298152\n",
            "Step 11800 : Loss : 3.65002179\n",
            "Step 11800 : Loss :  3.65002179\n",
            "Save weights at epoch: 19\n",
            "Epoch 21\n",
            "Step 11810 : Loss : 3.64711046\n",
            "Step 11820 : Loss : 3.64418626\n",
            "Step 11830 : Loss : 3.64125443\n",
            "Step 11840 : Loss : 3.63828826\n",
            "Step 11850 : Loss : 3.63537788\n",
            "Step 11860 : Loss : 3.63246655\n",
            "Step 11870 : Loss : 3.62954903\n",
            "Step 11880 : Loss : 3.62662816\n",
            "Step 11890 : Loss : 3.62378144\n",
            "Step 11900 : Loss : 3.62092185\n",
            "Step 11910 : Loss : 3.61807442\n",
            "Step 11920 : Loss : 3.61519742\n",
            "Step 11930 : Loss : 3.61237907\n",
            "Step 11940 : Loss : 3.60954523\n",
            "Step 11950 : Loss : 3.60671782\n",
            "Step 11960 : Loss : 3.60385799\n",
            "Step 11970 : Loss : 3.60103393\n",
            "Step 11980 : Loss : 3.59821057\n",
            "Step 11990 : Loss : 3.59540296\n",
            "Step 12000 : Loss : 3.59259701\n",
            "Step 12010 : Loss : 3.58977795\n",
            "Step 12020 : Loss : 3.58692193\n",
            "Step 12030 : Loss : 3.58414078\n",
            "Step 12040 : Loss : 3.58131289\n",
            "Step 12050 : Loss : 3.57845211\n",
            "Step 12060 : Loss : 3.57565665\n",
            "Step 12070 : Loss : 3.57280087\n",
            "Step 12080 : Loss : 3.57005024\n",
            "Step 12090 : Loss : 3.56724691\n",
            "Step 12100 : Loss : 3.564399\n",
            "Step 12110 : Loss : 3.56156683\n",
            "Step 12120 : Loss : 3.55875802\n",
            "Step 12130 : Loss : 3.55592775\n",
            "Step 12140 : Loss : 3.55314279\n",
            "Step 12150 : Loss : 3.55036139\n",
            "Step 12160 : Loss : 3.5476017\n",
            "Step 12170 : Loss : 3.54478455\n",
            "Step 12180 : Loss : 3.54201\n",
            "Step 12190 : Loss : 3.53924632\n",
            "Step 12200 : Loss : 3.53645205\n",
            "Step 12210 : Loss : 3.53367448\n",
            "Step 12220 : Loss : 3.53089523\n",
            "Step 12230 : Loss : 3.52812433\n",
            "Step 12240 : Loss : 3.5253489\n",
            "Step 12250 : Loss : 3.52257657\n",
            "Step 12260 : Loss : 3.51979566\n",
            "Step 12270 : Loss : 3.51702762\n",
            "Step 12280 : Loss : 3.51430488\n",
            "Step 12290 : Loss : 3.51157355\n",
            "Step 12300 : Loss : 3.50885773\n",
            "Step 12310 : Loss : 3.50612903\n",
            "Step 12320 : Loss : 3.50341702\n",
            "Step 12330 : Loss : 3.5007112\n",
            "Step 12340 : Loss : 3.49796796\n",
            "Step 12350 : Loss : 3.49524951\n",
            "Step 12360 : Loss : 3.49251556\n",
            "Step 12370 : Loss : 3.48979092\n",
            "Step 12380 : Loss : 3.48706055\n",
            "Step 12390 : Loss : 3.4843328\n",
            "Step 12390 : Loss :  3.4843328\n",
            "Save weights at epoch: 20\n",
            "Epoch 22\n",
            "Step 12400 : Loss : 3.48161769\n",
            "Step 12410 : Loss : 3.47887468\n",
            "Step 12420 : Loss : 3.47618532\n",
            "Step 12430 : Loss : 3.47347331\n",
            "Step 12440 : Loss : 3.47080588\n",
            "Step 12450 : Loss : 3.46812224\n",
            "Step 12460 : Loss : 3.46546602\n",
            "Step 12470 : Loss : 3.46277905\n",
            "Step 12480 : Loss : 3.46012235\n",
            "Step 12490 : Loss : 3.45743489\n",
            "Step 12500 : Loss : 3.45478439\n",
            "Step 12510 : Loss : 3.4521234\n",
            "Step 12520 : Loss : 3.44945145\n",
            "Step 12530 : Loss : 3.44680023\n",
            "Step 12540 : Loss : 3.44414425\n",
            "Step 12550 : Loss : 3.44149113\n",
            "Step 12560 : Loss : 3.4388411\n",
            "Step 12570 : Loss : 3.43622661\n",
            "Step 12580 : Loss : 3.433568\n",
            "Step 12590 : Loss : 3.43093395\n",
            "Step 12600 : Loss : 3.42831039\n",
            "Step 12610 : Loss : 3.42568326\n",
            "Step 12620 : Loss : 3.42306089\n",
            "Step 12630 : Loss : 3.42045951\n",
            "Step 12640 : Loss : 3.41784978\n",
            "Step 12650 : Loss : 3.4153316\n",
            "Step 12660 : Loss : 3.41280913\n",
            "Step 12670 : Loss : 3.41031075\n",
            "Step 12680 : Loss : 3.40774679\n",
            "Step 12690 : Loss : 3.40517282\n",
            "Step 12700 : Loss : 3.40261173\n",
            "Step 12710 : Loss : 3.40001321\n",
            "Step 12720 : Loss : 3.39745688\n",
            "Step 12730 : Loss : 3.394912\n",
            "Step 12740 : Loss : 3.39237094\n",
            "Step 12750 : Loss : 3.38980913\n",
            "Step 12760 : Loss : 3.38724017\n",
            "Step 12770 : Loss : 3.38470721\n",
            "Step 12780 : Loss : 3.38217926\n",
            "Step 12790 : Loss : 3.37961817\n",
            "Step 12800 : Loss : 3.37710047\n",
            "Step 12810 : Loss : 3.37458396\n",
            "Step 12820 : Loss : 3.37206149\n",
            "Step 12830 : Loss : 3.36954093\n",
            "Step 12840 : Loss : 3.36708975\n",
            "Step 12850 : Loss : 3.36457253\n",
            "Step 12860 : Loss : 3.36204982\n",
            "Step 12870 : Loss : 3.35953832\n",
            "Step 12880 : Loss : 3.3570447\n",
            "Step 12890 : Loss : 3.35452724\n",
            "Step 12900 : Loss : 3.35204124\n",
            "Step 12910 : Loss : 3.34954047\n",
            "Step 12920 : Loss : 3.34706521\n",
            "Step 12930 : Loss : 3.34457183\n",
            "Step 12940 : Loss : 3.34211254\n",
            "Step 12950 : Loss : 3.33964849\n",
            "Step 12960 : Loss : 3.33716297\n",
            "Step 12970 : Loss : 3.33470774\n",
            "Step 12980 : Loss : 3.33224702\n",
            "Step 12980 : Loss :  3.33224702\n",
            "Save weights at epoch: 21\n",
            "Epoch 23\n",
            "Step 12990 : Loss : 3.32980728\n",
            "Step 13000 : Loss : 3.32733679\n",
            "Step 13010 : Loss : 3.32488918\n",
            "Step 13020 : Loss : 3.32243776\n",
            "Step 13030 : Loss : 3.32001543\n",
            "Step 13040 : Loss : 3.31760669\n",
            "Step 13050 : Loss : 3.31522059\n",
            "Step 13060 : Loss : 3.31281352\n",
            "Step 13070 : Loss : 3.31039143\n",
            "Step 13080 : Loss : 3.30800962\n",
            "Step 13090 : Loss : 3.30565047\n",
            "Step 13100 : Loss : 3.3032403\n",
            "Step 13110 : Loss : 3.3008182\n",
            "Step 13120 : Loss : 3.29840255\n",
            "Step 13130 : Loss : 3.29600263\n",
            "Step 13140 : Loss : 3.29361391\n",
            "Step 13150 : Loss : 3.29124141\n",
            "Step 13160 : Loss : 3.28884792\n",
            "Step 13170 : Loss : 3.28647494\n",
            "Step 13180 : Loss : 3.28415489\n",
            "Step 13190 : Loss : 3.2818048\n",
            "Step 13200 : Loss : 3.27945733\n",
            "Step 13210 : Loss : 3.2770865\n",
            "Step 13220 : Loss : 3.274719\n",
            "Step 13230 : Loss : 3.2723496\n",
            "Step 13240 : Loss : 3.26998639\n",
            "Step 13250 : Loss : 3.26762342\n",
            "Step 13260 : Loss : 3.26530457\n",
            "Step 13270 : Loss : 3.26297426\n",
            "Step 13280 : Loss : 3.26065278\n",
            "Step 13290 : Loss : 3.25828958\n",
            "Step 13300 : Loss : 3.25594687\n",
            "Step 13310 : Loss : 3.25362945\n",
            "Step 13320 : Loss : 3.25133491\n",
            "Step 13330 : Loss : 3.24907446\n",
            "Step 13340 : Loss : 3.24678707\n",
            "Step 13350 : Loss : 3.24453235\n",
            "Step 13360 : Loss : 3.24224567\n",
            "Step 13370 : Loss : 3.23999619\n",
            "Step 13380 : Loss : 3.23770523\n",
            "Step 13390 : Loss : 3.2354393\n",
            "Step 13400 : Loss : 3.23313642\n",
            "Step 13410 : Loss : 3.23083162\n",
            "Step 13420 : Loss : 3.22853971\n",
            "Step 13430 : Loss : 3.22628617\n",
            "Step 13440 : Loss : 3.22399282\n",
            "Step 13450 : Loss : 3.22172189\n",
            "Step 13460 : Loss : 3.2194593\n",
            "Step 13470 : Loss : 3.21721\n",
            "Step 13480 : Loss : 3.2149477\n",
            "Step 13490 : Loss : 3.21273351\n",
            "Step 13500 : Loss : 3.21049142\n",
            "Step 13510 : Loss : 3.20823812\n",
            "Step 13520 : Loss : 3.20597649\n",
            "Step 13530 : Loss : 3.20373869\n",
            "Step 13540 : Loss : 3.20152593\n",
            "Step 13550 : Loss : 3.19927025\n",
            "Step 13560 : Loss : 3.19700575\n",
            "Step 13570 : Loss : 3.19474721\n",
            "Step 13570 : Loss :  3.19474721\n",
            "Save weights at epoch: 22\n",
            "Epoch 24\n",
            "Step 13580 : Loss : 3.19249201\n",
            "Step 13590 : Loss : 3.19026065\n",
            "Step 13600 : Loss : 3.18802309\n",
            "Step 13610 : Loss : 3.18579316\n",
            "Step 13620 : Loss : 3.18356705\n",
            "Step 13630 : Loss : 3.18137622\n",
            "Step 13640 : Loss : 3.17917895\n",
            "Step 13650 : Loss : 3.17696023\n",
            "Step 13660 : Loss : 3.17477036\n",
            "Step 13670 : Loss : 3.17255473\n",
            "Step 13680 : Loss : 3.17035222\n",
            "Step 13690 : Loss : 3.16812348\n",
            "Step 13700 : Loss : 3.16589618\n",
            "Step 13710 : Loss : 3.16368127\n",
            "Step 13720 : Loss : 3.16143537\n",
            "Step 13730 : Loss : 3.15922928\n",
            "Step 13740 : Loss : 3.15703773\n",
            "Step 13750 : Loss : 3.1548357\n",
            "Step 13760 : Loss : 3.15263391\n",
            "Step 13770 : Loss : 3.1504724\n",
            "Step 13780 : Loss : 3.14827394\n",
            "Step 13790 : Loss : 3.14607644\n",
            "Step 13800 : Loss : 3.14389348\n",
            "Step 13810 : Loss : 3.14169884\n",
            "Step 13820 : Loss : 3.1394918\n",
            "Step 13830 : Loss : 3.13730168\n",
            "Step 13840 : Loss : 3.13512182\n",
            "Step 13850 : Loss : 3.13296604\n",
            "Step 13860 : Loss : 3.13078618\n",
            "Step 13870 : Loss : 3.12861228\n",
            "Step 13880 : Loss : 3.12645221\n",
            "Step 13890 : Loss : 3.12431097\n",
            "Step 13900 : Loss : 3.12215209\n",
            "Step 13910 : Loss : 3.12000608\n",
            "Step 13920 : Loss : 3.11785197\n",
            "Step 13930 : Loss : 3.11570621\n",
            "Step 13940 : Loss : 3.11356449\n",
            "Step 13950 : Loss : 3.11145163\n",
            "Step 13960 : Loss : 3.10934782\n",
            "Step 13970 : Loss : 3.10722399\n",
            "Step 13980 : Loss : 3.10510969\n",
            "Step 13990 : Loss : 3.10297394\n",
            "Step 14000 : Loss : 3.10083723\n",
            "Step 14010 : Loss : 3.09871364\n",
            "Step 14020 : Loss : 3.09657574\n",
            "Step 14030 : Loss : 3.09445381\n",
            "Step 14040 : Loss : 3.09235644\n",
            "Step 14050 : Loss : 3.09029269\n",
            "Step 14060 : Loss : 3.0881846\n",
            "Step 14070 : Loss : 3.08607364\n",
            "Step 14080 : Loss : 3.08398485\n",
            "Step 14090 : Loss : 3.08192182\n",
            "Step 14100 : Loss : 3.07983112\n",
            "Step 14110 : Loss : 3.07774734\n",
            "Step 14120 : Loss : 3.07569861\n",
            "Step 14130 : Loss : 3.07364583\n",
            "Step 14140 : Loss : 3.07158113\n",
            "Step 14150 : Loss : 3.06951308\n",
            "Step 14160 : Loss : 3.06743598\n",
            "Step 14160 : Loss :  3.06743598\n",
            "Save weights at epoch: 23\n",
            "Epoch 25\n",
            "Step 14170 : Loss : 3.06536531\n",
            "Step 14180 : Loss : 3.06331849\n",
            "Step 14190 : Loss : 3.06127453\n",
            "Step 14200 : Loss : 3.05923867\n",
            "Step 14210 : Loss : 3.0571928\n",
            "Step 14220 : Loss : 3.05515265\n",
            "Step 14230 : Loss : 3.05309176\n",
            "Step 14240 : Loss : 3.05105972\n",
            "Step 14250 : Loss : 3.04901361\n",
            "Step 14260 : Loss : 3.04698324\n",
            "Step 14270 : Loss : 3.04497242\n",
            "Step 14280 : Loss : 3.04293847\n",
            "Step 14290 : Loss : 3.04090261\n",
            "Step 14300 : Loss : 3.03886938\n",
            "Step 14310 : Loss : 3.03683972\n",
            "Step 14320 : Loss : 3.03482723\n",
            "Step 14330 : Loss : 3.03283072\n",
            "Step 14340 : Loss : 3.03082228\n",
            "Step 14350 : Loss : 3.02880192\n",
            "Step 14360 : Loss : 3.0267725\n",
            "Step 14370 : Loss : 3.02475834\n",
            "Step 14380 : Loss : 3.02272224\n",
            "Step 14390 : Loss : 3.02069545\n",
            "Step 14400 : Loss : 3.018682\n",
            "Step 14410 : Loss : 3.01666975\n",
            "Step 14420 : Loss : 3.01466417\n",
            "Step 14430 : Loss : 3.01266456\n",
            "Step 14440 : Loss : 3.01070976\n",
            "Step 14450 : Loss : 3.00872421\n",
            "Step 14460 : Loss : 3.00673413\n",
            "Step 14470 : Loss : 3.00474072\n",
            "Step 14480 : Loss : 3.00276303\n",
            "Step 14490 : Loss : 3.00076723\n",
            "Step 14500 : Loss : 2.99881148\n",
            "Step 14510 : Loss : 2.99686074\n",
            "Step 14520 : Loss : 2.9949007\n",
            "Step 14530 : Loss : 2.99293256\n",
            "Step 14540 : Loss : 2.99097\n",
            "Step 14550 : Loss : 2.98903012\n",
            "Step 14560 : Loss : 2.9871006\n",
            "Step 14570 : Loss : 2.98517346\n",
            "Step 14580 : Loss : 2.98322964\n",
            "Step 14590 : Loss : 2.98128986\n",
            "Step 14600 : Loss : 2.97932982\n",
            "Step 14610 : Loss : 2.97737932\n",
            "Step 14620 : Loss : 2.97544265\n",
            "Step 14630 : Loss : 2.97351885\n",
            "Step 14640 : Loss : 2.97157693\n",
            "Step 14650 : Loss : 2.96964025\n",
            "Step 14660 : Loss : 2.96771049\n",
            "Step 14670 : Loss : 2.96580935\n",
            "Step 14680 : Loss : 2.96385479\n",
            "Step 14690 : Loss : 2.96191192\n",
            "Step 14700 : Loss : 2.9600296\n",
            "Step 14710 : Loss : 2.95812178\n",
            "Step 14720 : Loss : 2.95622563\n",
            "Step 14730 : Loss : 2.95432\n",
            "Step 14740 : Loss : 2.95242453\n",
            "Step 14750 : Loss : 2.95050716\n",
            "Step 14750 : Loss :  2.95050716\n",
            "Save weights at epoch: 24\n",
            "Epoch 26\n",
            "Step 14760 : Loss : 2.94860625\n",
            "Step 14770 : Loss : 2.94672656\n",
            "Step 14780 : Loss : 2.94484067\n",
            "Step 14790 : Loss : 2.94292569\n",
            "Step 14800 : Loss : 2.94108081\n",
            "Step 14810 : Loss : 2.93924522\n",
            "Step 14820 : Loss : 2.93737268\n",
            "Step 14830 : Loss : 2.93553877\n",
            "Step 14840 : Loss : 2.93370986\n",
            "Step 14850 : Loss : 2.93188667\n",
            "Step 14860 : Loss : 2.93002248\n",
            "Step 14870 : Loss : 2.92815423\n",
            "Step 14880 : Loss : 2.92630315\n",
            "Step 14890 : Loss : 2.92447186\n",
            "Step 14900 : Loss : 2.92261052\n",
            "Step 14910 : Loss : 2.92073488\n",
            "Step 14920 : Loss : 2.91890788\n",
            "Step 14930 : Loss : 2.91711378\n",
            "Step 14940 : Loss : 2.9152813\n",
            "Step 14950 : Loss : 2.91344738\n",
            "Step 14960 : Loss : 2.9116354\n",
            "Step 14970 : Loss : 2.90980935\n",
            "Step 14980 : Loss : 2.9079802\n",
            "Step 14990 : Loss : 2.90616703\n",
            "Step 15000 : Loss : 2.90430832\n",
            "Step 15010 : Loss : 2.90249\n",
            "Step 15020 : Loss : 2.90068817\n",
            "Step 15030 : Loss : 2.89889336\n",
            "Step 15040 : Loss : 2.89709806\n",
            "Step 15050 : Loss : 2.89529443\n",
            "Step 15060 : Loss : 2.89352417\n",
            "Step 15070 : Loss : 2.89174366\n",
            "Step 15080 : Loss : 2.88995099\n",
            "Step 15090 : Loss : 2.8881824\n",
            "Step 15100 : Loss : 2.88640738\n",
            "Step 15110 : Loss : 2.88458848\n",
            "Step 15120 : Loss : 2.88280439\n",
            "Step 15130 : Loss : 2.88103962\n",
            "Step 15140 : Loss : 2.87928438\n",
            "Step 15150 : Loss : 2.87751484\n",
            "Step 15160 : Loss : 2.87573957\n",
            "Step 15170 : Loss : 2.8739357\n",
            "Step 15180 : Loss : 2.87217641\n",
            "Step 15190 : Loss : 2.87037086\n",
            "Step 15200 : Loss : 2.86857533\n",
            "Step 15210 : Loss : 2.86679792\n",
            "Step 15220 : Loss : 2.86501932\n",
            "Step 15230 : Loss : 2.8632338\n",
            "Step 15240 : Loss : 2.86145973\n",
            "Step 15250 : Loss : 2.85966682\n",
            "Step 15260 : Loss : 2.85789943\n",
            "Step 15270 : Loss : 2.85611439\n",
            "Step 15280 : Loss : 2.8543303\n",
            "Step 15290 : Loss : 2.85257244\n",
            "Step 15300 : Loss : 2.85081196\n",
            "Step 15310 : Loss : 2.84908175\n",
            "Step 15320 : Loss : 2.84728503\n",
            "Step 15330 : Loss : 2.84549975\n",
            "Step 15340 : Loss : 2.84375262\n",
            "Step 15340 : Loss :  2.84375262\n",
            "Save weights at epoch: 25\n",
            "Epoch 27\n",
            "Step 15350 : Loss : 2.84198046\n",
            "Step 15360 : Loss : 2.84019184\n",
            "Step 15370 : Loss : 2.83842301\n",
            "Step 15380 : Loss : 2.83665442\n",
            "Step 15390 : Loss : 2.83487964\n",
            "Step 15400 : Loss : 2.83309793\n",
            "Step 15410 : Loss : 2.83134723\n",
            "Step 15420 : Loss : 2.82959533\n",
            "Step 15430 : Loss : 2.82782865\n",
            "Step 15440 : Loss : 2.82607961\n",
            "Step 15450 : Loss : 2.82433748\n",
            "Step 15460 : Loss : 2.82257843\n",
            "Step 15470 : Loss : 2.82079911\n",
            "Step 15480 : Loss : 2.81905842\n",
            "Step 15490 : Loss : 2.81729913\n",
            "Step 15500 : Loss : 2.81558585\n",
            "Step 15510 : Loss : 2.81386733\n",
            "Step 15520 : Loss : 2.81215596\n",
            "Step 15530 : Loss : 2.81044793\n",
            "Step 15540 : Loss : 2.80871964\n",
            "Step 15550 : Loss : 2.80699182\n",
            "Step 15560 : Loss : 2.80526\n",
            "Step 15570 : Loss : 2.80353022\n",
            "Step 15580 : Loss : 2.80182409\n",
            "Step 15590 : Loss : 2.80010176\n",
            "Step 15600 : Loss : 2.79841971\n",
            "Step 15610 : Loss : 2.79674649\n",
            "Step 15620 : Loss : 2.79507041\n",
            "Step 15630 : Loss : 2.79337668\n",
            "Step 15640 : Loss : 2.79168153\n",
            "Step 15650 : Loss : 2.78999424\n",
            "Step 15660 : Loss : 2.78827953\n",
            "Step 15670 : Loss : 2.78657508\n",
            "Step 15680 : Loss : 2.78487635\n",
            "Step 15690 : Loss : 2.78316307\n",
            "Step 15700 : Loss : 2.7814672\n",
            "Step 15710 : Loss : 2.77975869\n",
            "Step 15720 : Loss : 2.77805734\n",
            "Step 15730 : Loss : 2.77638483\n",
            "Step 15740 : Loss : 2.77469182\n",
            "Step 15750 : Loss : 2.77302694\n",
            "Step 15760 : Loss : 2.77135444\n",
            "Step 15770 : Loss : 2.7696836\n",
            "Step 15780 : Loss : 2.76798964\n",
            "Step 15790 : Loss : 2.76633263\n",
            "Step 15800 : Loss : 2.76465583\n",
            "Step 15810 : Loss : 2.762995\n",
            "Step 15820 : Loss : 2.76131392\n",
            "Step 15830 : Loss : 2.75966382\n",
            "Step 15840 : Loss : 2.75798464\n",
            "Step 15850 : Loss : 2.7563417\n",
            "Step 15860 : Loss : 2.75467944\n",
            "Step 15870 : Loss : 2.75305867\n",
            "Step 15880 : Loss : 2.75139165\n",
            "Step 15890 : Loss : 2.74975848\n",
            "Step 15900 : Loss : 2.74809599\n",
            "Step 15910 : Loss : 2.74641848\n",
            "Step 15920 : Loss : 2.74475312\n",
            "Step 15930 : Loss : 2.74311852\n",
            "Step 15930 : Loss :  2.74311852\n",
            "Save weights at epoch: 26\n",
            "Epoch 28\n",
            "Step 15940 : Loss : 2.74146199\n",
            "Step 15950 : Loss : 2.73979354\n",
            "Step 15960 : Loss : 2.73813081\n",
            "Step 15970 : Loss : 2.7364676\n",
            "Step 15980 : Loss : 2.73482895\n",
            "Step 15990 : Loss : 2.73318172\n",
            "Step 16000 : Loss : 2.73156118\n",
            "Step 16010 : Loss : 2.72991252\n",
            "Step 16020 : Loss : 2.72828245\n",
            "Step 16030 : Loss : 2.72666574\n",
            "Step 16040 : Loss : 2.7250483\n",
            "Step 16050 : Loss : 2.72342157\n",
            "Step 16060 : Loss : 2.7217803\n",
            "Step 16070 : Loss : 2.72016525\n",
            "Step 16080 : Loss : 2.71854568\n",
            "Step 16090 : Loss : 2.71692085\n",
            "Step 16100 : Loss : 2.71528077\n",
            "Step 16110 : Loss : 2.71367359\n",
            "Step 16120 : Loss : 2.71206141\n",
            "Step 16130 : Loss : 2.7104578\n",
            "Step 16140 : Loss : 2.70887399\n",
            "Step 16150 : Loss : 2.70729041\n",
            "Step 16160 : Loss : 2.70567584\n",
            "Step 16170 : Loss : 2.70407343\n",
            "Step 16180 : Loss : 2.70245433\n",
            "Step 16190 : Loss : 2.70083356\n",
            "Step 16200 : Loss : 2.69925928\n",
            "Step 16210 : Loss : 2.69770479\n",
            "Step 16220 : Loss : 2.69612956\n",
            "Step 16230 : Loss : 2.69453621\n",
            "Step 16240 : Loss : 2.69297028\n",
            "Step 16250 : Loss : 2.69137692\n",
            "Step 16260 : Loss : 2.68979073\n",
            "Step 16270 : Loss : 2.68823743\n",
            "Step 16280 : Loss : 2.68667388\n",
            "Step 16290 : Loss : 2.68510532\n",
            "Step 16300 : Loss : 2.6835351\n",
            "Step 16310 : Loss : 2.68198729\n",
            "Step 16320 : Loss : 2.6804359\n",
            "Step 16330 : Loss : 2.67885804\n",
            "Step 16340 : Loss : 2.67731738\n",
            "Step 16350 : Loss : 2.67575574\n",
            "Step 16360 : Loss : 2.67421031\n",
            "Step 16370 : Loss : 2.67264462\n",
            "Step 16380 : Loss : 2.67107153\n",
            "Step 16390 : Loss : 2.66951013\n",
            "Step 16400 : Loss : 2.66794634\n",
            "Step 16410 : Loss : 2.66638899\n",
            "Step 16420 : Loss : 2.66487479\n",
            "Step 16430 : Loss : 2.66334748\n",
            "Step 16440 : Loss : 2.66184282\n",
            "Step 16450 : Loss : 2.6603229\n",
            "Step 16460 : Loss : 2.65881133\n",
            "Step 16470 : Loss : 2.65729618\n",
            "Step 16480 : Loss : 2.65580916\n",
            "Step 16490 : Loss : 2.65431118\n",
            "Step 16500 : Loss : 2.6527915\n",
            "Step 16510 : Loss : 2.65130687\n",
            "Step 16520 : Loss : 2.64979243\n",
            "Step 16520 : Loss :  2.64979243\n",
            "Save weights at epoch: 27\n",
            "Epoch 29\n",
            "Step 16530 : Loss : 2.64828563\n",
            "Step 16540 : Loss : 2.64676595\n",
            "Step 16550 : Loss : 2.64525962\n",
            "Step 16560 : Loss : 2.64375091\n",
            "Step 16570 : Loss : 2.64226246\n",
            "Step 16580 : Loss : 2.64076638\n",
            "Step 16590 : Loss : 2.63926625\n",
            "Step 16600 : Loss : 2.63776088\n",
            "Step 16610 : Loss : 2.63627982\n",
            "Step 16620 : Loss : 2.63477278\n",
            "Step 16630 : Loss : 2.63327527\n",
            "Step 16640 : Loss : 2.63177419\n",
            "Step 16650 : Loss : 2.63027811\n",
            "Step 16660 : Loss : 2.6287787\n",
            "Step 16670 : Loss : 2.62727785\n",
            "Step 16680 : Loss : 2.62579298\n",
            "Step 16690 : Loss : 2.6243329\n",
            "Step 16700 : Loss : 2.62283587\n",
            "Step 16710 : Loss : 2.62135983\n",
            "Step 16720 : Loss : 2.61986828\n",
            "Step 16730 : Loss : 2.61835098\n",
            "Step 16740 : Loss : 2.61685944\n",
            "Step 16750 : Loss : 2.61538172\n",
            "Step 16760 : Loss : 2.61389923\n",
            "Step 16770 : Loss : 2.61238432\n",
            "Step 16780 : Loss : 2.61091113\n",
            "Step 16790 : Loss : 2.6094141\n",
            "Step 16800 : Loss : 2.60798621\n",
            "Step 16810 : Loss : 2.60651302\n",
            "Step 16820 : Loss : 2.60504627\n",
            "Step 16830 : Loss : 2.60358262\n",
            "Step 16840 : Loss : 2.60209441\n",
            "Step 16850 : Loss : 2.60062337\n",
            "Step 16860 : Loss : 2.59916282\n",
            "Step 16870 : Loss : 2.59770346\n",
            "Step 16880 : Loss : 2.59625363\n",
            "Step 16890 : Loss : 2.59477\n",
            "Step 16900 : Loss : 2.59331036\n",
            "Step 16910 : Loss : 2.59185028\n",
            "Step 16920 : Loss : 2.59038472\n",
            "Step 16930 : Loss : 2.58893943\n",
            "Step 16940 : Loss : 2.58749557\n",
            "Step 16950 : Loss : 2.58604074\n",
            "Step 16960 : Loss : 2.58458209\n",
            "Step 16970 : Loss : 2.58313084\n",
            "Step 16980 : Loss : 2.58166599\n",
            "Step 16990 : Loss : 2.580199\n",
            "Step 17000 : Loss : 2.57877016\n",
            "Step 17010 : Loss : 2.57734752\n",
            "Step 17020 : Loss : 2.57590127\n",
            "Step 17030 : Loss : 2.57446408\n",
            "Step 17040 : Loss : 2.57301617\n",
            "Step 17050 : Loss : 2.57158709\n",
            "Step 17060 : Loss : 2.57014036\n",
            "Step 17070 : Loss : 2.56872153\n",
            "Step 17080 : Loss : 2.56729293\n",
            "Step 17090 : Loss : 2.56586766\n",
            "Step 17100 : Loss : 2.56444335\n",
            "Step 17110 : Loss : 2.56302094\n",
            "Step 17110 : Loss :  2.56302094\n",
            "Save weights at epoch: 28\n",
            "Epoch 30\n",
            "Step 17120 : Loss : 2.561584\n",
            "Step 17130 : Loss : 2.56016421\n",
            "Step 17140 : Loss : 2.55872726\n",
            "Step 17150 : Loss : 2.55728483\n",
            "Step 17160 : Loss : 2.55585289\n",
            "Step 17170 : Loss : 2.55441785\n",
            "Step 17180 : Loss : 2.55298901\n",
            "Step 17190 : Loss : 2.5515542\n",
            "Step 17200 : Loss : 2.55012274\n",
            "Step 17210 : Loss : 2.54869914\n",
            "Step 17220 : Loss : 2.5472815\n",
            "Step 17230 : Loss : 2.54586959\n",
            "Step 17240 : Loss : 2.54446125\n",
            "Step 17250 : Loss : 2.54303455\n",
            "Step 17260 : Loss : 2.54160357\n",
            "Step 17270 : Loss : 2.54017663\n",
            "Step 17280 : Loss : 2.53877139\n",
            "Step 17290 : Loss : 2.53736901\n",
            "Step 17300 : Loss : 2.5359509\n",
            "Step 17310 : Loss : 2.53455496\n",
            "Step 17320 : Loss : 2.53315187\n",
            "Step 17330 : Loss : 2.53172612\n",
            "Step 17340 : Loss : 2.53030944\n",
            "Step 17350 : Loss : 2.52890658\n",
            "Step 17360 : Loss : 2.52750421\n",
            "Step 17370 : Loss : 2.52610159\n",
            "Step 17380 : Loss : 2.52471137\n",
            "Step 17390 : Loss : 2.52336216\n",
            "Step 17400 : Loss : 2.52198219\n",
            "Step 17410 : Loss : 2.52061462\n",
            "Step 17420 : Loss : 2.51923203\n",
            "Step 17430 : Loss : 2.51787138\n",
            "Step 17440 : Loss : 2.51651859\n",
            "Step 17450 : Loss : 2.51512671\n",
            "Step 17460 : Loss : 2.51377678\n",
            "Step 17470 : Loss : 2.51240039\n",
            "Step 17480 : Loss : 2.51104665\n",
            "Step 17490 : Loss : 2.50969768\n",
            "Step 17500 : Loss : 2.50833917\n",
            "Step 17510 : Loss : 2.50696921\n",
            "Step 17520 : Loss : 2.50563097\n",
            "Step 17530 : Loss : 2.50426102\n",
            "Step 17540 : Loss : 2.50292373\n",
            "Step 17550 : Loss : 2.50158668\n",
            "Step 17560 : Loss : 2.50023532\n",
            "Step 17570 : Loss : 2.49886966\n",
            "Step 17580 : Loss : 2.49753356\n",
            "Step 17590 : Loss : 2.49620724\n",
            "Step 17600 : Loss : 2.49484038\n",
            "Step 17610 : Loss : 2.49348974\n",
            "Step 17620 : Loss : 2.49214172\n",
            "Step 17630 : Loss : 2.49078631\n",
            "Step 17640 : Loss : 2.48944783\n",
            "Step 17650 : Loss : 2.48807764\n",
            "Step 17660 : Loss : 2.48673296\n",
            "Step 17670 : Loss : 2.48540378\n",
            "Step 17680 : Loss : 2.48404574\n",
            "Step 17690 : Loss : 2.48269343\n",
            "Step 17700 : Loss : 2.4813354\n",
            "Step 17700 : Loss :  2.4813354\n",
            "Save weights at epoch: 29\n",
            "Epoch 31\n",
            "Step 17710 : Loss : 2.4800086\n",
            "Step 17720 : Loss : 2.47867417\n",
            "Step 17730 : Loss : 2.47733474\n",
            "Step 17740 : Loss : 2.47598529\n",
            "Step 17750 : Loss : 2.47466\n",
            "Step 17760 : Loss : 2.47334456\n",
            "Step 17770 : Loss : 2.47200131\n",
            "Step 17780 : Loss : 2.47065187\n",
            "Step 17790 : Loss : 2.46933317\n",
            "Step 17800 : Loss : 2.46798015\n",
            "Step 17810 : Loss : 2.46665621\n",
            "Step 17820 : Loss : 2.46534276\n",
            "Step 17830 : Loss : 2.46400499\n",
            "Step 17840 : Loss : 2.46268868\n",
            "Step 17850 : Loss : 2.461375\n",
            "Step 17860 : Loss : 2.46006155\n",
            "Step 17870 : Loss : 2.45874643\n",
            "Step 17880 : Loss : 2.45745015\n",
            "Step 17890 : Loss : 2.45616817\n",
            "Step 17900 : Loss : 2.45485282\n",
            "Step 17910 : Loss : 2.45354867\n",
            "Step 17920 : Loss : 2.4522438\n",
            "Step 17930 : Loss : 2.45096111\n",
            "Step 17940 : Loss : 2.44967699\n",
            "Step 17950 : Loss : 2.4484036\n",
            "Step 17960 : Loss : 2.44714546\n",
            "Step 17970 : Loss : 2.44587588\n",
            "Step 17980 : Loss : 2.44460273\n",
            "Step 17990 : Loss : 2.44333148\n",
            "Step 18000 : Loss : 2.44203186\n",
            "Step 18010 : Loss : 2.44074154\n",
            "Step 18020 : Loss : 2.43945336\n",
            "Step 18030 : Loss : 2.43816185\n",
            "Step 18040 : Loss : 2.43690252\n",
            "Step 18050 : Loss : 2.43562555\n",
            "Step 18060 : Loss : 2.43435669\n",
            "Step 18070 : Loss : 2.43305969\n",
            "Step 18080 : Loss : 2.4318\n",
            "Step 18090 : Loss : 2.43055582\n",
            "Step 18100 : Loss : 2.42926741\n",
            "Step 18110 : Loss : 2.42798972\n",
            "Step 18120 : Loss : 2.42671919\n",
            "Step 18130 : Loss : 2.42546272\n",
            "Step 18140 : Loss : 2.42418075\n",
            "Step 18150 : Loss : 2.4229157\n",
            "Step 18160 : Loss : 2.42165685\n",
            "Step 18170 : Loss : 2.42038059\n",
            "Step 18180 : Loss : 2.41911387\n",
            "Step 18190 : Loss : 2.41782379\n",
            "Step 18200 : Loss : 2.41656089\n",
            "Step 18210 : Loss : 2.41529036\n",
            "Step 18220 : Loss : 2.4140358\n",
            "Step 18230 : Loss : 2.4127717\n",
            "Step 18240 : Loss : 2.41150379\n",
            "Step 18250 : Loss : 2.41026497\n",
            "Step 18260 : Loss : 2.40902662\n",
            "Step 18270 : Loss : 2.40777636\n",
            "Step 18280 : Loss : 2.40654302\n",
            "Step 18290 : Loss : 2.40531063\n",
            "Step 18290 : Loss :  2.40531063\n",
            "Save weights at epoch: 30\n",
            "Epoch 32\n",
            "Step 18300 : Loss : 2.40406752\n",
            "Step 18310 : Loss : 2.40281343\n",
            "Step 18320 : Loss : 2.40157104\n",
            "Step 18330 : Loss : 2.40031147\n",
            "Step 18340 : Loss : 2.39906287\n",
            "Step 18350 : Loss : 2.39784026\n",
            "Step 18360 : Loss : 2.39660215\n",
            "Step 18370 : Loss : 2.39541245\n",
            "Step 18380 : Loss : 2.39419365\n",
            "Step 18390 : Loss : 2.39299893\n",
            "Step 18400 : Loss : 2.39179802\n",
            "Step 18410 : Loss : 2.39058518\n",
            "Step 18420 : Loss : 2.38936162\n",
            "Step 18430 : Loss : 2.38816261\n",
            "Step 18440 : Loss : 2.38693714\n",
            "Step 18450 : Loss : 2.38571954\n",
            "Step 18460 : Loss : 2.38449526\n",
            "Step 18470 : Loss : 2.38327193\n",
            "Step 18480 : Loss : 2.3820622\n",
            "Step 18490 : Loss : 2.38085\n",
            "Step 18500 : Loss : 2.37964368\n",
            "Step 18510 : Loss : 2.37842226\n",
            "Step 18520 : Loss : 2.37721038\n",
            "Step 18530 : Loss : 2.37598729\n",
            "Step 18540 : Loss : 2.37474918\n",
            "Step 18550 : Loss : 2.37354445\n",
            "Step 18560 : Loss : 2.37234569\n",
            "Step 18570 : Loss : 2.37115\n",
            "Step 18580 : Loss : 2.36993718\n",
            "Step 18590 : Loss : 2.3687005\n",
            "Step 18600 : Loss : 2.36749339\n",
            "Step 18610 : Loss : 2.36627936\n",
            "Step 18620 : Loss : 2.36506152\n",
            "Step 18630 : Loss : 2.36385441\n",
            "Step 18640 : Loss : 2.36266971\n",
            "Step 18650 : Loss : 2.36149526\n",
            "Step 18660 : Loss : 2.36028433\n",
            "Step 18670 : Loss : 2.3590858\n",
            "Step 18680 : Loss : 2.35790753\n",
            "Step 18690 : Loss : 2.35670614\n",
            "Step 18700 : Loss : 2.35555339\n",
            "Step 18710 : Loss : 2.35435724\n",
            "Step 18720 : Loss : 2.35316896\n",
            "Step 18730 : Loss : 2.35196185\n",
            "Step 18740 : Loss : 2.35078311\n",
            "Step 18750 : Loss : 2.3495934\n",
            "Step 18760 : Loss : 2.34840322\n",
            "Step 18770 : Loss : 2.34721398\n",
            "Step 18780 : Loss : 2.34601212\n",
            "Step 18790 : Loss : 2.34483695\n",
            "Step 18800 : Loss : 2.34367299\n",
            "Step 18810 : Loss : 2.3425045\n",
            "Step 18820 : Loss : 2.34132409\n",
            "Step 18830 : Loss : 2.34015489\n",
            "Step 18840 : Loss : 2.33900476\n",
            "Step 18850 : Loss : 2.33783984\n",
            "Step 18860 : Loss : 2.33668923\n",
            "Step 18870 : Loss : 2.33551049\n",
            "Step 18880 : Loss : 2.33434629\n",
            "Step 18880 : Loss :  2.33434629\n",
            "Save weights at epoch: 31\n",
            "Epoch 33\n",
            "Step 18890 : Loss : 2.33317614\n",
            "Step 18900 : Loss : 2.33198977\n",
            "Step 18910 : Loss : 2.33082366\n",
            "Step 18920 : Loss : 2.32963538\n",
            "Step 18930 : Loss : 2.32847047\n",
            "Step 18940 : Loss : 2.32729888\n",
            "Step 18950 : Loss : 2.32613134\n",
            "Step 18960 : Loss : 2.324965\n",
            "Step 18970 : Loss : 2.32380223\n",
            "Step 18980 : Loss : 2.32262588\n",
            "Step 18990 : Loss : 2.32146525\n",
            "Step 19000 : Loss : 2.32029653\n",
            "Step 19010 : Loss : 2.31912875\n",
            "Step 19020 : Loss : 2.31794238\n",
            "Step 19030 : Loss : 2.31677222\n",
            "Step 19040 : Loss : 2.3155818\n",
            "Step 19050 : Loss : 2.3144362\n",
            "Step 19060 : Loss : 2.31327939\n",
            "Step 19070 : Loss : 2.31213808\n",
            "Step 19080 : Loss : 2.31100702\n",
            "Step 19090 : Loss : 2.30986142\n",
            "Step 19100 : Loss : 2.30870724\n",
            "Step 19110 : Loss : 2.30756235\n",
            "Step 19120 : Loss : 2.3064177\n",
            "Step 19130 : Loss : 2.30529475\n",
            "Step 19140 : Loss : 2.30419421\n",
            "Step 19150 : Loss : 2.30308318\n",
            "Step 19160 : Loss : 2.30199981\n",
            "Step 19170 : Loss : 2.30088234\n",
            "Step 19180 : Loss : 2.29977036\n",
            "Step 19190 : Loss : 2.29867268\n",
            "Step 19200 : Loss : 2.29756\n",
            "Step 19210 : Loss : 2.29642534\n",
            "Step 19220 : Loss : 2.29531741\n",
            "Step 19230 : Loss : 2.29417729\n",
            "Step 19240 : Loss : 2.29309821\n",
            "Step 19250 : Loss : 2.29196906\n",
            "Step 19260 : Loss : 2.29084587\n",
            "Step 19270 : Loss : 2.28974104\n",
            "Step 19280 : Loss : 2.28865957\n",
            "Step 19290 : Loss : 2.28753\n",
            "Step 19300 : Loss : 2.28641319\n",
            "Step 19310 : Loss : 2.28528142\n",
            "Step 19320 : Loss : 2.28415179\n",
            "Step 19330 : Loss : 2.28305244\n",
            "Step 19340 : Loss : 2.28194594\n",
            "Step 19350 : Loss : 2.28085256\n",
            "Step 19360 : Loss : 2.27976084\n",
            "Step 19370 : Loss : 2.27867651\n",
            "Step 19380 : Loss : 2.27757645\n",
            "Step 19390 : Loss : 2.27647734\n",
            "Step 19400 : Loss : 2.27540541\n",
            "Step 19410 : Loss : 2.27433228\n",
            "Step 19420 : Loss : 2.2732265\n",
            "Step 19430 : Loss : 2.27214\n",
            "Step 19440 : Loss : 2.27105427\n",
            "Step 19450 : Loss : 2.26996136\n",
            "Step 19460 : Loss : 2.26887488\n",
            "Step 19470 : Loss : 2.26777506\n",
            "Step 19470 : Loss :  2.26777506\n",
            "Save weights at epoch: 32\n",
            "Epoch 34\n",
            "Step 19480 : Loss : 2.26669812\n",
            "Step 19490 : Loss : 2.26560616\n",
            "Step 19500 : Loss : 2.26450706\n",
            "Step 19510 : Loss : 2.26342297\n",
            "Step 19520 : Loss : 2.26237512\n",
            "Step 19530 : Loss : 2.26128626\n",
            "Step 19540 : Loss : 2.26022363\n",
            "Step 19550 : Loss : 2.25913286\n",
            "Step 19560 : Loss : 2.25808525\n",
            "Step 19570 : Loss : 2.2570076\n",
            "Step 19580 : Loss : 2.25593805\n",
            "Step 19590 : Loss : 2.25483775\n",
            "Step 19600 : Loss : 2.25377083\n",
            "Step 19610 : Loss : 2.25267816\n",
            "Step 19620 : Loss : 2.25158787\n",
            "Step 19630 : Loss : 2.25050449\n",
            "Step 19640 : Loss : 2.2494607\n",
            "Step 19650 : Loss : 2.24841833\n",
            "Step 19660 : Loss : 2.24733233\n",
            "Step 19670 : Loss : 2.24625278\n",
            "Step 19680 : Loss : 2.24519253\n",
            "Step 19690 : Loss : 2.24410391\n",
            "Step 19700 : Loss : 2.24301863\n",
            "Step 19710 : Loss : 2.24194837\n",
            "Step 19720 : Loss : 2.24088359\n",
            "Step 19730 : Loss : 2.23981595\n",
            "Step 19740 : Loss : 2.23874855\n",
            "Step 19750 : Loss : 2.23771477\n",
            "Step 19760 : Loss : 2.23663139\n",
            "Step 19770 : Loss : 2.23557878\n",
            "Step 19780 : Loss : 2.23451257\n",
            "Step 19790 : Loss : 2.23344612\n",
            "Step 19800 : Loss : 2.23236179\n",
            "Step 19810 : Loss : 2.23132753\n",
            "Step 19820 : Loss : 2.23024344\n",
            "Step 19830 : Loss : 2.22918272\n",
            "Step 19840 : Loss : 2.22811723\n",
            "Step 19850 : Loss : 2.22706461\n",
            "Step 19860 : Loss : 2.22601867\n",
            "Step 19870 : Loss : 2.22495627\n",
            "Step 19880 : Loss : 2.22391558\n",
            "Step 19890 : Loss : 2.22285509\n",
            "Step 19900 : Loss : 2.22177291\n",
            "Step 19910 : Loss : 2.22069573\n",
            "Step 19920 : Loss : 2.21961117\n",
            "Step 19930 : Loss : 2.21854758\n",
            "Step 19940 : Loss : 2.2174685\n",
            "Step 19950 : Loss : 2.21640539\n",
            "Step 19960 : Loss : 2.21534467\n",
            "Step 19970 : Loss : 2.21430731\n",
            "Step 19980 : Loss : 2.21324921\n",
            "Step 19990 : Loss : 2.21218371\n",
            "Step 20000 : Loss : 2.21111345\n",
            "Step 20010 : Loss : 2.21004868\n",
            "Step 20020 : Loss : 2.20900226\n",
            "Step 20030 : Loss : 2.20794725\n",
            "Step 20040 : Loss : 2.20689631\n",
            "Step 20050 : Loss : 2.20583129\n",
            "Step 20060 : Loss : 2.20478535\n",
            "Step 20060 : Loss :  2.20478535\n",
            "Save weights at epoch: 33\n",
            "Epoch 35\n",
            "Step 20070 : Loss : 2.20372629\n",
            "Step 20080 : Loss : 2.20268345\n",
            "Step 20090 : Loss : 2.20163822\n",
            "Step 20100 : Loss : 2.2005837\n",
            "Step 20110 : Loss : 2.19954324\n",
            "Step 20120 : Loss : 2.19851351\n",
            "Step 20130 : Loss : 2.19746947\n",
            "Step 20140 : Loss : 2.19642949\n",
            "Step 20150 : Loss : 2.1953845\n",
            "Step 20160 : Loss : 2.19434595\n",
            "Step 20170 : Loss : 2.193295\n",
            "Step 20180 : Loss : 2.19225478\n",
            "Step 20190 : Loss : 2.19120908\n",
            "Step 20200 : Loss : 2.19018483\n",
            "Step 20210 : Loss : 2.18913245\n",
            "Step 20220 : Loss : 2.18809628\n",
            "Step 20230 : Loss : 2.18705058\n",
            "Step 20240 : Loss : 2.18601751\n",
            "Step 20250 : Loss : 2.18499613\n",
            "Step 20260 : Loss : 2.18395519\n",
            "Step 20270 : Loss : 2.18296719\n",
            "Step 20280 : Loss : 2.18193221\n",
            "Step 20290 : Loss : 2.18089271\n",
            "Step 20300 : Loss : 2.17987275\n",
            "Step 20310 : Loss : 2.17884755\n",
            "Step 20320 : Loss : 2.1778152\n",
            "Step 20330 : Loss : 2.17677617\n",
            "Step 20340 : Loss : 2.17577767\n",
            "Step 20350 : Loss : 2.17476392\n",
            "Step 20360 : Loss : 2.17371726\n",
            "Step 20370 : Loss : 2.17271137\n",
            "Step 20380 : Loss : 2.17167401\n",
            "Step 20390 : Loss : 2.17064476\n",
            "Step 20400 : Loss : 2.16960573\n",
            "Step 20410 : Loss : 2.16857362\n",
            "Step 20420 : Loss : 2.16757083\n",
            "Step 20430 : Loss : 2.16654181\n",
            "Step 20440 : Loss : 2.16551185\n",
            "Step 20450 : Loss : 2.16448975\n",
            "Step 20460 : Loss : 2.16347027\n",
            "Step 20470 : Loss : 2.16245151\n",
            "Step 20480 : Loss : 2.16142583\n",
            "Step 20490 : Loss : 2.1604259\n",
            "Step 20500 : Loss : 2.15939188\n",
            "Step 20510 : Loss : 2.1584065\n",
            "Step 20520 : Loss : 2.15740228\n",
            "Step 20530 : Loss : 2.15638566\n",
            "Step 20540 : Loss : 2.15536785\n",
            "Step 20550 : Loss : 2.15435743\n",
            "Step 20560 : Loss : 2.15334654\n",
            "Step 20570 : Loss : 2.15233183\n",
            "Step 20580 : Loss : 2.15132737\n",
            "Step 20590 : Loss : 2.15032387\n",
            "Step 20600 : Loss : 2.14933658\n",
            "Step 20610 : Loss : 2.14834738\n",
            "Step 20620 : Loss : 2.14734483\n",
            "Step 20630 : Loss : 2.1463542\n",
            "Step 20640 : Loss : 2.14535737\n",
            "Step 20650 : Loss : 2.14436412\n",
            "Step 20650 : Loss :  2.14436412\n",
            "Save weights at epoch: 34\n",
            "Epoch 36\n",
            "Step 20660 : Loss : 2.14339018\n",
            "Step 20670 : Loss : 2.14239931\n",
            "Step 20680 : Loss : 2.14140463\n",
            "Step 20690 : Loss : 2.1404283\n",
            "Step 20700 : Loss : 2.13946438\n",
            "Step 20710 : Loss : 2.13849258\n",
            "Step 20720 : Loss : 2.13751101\n",
            "Step 20730 : Loss : 2.13655257\n",
            "Step 20740 : Loss : 2.1355741\n",
            "Step 20750 : Loss : 2.1346221\n",
            "Step 20760 : Loss : 2.13368487\n",
            "Step 20770 : Loss : 2.13273144\n",
            "Step 20780 : Loss : 2.13177323\n",
            "Step 20790 : Loss : 2.13081765\n",
            "Step 20800 : Loss : 2.12984037\n",
            "Step 20810 : Loss : 2.12886429\n",
            "Step 20820 : Loss : 2.12790561\n",
            "Step 20830 : Loss : 2.12696242\n",
            "Step 20840 : Loss : 2.12600541\n",
            "Step 20850 : Loss : 2.12504649\n",
            "Step 20860 : Loss : 2.12409282\n",
            "Step 20870 : Loss : 2.12313032\n",
            "Step 20880 : Loss : 2.12220097\n",
            "Step 20890 : Loss : 2.1212635\n",
            "Step 20900 : Loss : 2.12030649\n",
            "Step 20910 : Loss : 2.11937904\n",
            "Step 20920 : Loss : 2.11844516\n",
            "Step 20930 : Loss : 2.11750627\n",
            "Step 20940 : Loss : 2.11655092\n",
            "Step 20950 : Loss : 2.11557364\n",
            "Step 20960 : Loss : 2.11461186\n",
            "Step 20970 : Loss : 2.11363745\n",
            "Step 20980 : Loss : 2.11267781\n",
            "Step 20990 : Loss : 2.11170077\n",
            "Step 21000 : Loss : 2.11073041\n",
            "Step 21010 : Loss : 2.1097765\n",
            "Step 21020 : Loss : 2.1088016\n",
            "Step 21030 : Loss : 2.10784984\n",
            "Step 21040 : Loss : 2.10691595\n",
            "Step 21050 : Loss : 2.10596418\n",
            "Step 21060 : Loss : 2.10500646\n",
            "Step 21070 : Loss : 2.10407948\n",
            "Step 21080 : Loss : 2.10315251\n",
            "Step 21090 : Loss : 2.10219979\n",
            "Step 21100 : Loss : 2.10128284\n",
            "Step 21110 : Loss : 2.10035133\n",
            "Step 21120 : Loss : 2.09941196\n",
            "Step 21130 : Loss : 2.09848571\n",
            "Step 21140 : Loss : 2.09756351\n",
            "Step 21150 : Loss : 2.09661913\n",
            "Step 21160 : Loss : 2.09566522\n",
            "Step 21170 : Loss : 2.0947473\n",
            "Step 21180 : Loss : 2.09379888\n",
            "Step 21190 : Loss : 2.09287453\n",
            "Step 21200 : Loss : 2.09193039\n",
            "Step 21210 : Loss : 2.09099865\n",
            "Step 21220 : Loss : 2.09008622\n",
            "Step 21230 : Loss : 2.08916378\n",
            "Step 21240 : Loss : 2.08825588\n",
            "Step 21240 : Loss :  2.08825588\n",
            "Save weights at epoch: 35\n",
            "Epoch 37\n",
            "Step 21250 : Loss : 2.08730817\n",
            "Step 21260 : Loss : 2.08639622\n",
            "Step 21270 : Loss : 2.08546782\n",
            "Step 21280 : Loss : 2.08454251\n",
            "Step 21290 : Loss : 2.08362889\n",
            "Step 21300 : Loss : 2.08269978\n",
            "Step 21310 : Loss : 2.08176541\n",
            "Step 21320 : Loss : 2.08084083\n",
            "Step 21330 : Loss : 2.0798974\n",
            "Step 21340 : Loss : 2.07897043\n",
            "Step 21350 : Loss : 2.07805347\n",
            "Step 21360 : Loss : 2.07711768\n",
            "Step 21370 : Loss : 2.0761981\n",
            "Step 21380 : Loss : 2.07527399\n",
            "Step 21390 : Loss : 2.0743587\n",
            "Step 21400 : Loss : 2.07343912\n",
            "Step 21410 : Loss : 2.07253265\n",
            "Step 21420 : Loss : 2.07163405\n",
            "Step 21430 : Loss : 2.07073188\n",
            "Step 21440 : Loss : 2.06981635\n",
            "Step 21450 : Loss : 2.06889653\n",
            "Step 21460 : Loss : 2.06798196\n",
            "Step 21470 : Loss : 2.06706452\n",
            "Step 21480 : Loss : 2.0661447\n",
            "Step 21490 : Loss : 2.0652132\n",
            "Step 21500 : Loss : 2.06429696\n",
            "Step 21510 : Loss : 2.06338191\n",
            "Step 21520 : Loss : 2.06247473\n",
            "Step 21530 : Loss : 2.06156158\n",
            "Step 21540 : Loss : 2.06063223\n",
            "Step 21550 : Loss : 2.05971122\n",
            "Step 21560 : Loss : 2.05878329\n",
            "Step 21570 : Loss : 2.05784845\n",
            "Step 21580 : Loss : 2.05693579\n",
            "Step 21590 : Loss : 2.05602479\n",
            "Step 21600 : Loss : 2.05511785\n",
            "Step 21610 : Loss : 2.05421066\n",
            "Step 21620 : Loss : 2.05331373\n",
            "Step 21630 : Loss : 2.05241179\n",
            "Step 21640 : Loss : 2.05152392\n",
            "Step 21650 : Loss : 2.05063319\n",
            "Step 21660 : Loss : 2.0497303\n",
            "Step 21670 : Loss : 2.04883552\n",
            "Step 21680 : Loss : 2.04794955\n",
            "Step 21690 : Loss : 2.04705644\n",
            "Step 21700 : Loss : 2.04614687\n",
            "Step 21710 : Loss : 2.04525971\n",
            "Step 21720 : Loss : 2.04436755\n",
            "Step 21730 : Loss : 2.04346514\n",
            "Step 21740 : Loss : 2.04257727\n",
            "Step 21750 : Loss : 2.04168725\n",
            "Step 21760 : Loss : 2.04081798\n",
            "Step 21770 : Loss : 2.03993177\n",
            "Step 21780 : Loss : 2.03904176\n",
            "Step 21790 : Loss : 2.0381813\n",
            "Step 21800 : Loss : 2.03729749\n",
            "Step 21810 : Loss : 2.03642106\n",
            "Step 21820 : Loss : 2.03553605\n",
            "Step 21830 : Loss : 2.03466272\n",
            "Step 21830 : Loss :  2.03466272\n",
            "Save weights at epoch: 36\n",
            "Epoch 38\n",
            "Step 21840 : Loss : 2.0337882\n",
            "Step 21850 : Loss : 2.0328989\n",
            "Step 21860 : Loss : 2.03206134\n",
            "Step 21870 : Loss : 2.03116\n",
            "Step 21880 : Loss : 2.03028512\n",
            "Step 21890 : Loss : 2.02941775\n",
            "Step 21900 : Loss : 2.02853036\n",
            "Step 21910 : Loss : 2.02765322\n",
            "Step 21920 : Loss : 2.02676892\n",
            "Step 21930 : Loss : 2.02589321\n",
            "Step 21940 : Loss : 2.02501488\n",
            "Step 21950 : Loss : 2.02412319\n",
            "Step 21960 : Loss : 2.02324867\n",
            "Step 21970 : Loss : 2.02237415\n",
            "Step 21980 : Loss : 2.02150846\n",
            "Step 21990 : Loss : 2.02064824\n",
            "Step 22000 : Loss : 2.0197742\n",
            "Step 22010 : Loss : 2.01889801\n",
            "Step 22020 : Loss : 2.0180347\n",
            "Step 22030 : Loss : 2.01715851\n",
            "Step 22040 : Loss : 2.01630545\n",
            "Step 22050 : Loss : 2.01542425\n",
            "Step 22060 : Loss : 2.01454353\n",
            "Step 22070 : Loss : 2.0136714\n",
            "Step 22080 : Loss : 2.0127852\n",
            "Step 22090 : Loss : 2.01192546\n",
            "Step 22100 : Loss : 2.01107121\n",
            "Step 22110 : Loss : 2.01022077\n",
            "Step 22120 : Loss : 2.00937104\n",
            "Step 22130 : Loss : 2.00849938\n",
            "Step 22140 : Loss : 2.00764871\n",
            "Step 22150 : Loss : 2.00679255\n",
            "Step 22160 : Loss : 2.00593901\n",
            "Step 22170 : Loss : 2.00508642\n",
            "Step 22180 : Loss : 2.00424457\n",
            "Step 22190 : Loss : 2.00337458\n",
            "Step 22200 : Loss : 2.00252438\n",
            "Step 22210 : Loss : 2.00165772\n",
            "Step 22220 : Loss : 2.00081301\n",
            "Step 22230 : Loss : 1.99997377\n",
            "Step 22240 : Loss : 1.9991188\n",
            "Step 22250 : Loss : 1.99826825\n",
            "Step 22260 : Loss : 1.99740422\n",
            "Step 22270 : Loss : 1.99654663\n",
            "Step 22280 : Loss : 1.99568105\n",
            "Step 22290 : Loss : 1.99480748\n",
            "Step 22300 : Loss : 1.99395859\n",
            "Step 22310 : Loss : 1.99314344\n",
            "Step 22320 : Loss : 1.99230337\n",
            "Step 22330 : Loss : 1.99145401\n",
            "Step 22340 : Loss : 1.9906323\n",
            "Step 22350 : Loss : 1.98976803\n",
            "Step 22360 : Loss : 1.98893833\n",
            "Step 22370 : Loss : 1.98810279\n",
            "Step 22380 : Loss : 1.98728251\n",
            "Step 22390 : Loss : 1.98643303\n",
            "Step 22400 : Loss : 1.98559833\n",
            "Step 22410 : Loss : 1.98475695\n",
            "Step 22420 : Loss : 1.98393595\n",
            "Step 22420 : Loss :  1.98393595\n",
            "Save weights at epoch: 37\n",
            "Epoch 39\n",
            "Step 22430 : Loss : 1.98311412\n",
            "Step 22440 : Loss : 1.98228037\n",
            "Step 22450 : Loss : 1.9814837\n",
            "Step 22460 : Loss : 1.98065507\n",
            "Step 22470 : Loss : 1.97985983\n",
            "Step 22480 : Loss : 1.97904456\n",
            "Step 22490 : Loss : 1.97821653\n",
            "Step 22500 : Loss : 1.97738791\n",
            "Step 22510 : Loss : 1.97656596\n",
            "Step 22520 : Loss : 1.97575068\n",
            "Step 22530 : Loss : 1.97493792\n",
            "Step 22540 : Loss : 1.97412956\n",
            "Step 22550 : Loss : 1.97330761\n",
            "Step 22560 : Loss : 1.97249293\n",
            "Step 22570 : Loss : 1.97170472\n",
            "Step 22580 : Loss : 1.97090411\n",
            "Step 22590 : Loss : 1.97009814\n",
            "Step 22600 : Loss : 1.96927488\n",
            "Step 22610 : Loss : 1.96844125\n",
            "Step 22620 : Loss : 1.96761298\n",
            "Step 22630 : Loss : 1.96680343\n",
            "Step 22640 : Loss : 1.96597147\n",
            "Step 22650 : Loss : 1.96515143\n",
            "Step 22660 : Loss : 1.96431422\n",
            "Step 22670 : Loss : 1.96348524\n",
            "Step 22680 : Loss : 1.96266139\n",
            "Step 22690 : Loss : 1.96184015\n",
            "Step 22700 : Loss : 1.96105778\n",
            "Step 22710 : Loss : 1.9602288\n",
            "Step 22720 : Loss : 1.95940268\n",
            "Step 22730 : Loss : 1.95857692\n",
            "Step 22740 : Loss : 1.95774639\n",
            "Step 22750 : Loss : 1.95691824\n",
            "Step 22760 : Loss : 1.9560883\n",
            "Step 22770 : Loss : 1.95528007\n",
            "Step 22780 : Loss : 1.9544636\n",
            "Step 22790 : Loss : 1.95364368\n",
            "Step 22800 : Loss : 1.95283687\n",
            "Step 22810 : Loss : 1.95201647\n",
            "Step 22820 : Loss : 1.95121384\n",
            "Step 22830 : Loss : 1.95041573\n",
            "Step 22840 : Loss : 1.94961452\n",
            "Step 22850 : Loss : 1.94881892\n",
            "Step 22860 : Loss : 1.94800055\n",
            "Step 22870 : Loss : 1.94719303\n",
            "Step 22880 : Loss : 1.94637287\n",
            "Step 22890 : Loss : 1.94555342\n",
            "Step 22900 : Loss : 1.94473374\n",
            "Step 22910 : Loss : 1.94392276\n",
            "Step 22920 : Loss : 1.94311297\n",
            "Step 22930 : Loss : 1.94231856\n",
            "Step 22940 : Loss : 1.94150472\n",
            "Step 22950 : Loss : 1.94070077\n",
            "Step 22960 : Loss : 1.93989408\n",
            "Step 22970 : Loss : 1.93910348\n",
            "Step 22980 : Loss : 1.93829966\n",
            "Step 22990 : Loss : 1.93749511\n",
            "Step 23000 : Loss : 1.93670058\n",
            "Step 23010 : Loss : 1.9359169\n",
            "Step 23010 : Loss :  1.9359169\n",
            "Save weights at epoch: 38\n",
            "Epoch 40\n",
            "Step 23020 : Loss : 1.93512201\n",
            "Step 23030 : Loss : 1.93434095\n",
            "Step 23040 : Loss : 1.93354309\n",
            "Step 23050 : Loss : 1.93275809\n",
            "Step 23060 : Loss : 1.93195939\n",
            "Step 23070 : Loss : 1.93116748\n",
            "Step 23080 : Loss : 1.93038869\n",
            "Step 23090 : Loss : 1.92959917\n",
            "Step 23100 : Loss : 1.92880118\n",
            "Step 23110 : Loss : 1.928002\n",
            "Step 23120 : Loss : 1.92721772\n",
            "Step 23130 : Loss : 1.92641592\n",
            "Step 23140 : Loss : 1.92561436\n",
            "Step 23150 : Loss : 1.9248147\n",
            "Step 23160 : Loss : 1.92401838\n",
            "Step 23170 : Loss : 1.92322302\n",
            "Step 23180 : Loss : 1.92241848\n",
            "Step 23190 : Loss : 1.9216367\n",
            "Step 23200 : Loss : 1.92083466\n",
            "Step 23210 : Loss : 1.92003798\n",
            "Step 23220 : Loss : 1.91924751\n",
            "Step 23230 : Loss : 1.9184525\n",
            "Step 23240 : Loss : 1.91765475\n",
            "Step 23250 : Loss : 1.91687429\n",
            "Step 23260 : Loss : 1.91608047\n",
            "Step 23270 : Loss : 1.91529667\n",
            "Step 23280 : Loss : 1.91451252\n",
            "Step 23290 : Loss : 1.91374886\n",
            "Step 23300 : Loss : 1.91295612\n",
            "Step 23310 : Loss : 1.91217577\n",
            "Step 23320 : Loss : 1.91139579\n",
            "Step 23330 : Loss : 1.91060829\n",
            "Step 23340 : Loss : 1.90982771\n",
            "Step 23350 : Loss : 1.90903342\n",
            "Step 23360 : Loss : 1.90825415\n",
            "Step 23370 : Loss : 1.90749156\n",
            "Step 23380 : Loss : 1.90671241\n",
            "Step 23390 : Loss : 1.90593266\n",
            "Step 23400 : Loss : 1.90516305\n",
            "Step 23410 : Loss : 1.90437579\n",
            "Step 23420 : Loss : 1.903602\n",
            "Step 23430 : Loss : 1.90281785\n",
            "Step 23440 : Loss : 1.90204811\n",
            "Step 23450 : Loss : 1.90127337\n",
            "Step 23460 : Loss : 1.90048957\n",
            "Step 23470 : Loss : 1.89971888\n",
            "Step 23480 : Loss : 1.89894104\n",
            "Step 23490 : Loss : 1.89816511\n",
            "Step 23500 : Loss : 1.89738333\n",
            "Step 23510 : Loss : 1.89660168\n",
            "Step 23520 : Loss : 1.89581871\n",
            "Step 23530 : Loss : 1.89505208\n",
            "Step 23540 : Loss : 1.8942647\n",
            "Step 23550 : Loss : 1.89348555\n",
            "Step 23560 : Loss : 1.89270759\n",
            "Step 23570 : Loss : 1.89192736\n",
            "Step 23580 : Loss : 1.89114785\n",
            "Step 23590 : Loss : 1.89037907\n",
            "Step 23600 : Loss : 1.88960552\n",
            "Step 23600 : Loss :  1.88960552\n",
            "Save weights at epoch: 39\n",
            "Epoch 41\n",
            "Step 23610 : Loss : 1.8888545\n",
            "Step 23620 : Loss : 1.88809168\n",
            "Step 23630 : Loss : 1.88731563\n",
            "Step 23640 : Loss : 1.88655066\n",
            "Step 23650 : Loss : 1.88577759\n",
            "Step 23660 : Loss : 1.88501525\n",
            "Step 23670 : Loss : 1.88425815\n",
            "Step 23680 : Loss : 1.8834933\n",
            "Step 23690 : Loss : 1.88273251\n",
            "Step 23700 : Loss : 1.88196647\n",
            "Step 23710 : Loss : 1.88121116\n",
            "Step 23720 : Loss : 1.88047051\n",
            "Step 23730 : Loss : 1.87970531\n",
            "Step 23740 : Loss : 1.87895465\n",
            "Step 23750 : Loss : 1.87819374\n",
            "Step 23760 : Loss : 1.87742496\n",
            "Step 23770 : Loss : 1.87665844\n",
            "Step 23780 : Loss : 1.87590516\n",
            "Step 23790 : Loss : 1.87514532\n",
            "Step 23800 : Loss : 1.87438583\n",
            "Step 23810 : Loss : 1.87364697\n",
            "Step 23820 : Loss : 1.87288702\n",
            "Step 23830 : Loss : 1.87213707\n",
            "Step 23840 : Loss : 1.87138951\n",
            "Step 23850 : Loss : 1.87064815\n",
            "Step 23860 : Loss : 1.86991906\n",
            "Step 23870 : Loss : 1.86916912\n",
            "Step 23880 : Loss : 1.86846244\n",
            "Step 23890 : Loss : 1.8677187\n",
            "Step 23900 : Loss : 1.86698318\n",
            "Step 23910 : Loss : 1.86625\n",
            "Step 23920 : Loss : 1.86550558\n",
            "Step 23930 : Loss : 1.86475909\n",
            "Step 23940 : Loss : 1.86401546\n",
            "Step 23950 : Loss : 1.86327386\n",
            "Step 23960 : Loss : 1.86253893\n",
            "Step 23970 : Loss : 1.86179876\n",
            "Step 23980 : Loss : 1.86105657\n",
            "Step 23990 : Loss : 1.86032808\n",
            "Step 24000 : Loss : 1.8595897\n",
            "Step 24010 : Loss : 1.85885811\n",
            "Step 24020 : Loss : 1.85814917\n",
            "Step 24030 : Loss : 1.85744119\n",
            "Step 24040 : Loss : 1.85672319\n",
            "Step 24050 : Loss : 1.85602355\n",
            "Step 24060 : Loss : 1.85530269\n",
            "Step 24070 : Loss : 1.85459244\n",
            "Step 24080 : Loss : 1.85388434\n",
            "Step 24090 : Loss : 1.85318482\n",
            "Step 24100 : Loss : 1.852458\n",
            "Step 24110 : Loss : 1.85174716\n",
            "Step 24120 : Loss : 1.85103297\n",
            "Step 24130 : Loss : 1.85029912\n",
            "Step 24140 : Loss : 1.84956813\n",
            "Step 24150 : Loss : 1.84883976\n",
            "Step 24160 : Loss : 1.8481102\n",
            "Step 24170 : Loss : 1.84739125\n",
            "Step 24180 : Loss : 1.84666455\n",
            "Step 24190 : Loss : 1.84593165\n",
            "Step 24190 : Loss :  1.84593165\n",
            "Save weights at epoch: 40\n",
            "Epoch 42\n",
            "Step 24200 : Loss : 1.84521496\n",
            "Step 24210 : Loss : 1.84449804\n",
            "Step 24220 : Loss : 1.843804\n",
            "Step 24230 : Loss : 1.84308243\n",
            "Step 24240 : Loss : 1.8423835\n",
            "Step 24250 : Loss : 1.84167814\n",
            "Step 24260 : Loss : 1.8410002\n",
            "Step 24270 : Loss : 1.84028912\n",
            "Step 24280 : Loss : 1.83958411\n",
            "Step 24290 : Loss : 1.83887291\n",
            "Step 24300 : Loss : 1.83818495\n",
            "Step 24310 : Loss : 1.83746302\n",
            "Step 24320 : Loss : 1.83675599\n",
            "Step 24330 : Loss : 1.83605313\n",
            "Step 24340 : Loss : 1.83535361\n",
            "Step 24350 : Loss : 1.83464873\n",
            "Step 24360 : Loss : 1.83394504\n",
            "Step 24370 : Loss : 1.83325171\n",
            "Step 24380 : Loss : 1.83254945\n",
            "Step 24390 : Loss : 1.83184373\n",
            "Step 24400 : Loss : 1.83113825\n",
            "Step 24410 : Loss : 1.83042216\n",
            "Step 24420 : Loss : 1.82974279\n",
            "Step 24430 : Loss : 1.82903433\n",
            "Step 24440 : Loss : 1.8283571\n",
            "Step 24450 : Loss : 1.82768917\n",
            "Step 24460 : Loss : 1.82701206\n",
            "Step 24470 : Loss : 1.8263582\n",
            "Step 24480 : Loss : 1.82568026\n",
            "Step 24490 : Loss : 1.82499027\n",
            "Step 24500 : Loss : 1.8242892\n",
            "Step 24510 : Loss : 1.82359922\n",
            "Step 24520 : Loss : 1.82290781\n",
            "Step 24530 : Loss : 1.82221425\n",
            "Step 24540 : Loss : 1.82151556\n",
            "Step 24550 : Loss : 1.82082283\n",
            "Step 24560 : Loss : 1.82012069\n",
            "Step 24570 : Loss : 1.81942713\n",
            "Step 24580 : Loss : 1.81873584\n",
            "Step 24590 : Loss : 1.81806314\n",
            "Step 24600 : Loss : 1.81739938\n",
            "Step 24610 : Loss : 1.81670773\n",
            "Step 24620 : Loss : 1.81600785\n",
            "Step 24630 : Loss : 1.815323\n",
            "Step 24640 : Loss : 1.81463206\n",
            "Step 24650 : Loss : 1.81393445\n",
            "Step 24660 : Loss : 1.8132453\n",
            "Step 24670 : Loss : 1.81255496\n",
            "Step 24680 : Loss : 1.81187212\n",
            "Step 24690 : Loss : 1.81117034\n",
            "Step 24700 : Loss : 1.8104918\n",
            "Step 24710 : Loss : 1.80979586\n",
            "Step 24720 : Loss : 1.8091048\n",
            "Step 24730 : Loss : 1.80842018\n",
            "Step 24740 : Loss : 1.80773056\n",
            "Step 24750 : Loss : 1.80704534\n",
            "Step 24760 : Loss : 1.80635917\n",
            "Step 24770 : Loss : 1.80567098\n",
            "Step 24780 : Loss : 1.80500484\n",
            "Step 24780 : Loss :  1.80500484\n",
            "Save weights at epoch: 41\n",
            "Epoch 43\n",
            "Step 24790 : Loss : 1.80432415\n",
            "Step 24800 : Loss : 1.80364168\n",
            "Step 24810 : Loss : 1.80296111\n",
            "Step 24820 : Loss : 1.80229783\n",
            "Step 24830 : Loss : 1.80160892\n",
            "Step 24840 : Loss : 1.80092549\n",
            "Step 24850 : Loss : 1.80023718\n",
            "Step 24860 : Loss : 1.79956317\n",
            "Step 24870 : Loss : 1.79887474\n",
            "Step 24880 : Loss : 1.79819965\n",
            "Step 24890 : Loss : 1.79752457\n",
            "Step 24900 : Loss : 1.79684687\n",
            "Step 24910 : Loss : 1.79615784\n",
            "Step 24920 : Loss : 1.79547775\n",
            "Step 24930 : Loss : 1.79477942\n",
            "Step 24940 : Loss : 1.7940855\n",
            "Step 24950 : Loss : 1.7934016\n",
            "Step 24960 : Loss : 1.79273403\n",
            "Step 24970 : Loss : 1.79204512\n",
            "Step 24980 : Loss : 1.79137087\n",
            "Step 24990 : Loss : 1.79069674\n",
            "Step 25000 : Loss : 1.79003143\n",
            "Step 25010 : Loss : 1.78936255\n",
            "Step 25020 : Loss : 1.78869355\n",
            "Step 25030 : Loss : 1.78800249\n",
            "Step 25040 : Loss : 1.78732085\n",
            "Step 25050 : Loss : 1.78664\n",
            "Step 25060 : Loss : 1.78596854\n",
            "Step 25070 : Loss : 1.78528953\n",
            "Step 25080 : Loss : 1.7845912\n",
            "Step 25090 : Loss : 1.78390837\n",
            "Step 25100 : Loss : 1.7832408\n",
            "Step 25110 : Loss : 1.78255427\n",
            "Step 25120 : Loss : 1.78187966\n",
            "Step 25130 : Loss : 1.78120959\n",
            "Step 25140 : Loss : 1.7805407\n",
            "Step 25150 : Loss : 1.77987683\n",
            "Step 25160 : Loss : 1.77922094\n",
            "Step 25170 : Loss : 1.77855349\n",
            "Step 25180 : Loss : 1.77787757\n",
            "Step 25190 : Loss : 1.77721727\n",
            "Step 25200 : Loss : 1.77654719\n",
            "Step 25210 : Loss : 1.77586865\n",
            "Step 25220 : Loss : 1.77519333\n",
            "Step 25230 : Loss : 1.77451956\n",
            "Step 25240 : Loss : 1.77384126\n",
            "Step 25250 : Loss : 1.77316833\n",
            "Step 25260 : Loss : 1.77249062\n",
            "Step 25270 : Loss : 1.77181411\n",
            "Step 25280 : Loss : 1.77114046\n",
            "Step 25290 : Loss : 1.7704761\n",
            "Step 25300 : Loss : 1.76980126\n",
            "Step 25310 : Loss : 1.7691375\n",
            "Step 25320 : Loss : 1.76846671\n",
            "Step 25330 : Loss : 1.76781094\n",
            "Step 25340 : Loss : 1.76713479\n",
            "Step 25350 : Loss : 1.76647007\n",
            "Step 25360 : Loss : 1.76580906\n",
            "Step 25370 : Loss : 1.76514912\n",
            "Step 25370 : Loss :  1.76514912\n",
            "Save weights at epoch: 42\n",
            "Epoch 44\n",
            "Step 25380 : Loss : 1.76448226\n",
            "Step 25390 : Loss : 1.76381183\n",
            "Step 25400 : Loss : 1.76314807\n",
            "Step 25410 : Loss : 1.7624917\n",
            "Step 25420 : Loss : 1.76182711\n",
            "Step 25430 : Loss : 1.76117218\n",
            "Step 25440 : Loss : 1.76051354\n",
            "Step 25450 : Loss : 1.75985515\n",
            "Step 25460 : Loss : 1.7591846\n",
            "Step 25470 : Loss : 1.75851357\n",
            "Step 25480 : Loss : 1.75785601\n",
            "Step 25490 : Loss : 1.75719464\n",
            "Step 25500 : Loss : 1.75652993\n",
            "Step 25510 : Loss : 1.7558701\n",
            "Step 25520 : Loss : 1.75520444\n",
            "Step 25530 : Loss : 1.7545377\n",
            "Step 25540 : Loss : 1.75388622\n",
            "Step 25550 : Loss : 1.75322282\n",
            "Step 25560 : Loss : 1.7525624\n",
            "Step 25570 : Loss : 1.75189304\n",
            "Step 25580 : Loss : 1.75124598\n",
            "Step 25590 : Loss : 1.75058186\n",
            "Step 25600 : Loss : 1.74991393\n",
            "Step 25610 : Loss : 1.74924433\n",
            "Step 25620 : Loss : 1.74857795\n",
            "Step 25630 : Loss : 1.74791706\n",
            "Step 25640 : Loss : 1.74725652\n",
            "Step 25650 : Loss : 1.74661827\n",
            "Step 25660 : Loss : 1.74596691\n",
            "Step 25670 : Loss : 1.74530566\n",
            "Step 25680 : Loss : 1.74464309\n",
            "Step 25690 : Loss : 1.74397612\n",
            "Step 25700 : Loss : 1.74331653\n",
            "Step 25710 : Loss : 1.74266624\n",
            "Step 25720 : Loss : 1.74201453\n",
            "Step 25730 : Loss : 1.74137533\n",
            "Step 25740 : Loss : 1.74072945\n",
            "Step 25750 : Loss : 1.74008417\n",
            "Step 25760 : Loss : 1.73943853\n",
            "Step 25770 : Loss : 1.73878956\n",
            "Step 25780 : Loss : 1.73814464\n",
            "Step 25790 : Loss : 1.73748958\n",
            "Step 25800 : Loss : 1.73684013\n",
            "Step 25810 : Loss : 1.73619735\n",
            "Step 25820 : Loss : 1.73554683\n",
            "Step 25830 : Loss : 1.73489416\n",
            "Step 25840 : Loss : 1.73426223\n",
            "Step 25850 : Loss : 1.73362291\n",
            "Step 25860 : Loss : 1.73298073\n",
            "Step 25870 : Loss : 1.73234546\n",
            "Step 25880 : Loss : 1.73171306\n",
            "Step 25890 : Loss : 1.73108888\n",
            "Step 25900 : Loss : 1.73046422\n",
            "Step 25910 : Loss : 1.72982836\n",
            "Step 25920 : Loss : 1.72920907\n",
            "Step 25930 : Loss : 1.72859085\n",
            "Step 25940 : Loss : 1.72797143\n",
            "Step 25950 : Loss : 1.72734571\n",
            "Step 25960 : Loss : 1.72671664\n",
            "Step 25960 : Loss :  1.72671664\n",
            "Save weights at epoch: 43\n",
            "Epoch 45\n",
            "Step 25970 : Loss : 1.72608888\n",
            "Step 25980 : Loss : 1.72546947\n",
            "Step 25990 : Loss : 1.72484374\n",
            "Step 26000 : Loss : 1.7242111\n",
            "Step 26010 : Loss : 1.72359264\n",
            "Step 26020 : Loss : 1.72297621\n",
            "Step 26030 : Loss : 1.72236693\n",
            "Step 26040 : Loss : 1.72175336\n",
            "Step 26050 : Loss : 1.72113931\n",
            "Step 26060 : Loss : 1.72051322\n",
            "Step 26070 : Loss : 1.71989989\n",
            "Step 26080 : Loss : 1.71928287\n",
            "Step 26090 : Loss : 1.71865594\n",
            "Step 26100 : Loss : 1.7180388\n",
            "Step 26110 : Loss : 1.71744394\n",
            "Step 26120 : Loss : 1.71683371\n",
            "Step 26130 : Loss : 1.71622181\n",
            "Step 26140 : Loss : 1.71562123\n",
            "Step 26150 : Loss : 1.71499979\n",
            "Step 26160 : Loss : 1.7143873\n",
            "Step 26170 : Loss : 1.7137841\n",
            "Step 26180 : Loss : 1.7131772\n",
            "Step 26190 : Loss : 1.71254861\n",
            "Step 26200 : Loss : 1.71193242\n",
            "Step 26210 : Loss : 1.71130717\n",
            "Step 26220 : Loss : 1.71070421\n",
            "Step 26230 : Loss : 1.71008933\n",
            "Step 26240 : Loss : 1.70949793\n",
            "Step 26250 : Loss : 1.7088871\n",
            "Step 26260 : Loss : 1.70828569\n",
            "Step 26270 : Loss : 1.70769477\n",
            "Step 26280 : Loss : 1.70708179\n",
            "Step 26290 : Loss : 1.70648122\n",
            "Step 26300 : Loss : 1.70586252\n",
            "Step 26310 : Loss : 1.70525765\n",
            "Step 26320 : Loss : 1.70466518\n",
            "Step 26330 : Loss : 1.70406568\n",
            "Step 26340 : Loss : 1.70345664\n",
            "Step 26350 : Loss : 1.70286739\n",
            "Step 26360 : Loss : 1.70227849\n",
            "Step 26370 : Loss : 1.70168555\n",
            "Step 26380 : Loss : 1.70107579\n",
            "Step 26390 : Loss : 1.7004689\n",
            "Step 26400 : Loss : 1.69986844\n",
            "Step 26410 : Loss : 1.69927073\n",
            "Step 26420 : Loss : 1.69866419\n",
            "Step 26430 : Loss : 1.69808114\n",
            "Step 26440 : Loss : 1.69749701\n",
            "Step 26450 : Loss : 1.69691122\n",
            "Step 26460 : Loss : 1.69631362\n",
            "Step 26470 : Loss : 1.69572246\n",
            "Step 26480 : Loss : 1.69512618\n",
            "Step 26490 : Loss : 1.69452751\n",
            "Step 26500 : Loss : 1.69393516\n",
            "Step 26510 : Loss : 1.69335496\n",
            "Step 26520 : Loss : 1.69277978\n",
            "Step 26530 : Loss : 1.69218647\n",
            "Step 26540 : Loss : 1.69159496\n",
            "Step 26550 : Loss : 1.69099712\n",
            "Step 26550 : Loss :  1.69099712\n",
            "Save weights at epoch: 44\n",
            "Epoch 46\n",
            "Step 26560 : Loss : 1.69040442\n",
            "Step 26570 : Loss : 1.68981791\n",
            "Step 26580 : Loss : 1.68922508\n",
            "Step 26590 : Loss : 1.68862271\n",
            "Step 26600 : Loss : 1.68804896\n",
            "Step 26610 : Loss : 1.68748176\n",
            "Step 26620 : Loss : 1.6869092\n",
            "Step 26630 : Loss : 1.68633354\n",
            "Step 26640 : Loss : 1.68576753\n",
            "Step 26650 : Loss : 1.6851716\n",
            "Step 26660 : Loss : 1.68460155\n",
            "Step 26670 : Loss : 1.68399537\n",
            "Step 26680 : Loss : 1.68340123\n",
            "Step 26690 : Loss : 1.6828084\n",
            "Step 26700 : Loss : 1.6822176\n",
            "Step 26710 : Loss : 1.68162596\n",
            "Step 26720 : Loss : 1.68105936\n",
            "Step 26730 : Loss : 1.6804589\n",
            "Step 26740 : Loss : 1.67989218\n",
            "Step 26750 : Loss : 1.67929912\n",
            "Step 26760 : Loss : 1.67872\n",
            "Step 26770 : Loss : 1.67813885\n",
            "Step 26780 : Loss : 1.6775471\n",
            "Step 26790 : Loss : 1.67696357\n",
            "Step 26800 : Loss : 1.67635667\n",
            "Step 26810 : Loss : 1.6757741\n",
            "Step 26820 : Loss : 1.67517173\n",
            "Step 26830 : Loss : 1.6746161\n",
            "Step 26840 : Loss : 1.67402196\n",
            "Step 26850 : Loss : 1.67343557\n",
            "Step 26860 : Loss : 1.67285132\n",
            "Step 26870 : Loss : 1.67225122\n",
            "Step 26880 : Loss : 1.67166138\n",
            "Step 26890 : Loss : 1.67107248\n",
            "Step 26900 : Loss : 1.67048192\n",
            "Step 26910 : Loss : 1.66990614\n",
            "Step 26920 : Loss : 1.66931868\n",
            "Step 26930 : Loss : 1.66872084\n",
            "Step 26940 : Loss : 1.66813409\n",
            "Step 26950 : Loss : 1.66753149\n",
            "Step 26960 : Loss : 1.66693854\n",
            "Step 26970 : Loss : 1.66635036\n",
            "Step 26980 : Loss : 1.66575646\n",
            "Step 26990 : Loss : 1.66516316\n",
            "Step 27000 : Loss : 1.66456568\n",
            "Step 27010 : Loss : 1.6639744\n",
            "Step 27020 : Loss : 1.6634022\n",
            "Step 27030 : Loss : 1.66282988\n",
            "Step 27040 : Loss : 1.66225612\n",
            "Step 27050 : Loss : 1.6616658\n",
            "Step 27060 : Loss : 1.66107881\n",
            "Step 27070 : Loss : 1.66050017\n",
            "Step 27080 : Loss : 1.65991437\n",
            "Step 27090 : Loss : 1.65932906\n",
            "Step 27100 : Loss : 1.6587466\n",
            "Step 27110 : Loss : 1.65816295\n",
            "Step 27120 : Loss : 1.65759683\n",
            "Step 27130 : Loss : 1.65701067\n",
            "Step 27140 : Loss : 1.65643442\n",
            "Step 27140 : Loss :  1.65643442\n",
            "Save weights at epoch: 45\n",
            "Epoch 47\n",
            "Step 27150 : Loss : 1.65585649\n",
            "Step 27160 : Loss : 1.65527773\n",
            "Step 27170 : Loss : 1.65470803\n",
            "Step 27180 : Loss : 1.654127\n",
            "Step 27190 : Loss : 1.65354848\n",
            "Step 27200 : Loss : 1.65297115\n",
            "Step 27210 : Loss : 1.65239012\n",
            "Step 27220 : Loss : 1.65180588\n",
            "Step 27230 : Loss : 1.65121758\n",
            "Step 27240 : Loss : 1.65063012\n",
            "Step 27250 : Loss : 1.65006435\n",
            "Step 27260 : Loss : 1.64948702\n",
            "Step 27270 : Loss : 1.64891875\n",
            "Step 27280 : Loss : 1.64833713\n",
            "Step 27290 : Loss : 1.64775145\n",
            "Step 27300 : Loss : 1.64717042\n",
            "Step 27310 : Loss : 1.64659846\n",
            "Step 27320 : Loss : 1.64602268\n",
            "Step 27330 : Loss : 1.64544523\n",
            "Step 27340 : Loss : 1.64486146\n",
            "Step 27350 : Loss : 1.64428079\n",
            "Step 27360 : Loss : 1.64370179\n",
            "Step 27370 : Loss : 1.64312255\n",
            "Step 27380 : Loss : 1.64255404\n",
            "Step 27390 : Loss : 1.641976\n",
            "Step 27400 : Loss : 1.64140236\n",
            "Step 27410 : Loss : 1.64083791\n",
            "Step 27420 : Loss : 1.64027715\n",
            "Step 27430 : Loss : 1.6397053\n",
            "Step 27440 : Loss : 1.63913834\n",
            "Step 27450 : Loss : 1.63856268\n",
            "Step 27460 : Loss : 1.63799632\n",
            "Step 27470 : Loss : 1.63742423\n",
            "Step 27480 : Loss : 1.63685763\n",
            "Step 27490 : Loss : 1.63630033\n",
            "Step 27500 : Loss : 1.63573301\n",
            "Step 27510 : Loss : 1.63516021\n",
            "Step 27520 : Loss : 1.63460457\n",
            "Step 27530 : Loss : 1.63405061\n",
            "Step 27540 : Loss : 1.63349271\n",
            "Step 27550 : Loss : 1.63293076\n",
            "Step 27560 : Loss : 1.6323657\n",
            "Step 27570 : Loss : 1.63181841\n",
            "Step 27580 : Loss : 1.63125026\n",
            "Step 27590 : Loss : 1.6306957\n",
            "Step 27600 : Loss : 1.63014424\n",
            "Step 27610 : Loss : 1.62958\n",
            "Step 27620 : Loss : 1.62902129\n",
            "Step 27630 : Loss : 1.62845814\n",
            "Step 27640 : Loss : 1.62789941\n",
            "Step 27650 : Loss : 1.62734687\n",
            "Step 27660 : Loss : 1.62678134\n",
            "Step 27670 : Loss : 1.62621307\n",
            "Step 27680 : Loss : 1.62565053\n",
            "Step 27690 : Loss : 1.62509203\n",
            "Step 27700 : Loss : 1.62454295\n",
            "Step 27710 : Loss : 1.62399089\n",
            "Step 27720 : Loss : 1.62343717\n",
            "Step 27730 : Loss : 1.62287891\n",
            "Step 27730 : Loss :  1.62287891\n",
            "Save weights at epoch: 46\n",
            "Epoch 48\n",
            "Step 27740 : Loss : 1.62232602\n",
            "Step 27750 : Loss : 1.62176466\n",
            "Step 27760 : Loss : 1.62120152\n",
            "Step 27770 : Loss : 1.62065506\n",
            "Step 27780 : Loss : 1.62010527\n",
            "Step 27790 : Loss : 1.61955285\n",
            "Step 27800 : Loss : 1.61899662\n",
            "Step 27810 : Loss : 1.61846125\n",
            "Step 27820 : Loss : 1.61790466\n",
            "Step 27830 : Loss : 1.61735535\n",
            "Step 27840 : Loss : 1.61680472\n",
            "Step 27850 : Loss : 1.61625028\n",
            "Step 27860 : Loss : 1.61570024\n",
            "Step 27870 : Loss : 1.61515367\n",
            "Step 27880 : Loss : 1.61460245\n",
            "Step 27890 : Loss : 1.61406124\n",
            "Step 27900 : Loss : 1.61351144\n",
            "Step 27910 : Loss : 1.61297655\n",
            "Step 27920 : Loss : 1.61243951\n",
            "Step 27930 : Loss : 1.61189353\n",
            "Step 27940 : Loss : 1.61134171\n",
            "Step 27950 : Loss : 1.61079359\n",
            "Step 27960 : Loss : 1.61024594\n",
            "Step 27970 : Loss : 1.60969746\n",
            "Step 27980 : Loss : 1.60914195\n",
            "Step 27990 : Loss : 1.60860133\n",
            "Step 28000 : Loss : 1.60805249\n",
            "Step 28010 : Loss : 1.60751474\n",
            "Step 28020 : Loss : 1.60696769\n",
            "Step 28030 : Loss : 1.60640931\n",
            "Step 28040 : Loss : 1.60587692\n",
            "Step 28050 : Loss : 1.60533273\n",
            "Step 28060 : Loss : 1.6047895\n",
            "Step 28070 : Loss : 1.60424566\n",
            "Step 28080 : Loss : 1.60369205\n",
            "Step 28090 : Loss : 1.60315597\n",
            "Step 28100 : Loss : 1.60261\n",
            "Step 28110 : Loss : 1.60206139\n",
            "Step 28120 : Loss : 1.6015172\n",
            "Step 28130 : Loss : 1.60098112\n",
            "Step 28140 : Loss : 1.60044825\n",
            "Step 28150 : Loss : 1.59991241\n",
            "Step 28160 : Loss : 1.59937513\n",
            "Step 28170 : Loss : 1.59883046\n",
            "Step 28180 : Loss : 1.59829855\n",
            "Step 28190 : Loss : 1.59774637\n",
            "Step 28200 : Loss : 1.59719646\n",
            "Step 28210 : Loss : 1.59664702\n",
            "Step 28220 : Loss : 1.59611535\n",
            "Step 28230 : Loss : 1.59557343\n",
            "Step 28240 : Loss : 1.59502864\n",
            "Step 28250 : Loss : 1.59448636\n",
            "Step 28260 : Loss : 1.59394228\n",
            "Step 28270 : Loss : 1.59338784\n",
            "Step 28280 : Loss : 1.59285069\n",
            "Step 28290 : Loss : 1.59231031\n",
            "Step 28300 : Loss : 1.59177065\n",
            "Step 28310 : Loss : 1.5912317\n",
            "Step 28320 : Loss : 1.59070361\n",
            "Step 28320 : Loss :  1.59070361\n",
            "Save weights at epoch: 47\n",
            "Epoch 49\n",
            "Step 28330 : Loss : 1.5901705\n",
            "Step 28340 : Loss : 1.58963764\n",
            "Step 28350 : Loss : 1.58910525\n",
            "Step 28360 : Loss : 1.5885669\n",
            "Step 28370 : Loss : 1.58804178\n",
            "Step 28380 : Loss : 1.58750033\n",
            "Step 28390 : Loss : 1.58697438\n",
            "Step 28400 : Loss : 1.58645272\n",
            "Step 28410 : Loss : 1.58593273\n",
            "Step 28420 : Loss : 1.58542573\n",
            "Step 28430 : Loss : 1.58490181\n",
            "Step 28440 : Loss : 1.58437443\n",
            "Step 28450 : Loss : 1.58385015\n",
            "Step 28460 : Loss : 1.5833348\n",
            "Step 28470 : Loss : 1.58280849\n",
            "Step 28480 : Loss : 1.58229792\n",
            "Step 28490 : Loss : 1.58177912\n",
            "Step 28500 : Loss : 1.58125818\n",
            "Step 28510 : Loss : 1.58072507\n",
            "Step 28520 : Loss : 1.58020484\n",
            "Step 28530 : Loss : 1.57969117\n",
            "Step 28540 : Loss : 1.57917714\n",
            "Step 28550 : Loss : 1.57866383\n",
            "Step 28560 : Loss : 1.57813907\n",
            "Step 28570 : Loss : 1.57762742\n",
            "Step 28580 : Loss : 1.57713234\n",
            "Step 28590 : Loss : 1.5766207\n",
            "Step 28600 : Loss : 1.57612669\n",
            "Step 28610 : Loss : 1.57561958\n",
            "Step 28620 : Loss : 1.57509553\n",
            "Step 28630 : Loss : 1.57459033\n",
            "Step 28640 : Loss : 1.57407153\n",
            "Step 28650 : Loss : 1.5735532\n",
            "Step 28660 : Loss : 1.57304311\n",
            "Step 28670 : Loss : 1.57253087\n",
            "Step 28680 : Loss : 1.57202303\n",
            "Step 28690 : Loss : 1.57149744\n",
            "Step 28700 : Loss : 1.57099617\n",
            "Step 28710 : Loss : 1.57049048\n",
            "Step 28720 : Loss : 1.5699898\n",
            "Step 28730 : Loss : 1.56947339\n",
            "Step 28740 : Loss : 1.56898332\n",
            "Step 28750 : Loss : 1.56847346\n",
            "Step 28760 : Loss : 1.56795919\n",
            "Step 28770 : Loss : 1.56746268\n",
            "Step 28780 : Loss : 1.56694901\n",
            "Step 28790 : Loss : 1.56643653\n",
            "Step 28800 : Loss : 1.56591475\n",
            "Step 28810 : Loss : 1.56540763\n",
            "Step 28820 : Loss : 1.56492829\n",
            "Step 28830 : Loss : 1.56443155\n",
            "Step 28840 : Loss : 1.5639571\n",
            "Step 28850 : Loss : 1.56347597\n",
            "Step 28860 : Loss : 1.56297076\n",
            "Step 28870 : Loss : 1.56248879\n",
            "Step 28880 : Loss : 1.56198585\n",
            "Step 28890 : Loss : 1.56146979\n",
            "Step 28900 : Loss : 1.56095457\n",
            "Step 28910 : Loss : 1.56045127\n",
            "Step 28910 : Loss :  1.56045127\n",
            "Save weights at epoch: 48\n",
            "Epoch 50\n",
            "Step 28920 : Loss : 1.55993867\n",
            "Step 28930 : Loss : 1.55942488\n",
            "Step 28940 : Loss : 1.5589118\n",
            "Step 28950 : Loss : 1.55840075\n",
            "Step 28960 : Loss : 1.55789042\n",
            "Step 28970 : Loss : 1.55738425\n",
            "Step 28980 : Loss : 1.55686975\n",
            "Step 28990 : Loss : 1.55637825\n",
            "Step 29000 : Loss : 1.55587757\n",
            "Step 29010 : Loss : 1.5554024\n",
            "Step 29020 : Loss : 1.55490625\n",
            "Step 29030 : Loss : 1.55440629\n",
            "Step 29040 : Loss : 1.55389524\n",
            "Step 29050 : Loss : 1.55339134\n",
            "Step 29060 : Loss : 1.55289316\n",
            "Step 29070 : Loss : 1.55238533\n",
            "Step 29080 : Loss : 1.55188942\n",
            "Step 29090 : Loss : 1.55138803\n",
            "Step 29100 : Loss : 1.5508976\n",
            "Step 29110 : Loss : 1.55039394\n",
            "Step 29120 : Loss : 1.54989541\n",
            "Step 29130 : Loss : 1.54939222\n",
            "Step 29140 : Loss : 1.5489043\n",
            "Step 29150 : Loss : 1.54840577\n",
            "Step 29160 : Loss : 1.54790592\n",
            "Step 29170 : Loss : 1.54741275\n",
            "Step 29180 : Loss : 1.5469203\n",
            "Step 29190 : Loss : 1.5464282\n",
            "Step 29200 : Loss : 1.54594135\n",
            "Step 29210 : Loss : 1.5454433\n",
            "Step 29220 : Loss : 1.54495168\n",
            "Step 29230 : Loss : 1.54444051\n",
            "Step 29240 : Loss : 1.54395103\n",
            "Step 29250 : Loss : 1.54345\n",
            "Step 29260 : Loss : 1.54294753\n",
            "Step 29270 : Loss : 1.5424608\n",
            "Step 29280 : Loss : 1.54196823\n",
            "Step 29290 : Loss : 1.5414809\n",
            "Step 29300 : Loss : 1.54099488\n",
            "Step 29310 : Loss : 1.54050028\n",
            "Step 29320 : Loss : 1.54000282\n",
            "Step 29330 : Loss : 1.53949809\n",
            "Step 29340 : Loss : 1.53900397\n",
            "Step 29350 : Loss : 1.53849876\n",
            "Step 29360 : Loss : 1.53800273\n",
            "Step 29370 : Loss : 1.53749883\n",
            "Step 29380 : Loss : 1.53700674\n",
            "Step 29390 : Loss : 1.53651869\n",
            "Step 29400 : Loss : 1.5360297\n",
            "Step 29410 : Loss : 1.53553128\n",
            "Step 29420 : Loss : 1.53503144\n",
            "Step 29430 : Loss : 1.53453171\n",
            "Step 29440 : Loss : 1.53403449\n",
            "Step 29450 : Loss : 1.53354347\n",
            "Step 29460 : Loss : 1.53305805\n",
            "Step 29470 : Loss : 1.53257227\n",
            "Step 29480 : Loss : 1.53208804\n",
            "Step 29490 : Loss : 1.5315913\n",
            "Step 29500 : Loss : 1.53109848\n",
            "Step 29500 : Loss :  1.53109848\n",
            "Save weights at epoch: 49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqo8N4K8x8j_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp bin/2blocks_model_weights.h5 /content/gdrive/My\\ Drive/AppliedML/2blocks_model_weights.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ0si25oCwY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "654cca71-56aa-4819-ef46-ec2c6a2fd1e5"
      },
      "source": [
        "!python conversion.py --config config/resnet.2blocks.json"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:TensorFlow version 2.0.0 detected. Last version known to be fully compatible is 1.14.0 .\n",
            "WARNING:root:Keras version 2.2.5 detected. Last version known to be fully compatible of Keras is 2.2.4 .\n",
            "2020-01-09 20:42:40.197307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-01-09 20:42:40.200261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:40.201117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-01-09 20:42:40.201448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-01-09 20:42:40.203040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-01-09 20:42:40.214675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-01-09 20:42:40.215118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-01-09 20:42:40.216742: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-01-09 20:42:40.224981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-01-09 20:42:40.233100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-01-09 20:42:40.233283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:40.234410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:40.235282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-01-09 20:42:40.235643: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-01-09 20:42:40.240617: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2020-01-09 20:42:40.240861: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x26df480 executing computations on platform Host. Devices:\n",
            "2020-01-09 20:42:40.240918: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
            "2020-01-09 20:42:40.349365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:40.350374: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x26df640 executing computations on platform CUDA. Devices:\n",
            "2020-01-09 20:42:40.350404: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-01-09 20:42:40.350657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:40.351433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-01-09 20:42:40.351514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-01-09 20:42:40.351544: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-01-09 20:42:40.351573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-01-09 20:42:40.351598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-01-09 20:42:40.351632: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-01-09 20:42:40.351664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-01-09 20:42:40.351693: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-01-09 20:42:40.351779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:40.352618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:40.353370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-01-09 20:42:40.353445: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-01-09 20:42:40.355056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-01-09 20:42:40.355087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-01-09 20:42:40.355100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-01-09 20:42:40.355266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:40.356318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:40.357082: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-01-09 20:42:40.357140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14769 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "2020-01-09 20:42:43.493834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:43.494732: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
            "2020-01-09 20:42:43.494863: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
            "2020-01-09 20:42:43.495720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:43.496572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-01-09 20:42:43.496728: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-01-09 20:42:43.496760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-01-09 20:42:43.496811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-01-09 20:42:43.496843: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-01-09 20:42:43.496921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-01-09 20:42:43.496976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-01-09 20:42:43.497011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-01-09 20:42:43.497119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:43.498068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:43.499001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-01-09 20:42:43.499045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-01-09 20:42:43.499060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-01-09 20:42:43.499077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-01-09 20:42:43.499202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:43.500108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:43.500974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14769 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2020-01-09 20:42:43.505049: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\n",
            "2020-01-09 20:42:43.505081: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.004ms.\n",
            "2020-01-09 20:42:43.505092: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\n",
            "0 assert nodes deleted\n",
            "2020-01-09 20:42:45.110614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:45.111471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-01-09 20:42:45.111541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-01-09 20:42:45.111569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-01-09 20:42:45.111592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-01-09 20:42:45.111617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-01-09 20:42:45.111640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-01-09 20:42:45.111662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-01-09 20:42:45.111686: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-01-09 20:42:45.111767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:45.112814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:45.113659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-01-09 20:42:45.113702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-01-09 20:42:45.113721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-01-09 20:42:45.113731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-01-09 20:42:45.113842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:45.114723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-01-09 20:42:45.115579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14769 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "['model/batch_normalization_3/FusedBatchNormV3/ReadVariableOp:0', 'model/res2a_branch2a/BiasAdd/ReadVariableOp:0', 'model/res1a_branch2b/BiasAdd/ReadVariableOp:0', 'model/batch_normalization_3/ReadVariableOp/resource:0', 'model/batch_normalization_4/ReadVariableOp_1:0', 'model/batch_normalization_8/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/batch_normalization_13/ReadVariableOp_1/resource:0', 'model/batch_normalization_5/FusedBatchNormV3/ReadVariableOp_1:0', 'model/conv2d_3/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_1/ReadVariableOp:0', 'model/batch_normalization_6/ReadVariableOp_1:0', 'model/res1a_branch2a/Conv2D/ReadVariableOp/resource:0', 'model/conv2d_3/Conv2D/ReadVariableOp:0', 'model/batch_normalization_5/ReadVariableOp/resource:0', 'model/batch_normalization_1/ReadVariableOp_1/resource:0', 'model/batch_normalization/FusedBatchNormV3/ReadVariableOp:0', 'model/batch_normalization_1/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/batch_normalization_11/ReadVariableOp_1/resource:0', 'model/batch_normalization_13/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/res1b_branch2b/Conv2D/ReadVariableOp:0', 'model/batch_normalization_2/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization_9/ReadVariableOp:0', 'model/batch_normalization_2/FusedBatchNormV3/ReadVariableOp:0', 'model/batch_normalization_7/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/res2b_branch2b/BiasAdd/ReadVariableOp/resource:0', 'model/batch_normalization_2/ReadVariableOp/resource:0', 'model/batch_normalization/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/batch_normalization_8/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization_6/ReadVariableOp:0', 'model/batch_normalization_5/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/res3a_branch2a/BiasAdd/ReadVariableOp:0', 'model/batch_normalization_2/ReadVariableOp:0', 'model/batch_normalization/ReadVariableOp_1/resource:0', 'model/conv2d_1/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_1/ReadVariableOp_1:0', 'model/batch_normalization_12/ReadVariableOp_1/resource:0', 'model/batch_normalization_12/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_4/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_14/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/time_distributed/softmax/MatMul/ReadVariableOp/resource:0', 'model/batch_normalization_13/ReadVariableOp/resource:0', 'model/batch_normalization_11/ReadVariableOp/resource:0', 'model/batch_normalization_10/ReadVariableOp_1/resource:0', 'model/batch_normalization/ReadVariableOp/resource:0', 'model/batch_normalization_14/ReadVariableOp:0', 'model/batch_normalization/ReadVariableOp:0', 'model/batch_normalization_7/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_9/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization_3/ReadVariableOp:0', 'model/conv2d/Conv2D/ReadVariableOp:0', 'model/res2b_branch2a/BiasAdd/ReadVariableOp/resource:0', 'model/batch_normalization_12/ReadVariableOp:0', 'model/conv2d/BiasAdd/ReadVariableOp:0', 'model/res2a_branch2b/BiasAdd/ReadVariableOp/resource:0', 'model/res1b_branch2b/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_2/ReadVariableOp_1/resource:0', 'model/res1b_branch2b/BiasAdd/ReadVariableOp:0', 'model/res3b_branch2a/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_14/ReadVariableOp_1:0', 'model/batch_normalization_11/ReadVariableOp:0', 'model/batch_normalization_5/ReadVariableOp:0', 'model/batch_normalization_14/FusedBatchNormV3/ReadVariableOp:0', 'model/time_distributed/Reshape_1/shape:0', 'model/res2b_branch2b/Conv2D/ReadVariableOp:0', 'model/batch_normalization_11/FusedBatchNormV3/ReadVariableOp_1:0', 'model/res1b_branch2a/Conv2D/ReadVariableOp:0', 'model/res2a_branch2a/BiasAdd/ReadVariableOp/resource:0', 'model/res1b_branch2a/Conv2D/ReadVariableOp/resource:0', 'model/res3a_branch2b/BiasAdd/ReadVariableOp:0', 'model/res3a_branch2b/BiasAdd/ReadVariableOp/resource:0', 'model/batch_normalization_11/ReadVariableOp_1:0', 'model/res3b_branch2b/Conv2D/ReadVariableOp:0', 'model/batch_normalization_2/ReadVariableOp_1:0', 'model/conv2d_1/Conv2D/ReadVariableOp:0', 'model/batch_normalization_6/ReadVariableOp/resource:0', 'model/batch_normalization_1/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization_12/FusedBatchNormV3/ReadVariableOp:0', 'model/res2a_branch2a/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_14/ReadVariableOp/resource:0', 'model/res2b_branch2b/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_3/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/batch_normalization_8/FusedBatchNormV3/ReadVariableOp:0', 'model/batch_normalization_5/FusedBatchNormV3/ReadVariableOp:0', 'model/batch_normalization_10/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_10/FusedBatchNormV3/ReadVariableOp:0', 'model/conv2d_2/Conv2D/ReadVariableOp:0', 'model/res3b_branch2a/Conv2D/ReadVariableOp:0', 'model/res1a_branch2b/BiasAdd/ReadVariableOp/resource:0', 'model/res1b_branch2a/BiasAdd/ReadVariableOp/resource:0', 'model/batch_normalization_13/FusedBatchNormV3/ReadVariableOp:0', 'model/conv2d/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_6/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization_11/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization_2/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_8/ReadVariableOp/resource:0', 'model/batch_normalization_14/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization_8/ReadVariableOp_1:0', 'model/batch_normalization_5/ReadVariableOp_1:0', 'model/batch_normalization_5/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization/ReadVariableOp_1:0', 'model/batch_normalization_2/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/res3b_branch2b/BiasAdd/ReadVariableOp/resource:0', 'model/res3a_branch2b/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_3/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/conv2d_3/BiasAdd/ReadVariableOp/resource:0', 'model/batch_normalization_13/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/res2b_branch2a/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_4/FusedBatchNormV3/ReadVariableOp:0', 'model/res3a_branch2b/Conv2D/ReadVariableOp:0', 'model/res3a_branch2a/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_9/FusedBatchNormV3/ReadVariableOp:0', 'model/batch_normalization_10/ReadVariableOp_1:0', 'model/batch_normalization_13/ReadVariableOp:0', 'model/batch_normalization_14/ReadVariableOp_1/resource:0', 'model/res1a_branch2a/BiasAdd/ReadVariableOp/resource:0', 'model/batch_normalization_7/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization_6/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/time_distributed/softmax/BiasAdd/ReadVariableOp:0', 'model/res2b_branch2a/Conv2D/ReadVariableOp:0', 'model/batch_normalization_12/ReadVariableOp_1:0', 'model/res2b_branch2a/BiasAdd/ReadVariableOp:0', 'model/conv2d_2/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_7/FusedBatchNormV3/ReadVariableOp:0', 'model/batch_normalization_6/FusedBatchNormV3/ReadVariableOp:0', 'model/res3a_branch2a/BiasAdd/ReadVariableOp/resource:0', 'model/conv2d_1/BiasAdd/ReadVariableOp/resource:0', 'model/res3b_branch2a/BiasAdd/ReadVariableOp/resource:0', 'model/batch_normalization_6/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_12/ReadVariableOp/resource:0', 'model/batch_normalization_8/ReadVariableOp:0', 'model/time_distributed/Reshape/shape:0', 'model/res2a_branch2b/Conv2D/ReadVariableOp:0', 'model/batch_normalization_7/ReadVariableOp_1:0', 'model/batch_normalization_9/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/res1a_branch2b/Conv2D/ReadVariableOp:0', 'model/batch_normalization_4/ReadVariableOp/resource:0', 'model/batch_normalization_4/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/conv2d_3/BiasAdd/ReadVariableOp:0', 'model/res1b_branch2b/BiasAdd/ReadVariableOp/resource:0', 'model/res1a_branch2a/BiasAdd/ReadVariableOp:0', 'model/batch_normalization_10/ReadVariableOp:0', 'model/batch_normalization_5/ReadVariableOp_1/resource:0', 'model/batch_normalization_10/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/time_distributed/softmax/BiasAdd/ReadVariableOp/resource:0', 'model/batch_normalization_7/ReadVariableOp/resource:0', 'model/res3a_branch2a/Conv2D/ReadVariableOp:0', 'model/batch_normalization_8/ReadVariableOp_1/resource:0', 'model/conv2d_1/BiasAdd/ReadVariableOp:0', 'model/batch_normalization_11/FusedBatchNormV3/ReadVariableOp:0', 'model/res2b_branch2b/BiasAdd/ReadVariableOp:0', 'model/batch_normalization_4/ReadVariableOp_1/resource:0', 'model/batch_normalization_4/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization_9/ReadVariableOp_1/resource:0', 'model/res1a_branch2a/Conv2D/ReadVariableOp:0', 'model/res2a_branch2b/Conv2D/ReadVariableOp/resource:0', 'model/time_distributed/softmax/MatMul/ReadVariableOp:0', 'model/batch_normalization_3/ReadVariableOp_1:0', 'model/batch_normalization_11/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/batch_normalization_13/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_7/ReadVariableOp:0', 'model/batch_normalization_12/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/res1a_branch2b/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_7/ReadVariableOp_1/resource:0', 'model/batch_normalization/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_1/ReadVariableOp/resource:0', 'model/res1b_branch2a/BiasAdd/ReadVariableOp:0', 'model/conv2d_2/BiasAdd/ReadVariableOp:0', 'model/conv2d_2/BiasAdd/ReadVariableOp/resource:0', 'model/batch_normalization_8/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_4/ReadVariableOp:0', 'model/batch_normalization_1/FusedBatchNormV3/ReadVariableOp:0', 'model/batch_normalization_3/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_12/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/batch_normalization_9/ReadVariableOp/resource:0', 'model/batch_normalization_1/FusedBatchNormV3/ReadVariableOp_1:0', 'model/res3b_branch2a/BiasAdd/ReadVariableOp:0', 'model/batch_normalization_14/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_9/ReadVariableOp_1:0', 'model/batch_normalization_10/FusedBatchNormV3/ReadVariableOp/resource:0', 'model/batch_normalization_9/FusedBatchNormV3/ReadVariableOp_1:0', 'model/batch_normalization_13/ReadVariableOp_1:0', 'model/batch_normalization_3/ReadVariableOp_1/resource:0', 'model/batch_normalization_6/ReadVariableOp_1/resource:0', 'model/conv2d/BiasAdd/ReadVariableOp/resource:0', 'model/batch_normalization/FusedBatchNormV3/ReadVariableOp_1/resource:0', 'model/res2a_branch2a/Conv2D/ReadVariableOp:0', 'model/res3b_branch2b/BiasAdd/ReadVariableOp:0', 'model/res2a_branch2b/BiasAdd/ReadVariableOp:0', 'model/res3b_branch2b/Conv2D/ReadVariableOp/resource:0', 'model/batch_normalization_10/ReadVariableOp/resource:0']\n",
            "94 nodes deleted\n",
            "45 nodes deleted\n",
            "0 nodes deleted\n",
            "[Op Fusion] fuse_bias_add() deleted 34 nodes.\n",
            "1 identity nodes deleted\n",
            "17 disconnected nodes deleted\n",
            "[Op Fusion] Fused 90 nodes into BatchNorms.\n",
            "[SSAConverter] Converting function main ...\n",
            "[SSAConverter] [1/60] Converting op type: 'Placeholder', name: 'input_1', output_shape: (-1, 32, 200, 3).\n",
            "[SSAConverter] [2/60] Converting op type: 'Const', name: 'model/time_distributed/Reshape/shape', output_shape: (2,).\n",
            "[SSAConverter] [3/60] Converting op type: 'Const', name: 'model/time_distributed/Reshape_1/shape', output_shape: (3,).\n",
            "[SSAConverter] [4/60] Converting op type: 'Transpose', name: 'input_1_to_nchw', output_shape: (-1, 3, 32, 200).\n",
            "[SSAConverter] [5/60] Converting op type: 'Conv2D', name: 'model/conv2d/Conv2D', output_shape: (-1, 64, 16, 100).\n",
            "[SSAConverter] [6/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 64, 16, 100).\n",
            "[SSAConverter] [7/60] Converting op type: 'Relu', name: 'model/re_lu/Relu', output_shape: (-1, 64, 16, 100).\n",
            "[SSAConverter] [8/60] Converting op type: 'MaxPool', name: 'model/max_pooling2d/MaxPool', output_shape: (-1, 64, 8, 50).\n",
            "[SSAConverter] [9/60] Converting op type: 'Conv2D', name: 'model/conv2d_1/Conv2D', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [10/60] Converting op type: 'Conv2D', name: 'model/res1a_branch2a/Conv2D', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [11/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_2/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [12/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_1/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [13/60] Converting op type: 'Relu', name: 'model/re_lu_1/Relu', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [14/60] Converting op type: 'Conv2D', name: 'model/res1a_branch2b/Conv2D', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [15/60] Converting op type: 'AddV2', name: 'model/add/add', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [16/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_3/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [17/60] Converting op type: 'Relu', name: 'model/re_lu_2/Relu', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [18/60] Converting op type: 'Conv2D', name: 'model/res1b_branch2a/Conv2D', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [19/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_4/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [20/60] Converting op type: 'Relu', name: 'model/re_lu_3/Relu', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [21/60] Converting op type: 'Conv2D', name: 'model/res1b_branch2b/Conv2D', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [22/60] Converting op type: 'AddV2', name: 'model/add_1/add', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [23/60] Converting op type: 'Conv2D', name: 'model/conv2d_2/Conv2D', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [24/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_5/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [25/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_7/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [26/60] Converting op type: 'Relu', name: 'model/re_lu_4/Relu', output_shape: (-1, 128, 8, 50).\n",
            "[SSAConverter] [27/60] Converting op type: 'Conv2D', name: 'model/res2a_branch2a/Conv2D', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [28/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_6/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [29/60] Converting op type: 'Relu', name: 'model/re_lu_5/Relu', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [30/60] Converting op type: 'Conv2D', name: 'model/res2a_branch2b/Conv2D', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [31/60] Converting op type: 'AddV2', name: 'model/add_2/add', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [32/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_8/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [33/60] Converting op type: 'Relu', name: 'model/re_lu_6/Relu', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [34/60] Converting op type: 'Conv2D', name: 'model/res2b_branch2a/Conv2D', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [35/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_9/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [36/60] Converting op type: 'Relu', name: 'model/re_lu_7/Relu', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [37/60] Converting op type: 'Conv2D', name: 'model/res2b_branch2b/Conv2D', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [38/60] Converting op type: 'AddV2', name: 'model/add_3/add', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [39/60] Converting op type: 'Conv2D', name: 'model/conv2d_3/Conv2D', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [40/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_10/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [41/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_12/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [42/60] Converting op type: 'Relu', name: 'model/re_lu_8/Relu', output_shape: (-1, 256, 4, 50).\n",
            "[SSAConverter] [43/60] Converting op type: 'Conv2D', name: 'model/res3a_branch2a/Conv2D', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [44/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_11/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [45/60] Converting op type: 'Relu', name: 'model/re_lu_9/Relu', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [46/60] Converting op type: 'Conv2D', name: 'model/res3a_branch2b/Conv2D', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [47/60] Converting op type: 'AddV2', name: 'model/add_4/add', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [48/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_13/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [49/60] Converting op type: 'Relu', name: 'model/re_lu_10/Relu', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [50/60] Converting op type: 'Conv2D', name: 'model/res3b_branch2a/Conv2D', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [51/60] Converting op type: 'BatchNorm', name: 'model/batch_normalization_14/FusedBatchNormV3/Add_batch_norm', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [52/60] Converting op type: 'Relu', name: 'model/re_lu_11/Relu', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [53/60] Converting op type: 'Conv2D', name: 'model/res3b_branch2b/Conv2D', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [54/60] Converting op type: 'AddV2', name: 'model/add_5/add', output_shape: (-1, 384, 1, 50).\n",
            "[SSAConverter] [55/60] Converting op type: 'Transpose', name: 'model/add_5/add_to_nhwc', output_shape: (-1, 1, 50, 384).\n",
            "[SSAConverter] [56/60] Converting op type: 'Squeeze', name: 'model/lambda/Squeeze', output_shape: (-1, 50, 384).\n",
            "[SSAConverter] [57/60] Converting op type: 'Reshape', name: 'model/time_distributed/Reshape', output_shape: (-1, 384).\n",
            "[SSAConverter] [58/60] Converting op type: 'MatMul', name: 'model/time_distributed/softmax/MatMul', output_shape: (-1, 91).\n",
            "[SSAConverter] [59/60] Converting op type: 'Softmax', name: 'model/time_distributed/softmax/Softmax', output_shape: (-1, 91).\n",
            "[SSAConverter] [60/60] Converting op type: 'Reshape', name: 'Identity', output_shape: (-1, 50, 91).\n",
            "[Core ML Pass] 2 disconnected constants nodes deleted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOHQsJkhi8Ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import coremltools\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF-sQNKaB-58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlmodel = coremltools.models.MLModel(\"bin/2blocks_coreml_weights.mlmodel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogV3rawiCW0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f0a52881-cc2f-4fb1-ba42-dd46aa6a09f4"
      },
      "source": [
        "labels = pd.read_csv(\"data/labels.csv\")\n",
        "labels.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>text</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cfcd208495d565ef66e7dff9f98764da</td>\n",
              "      <td>hibernates</td>\n",
              "      <td>112</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>c4ca4238a0b923820dcc509a6f75849b</td>\n",
              "      <td>muscles</td>\n",
              "      <td>79</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c81e728d9d4c2f636f067f89cc14862c</td>\n",
              "      <td>domination</td>\n",
              "      <td>109</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>eccbc87e4b5ce2fe28308fd9f2a7baf3</td>\n",
              "      <td>sitars</td>\n",
              "      <td>59</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a87ff679a2f3e71d9181a67b7542122c</td>\n",
              "      <td>fuzzes</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           image_id        text  width  height\n",
              "0  cfcd208495d565ef66e7dff9f98764da  hibernates    112      32\n",
              "1  c4ca4238a0b923820dcc509a6f75849b     muscles     79      32\n",
              "2  c81e728d9d4c2f636f067f89cc14862c  domination    109      32\n",
              "3  eccbc87e4b5ce2fe28308fd9f2a7baf3      sitars     59      32\n",
              "4  a87ff679a2f3e71d9181a67b7542122c      fuzzes     64      32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAQiR2beDrff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "481f0d37-5717-4c9d-dc43-78db9762b462"
      },
      "source": [
        "labels.loc[0, \"image_id\"]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cfcd208495d565ef66e7dff9f98764da'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADqJySrtDjwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "15480f20-721e-48be-a21d-d7fd14ed4e57"
      },
      "source": [
        "img = cv2.imread(f\"data/images/{labels.loc[0, 'image_id']}.png\")\n",
        "plt.imshow(img)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd1c2db1320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACBCAYAAADZoOE3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAASOUlEQVR4nO3dfZAcxXkG8OfRSWCd5ICELkIRqggn\nCg5FYj5OWASDMSDrsJ3IfzjGlCuWq0hwHBLg+JQBg20+jIFAnBRxRQUEQREIxsQoxCWsKNhCBAmd\ngGC+IRCMKIEOMOJDFEbizR87mu2TtnU9OzO703PPr2rqeudmp3um9/pm3u3uoZlBRETiM67bBRAR\nkfaoARcRiZQacBGRSKkBFxGJlBpwEZFIqQEXEYlUrgac5ADJp0g+S3JxUYUSEZHRsd1+4CR7ADwN\nYD6ADQDWATjBzB4vrngiIuIzPsd7DwXwrJk9BwAkbwWwEIC3AZ82bZrNnj07R5YiksX69evT9CGH\nHNLFkrRJ4wwBAOsfXP+qmfXtuD5PAz4TwIvO6w0APr6rN8yePRtDa4dyZBkBNygV8uFjjrzyvFfG\nBLL5IRkaCvjb+8B9c0AGWRvYsrevKU7gC63Wl/4lJsmTSA6RHBoeHi47OxGRMSNPA/4SgFnO632S\ndSOY2RIz6zez/r6+ne4ARKQFkumSh31g6SL1k6cBXwdgDsl9Se4G4EsAlhVTLBERGU3bMXAz20ry\nrwHcDaAHwPVm9lhhJRMRkV3K8yUmzOwnAH5SUFkkQu4tvqYmLo7OpYTQSEwRkUipARcRiVSuEMqY\nENIJIGNHAfZ4wg5l9xsvgW71RbpHV+AiIpFSAy4iEqnOh1DKDAFkHcaelW+f7nqOnvaGHZxt1Luj\nXrpWn1n/3vJMBVGF7avAd85LaJ90BS4iEik14CIikapXL5Q8tyUF9TYZcaucY/4JhU3qpYz6zByW\n8YX6Ytm+hHAKJzjn8P2AnfpCpO5qt5fZVmefH7TYOCddgYuIREoNuIhIpKrdCyVPj5UuDYrpZOgj\nxp4qMZa5qtzzF3Res37ey94+T6+MgFBGyD5HhDh8IR1fOT37H7FPV44Bfz66AhcRiZQacBGRSFW6\nF0qu2+2KzRlShm6FIPLUi8Im5ch8XkN6gHRyEE3WHim+9+bhC8uE9B7JOngnJLyzzQmRecIpugIX\nEYmUGnARkUhVuheKbrerSfVSDXlCWRxXwiCgPCGXsucxyspXhoy9X0bU0bbiD0xX4CIikVIDLiIS\nqeh7oVRtYEjVyhMLnbfs8pwn75OgQsIjQRnkeG8VppAtaKDQCCVcLo+6S5LXk9xE8lFn3VSSK0g+\nk/ycUnzRRERkV0L+J9wAYGCHdYsBrDSzOQBWJq9FRKSDRm3AzWwVgNd3WL0QwNIkvRTA5wsu1/a8\n0yXPNp1UtfLEoqrnjWS61BadJc82ZTBnycMpP8czXUYc1zhncdebZ/Hs393PiM+1bz8hi0e7UZnp\nZrYxSb8MYHqb+xERkTblDqtb45LJ+z+C5Ekkh0gODQ8P581OREQS7Tbgr5CcAQDJz02+Dc1siZn1\nm1l/X19fun5M3JpK9IoK7dTq8x4S1nDDEXn4whq+pcdZfO/1yREqCQq5hJTft+zi9LRjGYBFSXoR\ngDvb3I+IiLQppBvhLQDuB7AfyQ0kTwRwGYD5JJ8BcGzyWkREOmjUgTxmdoLnV8fkybhqvQ2qoKjB\nLO+89lqanrTXXrnKFIMYBgFVtVxtyTpviW9QTMbpVYMETP0a9LDxPJGuDkbJNJReRCRSasBFRCJV\n6blQQvT29o66zZYtWzpQkl1zb/MnTpyYpt2yZb3N3mPypDT9vue2sArHXrZahSdik3XekqzTtHrC\nLO4TakaERIp6mk+euVw6SFfgIiKRUgMuIhKp6EMosYQI3LBJUTa//U7L9SFhpbEmhp4q0XNDDduc\ndNbLxIAwi3c63JAQTcj6SOgKXEQkUmrARUQiFX0IxeWGDqoWWimqPL5QwOeOyTWuqvYUNikujBSy\nn94P5/hbdDuVjPP0NqlAL5EqhOV0BS4iEik14CIikapVCGUs8N2qbd26tdR8q3C7mFWMZS5TUQ8G\nL/1c+nqbVEwVyqYrcBGRSKkBFxGJVKVDKGXcAvsGuVxy0cVpevCM0wvJK6QMvm/osx57yBfxeY49\nz/n35vudi5r5nnlG2/v3nasF8xe0Va6svSZ8+Z/ytb9M09fedGMheYXIOj9Qnro97tPNc/zz1fe2\nvZ+iDDjlWeUpTxnnfMHAQJq+d9Wqltt899JL0vSppw0Wkq+uwEVEIqUGXEQkUpUOoZQRNnn+uefT\n9PS9p7fc5th5H0/Tf3D44YXkm1XWYw8JofhuHXsnNaelzRM+GvybU9L0P113bZp++8230vS48T3N\nfJ3zc94F3xy1nD5nf/3klvv0+c1p03bKx33fujVr0vTcefPS9NVX/G2zvBc2y+ub58YNm3jPfZ7B\nZ+81ex71TvmNNP3W65vTdM+HJhSer3sefr56dcv9jDif992fpj853xlw5nxot7zTfigxayisqAF/\nAwuOS9P33tsMm4Tk+41zz0vTk5z1w6++mqkMugIXEYmUGnARkUixk53R+/v7bWhoqLT9F9WroJQ5\nVbY1n7ba++HJhe//s588Mk3fs655jkP2f/PS5q3+X3y92XOik+fNfe+7776bpovq/TJaeYoq+4L5\nn07T/3bnj0d978CnjkrTF11yWZqe+0fzWmztz9cVUv6y5w0qe/++3iYheS1w3vs7c34vTf/jNf+Q\nqQxFfWZC3ktyvZn177h+1CtwkrNI3kPycZKPkTw1WT+V5AqSzyQ/p2Q6AhERySUkhLIVwBlmtj+A\neQBOJrk/gMUAVprZHAArk9ciItIho/ZCMbONADYm6bdIPgFgJoCFAI5KNlsK4GcAzimllG2o2nSy\n8Dx0uCjjrP2vM7686Ctp2g2hPLBmbZo+1OmZ47rq0u+2na/ryCOaIaDldy8fdfuiB3kV9XkJCZu4\nVq19IE37wiaa02VnWcMmrrt/eneadkMZISEU38PJn3vxpTT9kVkzM5Unj0x/9SRnAzgIwFoA05PG\nHQBeBjDd856TSA6RHBoeHs5RVBERcQU34CQnA/gRgNPM7E33d9a4LGh5aWBmS8ys38z6+/r6chVW\nRESaggbykJyARuN9s5ndkax+heQMM9tIcgaATWUVshacXihlGF/CU0l8YRPX+Rc35zM59MAD285r\nlTMQIuQb+k6FEqoQvvDlW7kwYeTWrVmXpufOm9tyG7cuHrnnZ2n6gP3mpOkjjzgiTS9fdlfzzbs3\nB1UVVXchvVAI4DoAT5jZVc6vlgFYlKQXAbizkBKJiEiQkCvwwwH8GYBfkHw4WXcugMsA3EbyRAAv\nAPhiOUUUEZFWQnqhrIZ/qg09STdUySGUnm492dVx+ZVXp+k/Pv6ENL3y37PdnFUpNBBLr48lV1yZ\npk+78IKW27gDpHzzt4xQgQcHh9i85b00PWNatuEo557d7P3sC5v4/KEzCMv9zO65xx5punfKHmjl\nsouaU8ueckb7U8tqKL2ISKTUgIuIRKrS08nWyraAhw777tYDbl/He9/cOUcd+6k0XaUwSF3lmQ8k\naKrjqoVNPB/xpx96ME1X4XP3xubNLdd/9Qt/mqYXf7M5naxCKCIiY5AacBGRSFUuhNLRgRN5vmV3\nO5W47/UVOU8vlIDQSk+X/hfPP+jgNL3CuZUtQxUG1VRJnvlAusWtQ5e3Pn1/Z465hx/m7Gj07Yty\n9eXNJxMNnj36Q7lvuP2Hafo2dw6Wq76fpv/q9FMzlUFX4CIikVIDLiISqcqFUDoaNglZ7/JFQcyT\ndm7hOLX50FnvIIoch94Tcrvou73Mke+d9zUfapvnIc4u31wosYVNFPLZWdB5cP/OAi4xL/7Wd9L0\n+d9qPYhpZCGayd5J7T9V5/RzzkzTISEUn5tuvCFNK4QiIjJGqAEXEYlU5UIoXgHfRnv5QhxZ5Qi/\nLLuiOZHj8Rec13qjHGXLXJFOXhzX+ikjIe91ubegWW9r3bDJ26+1HgjRdRl7OMQYNnEHB7kPaB48\nq/0QQUheWXvUuNu4n52gEEpBvVNC/lZ8YTR3Xpr7H36o7TLoClxEJFJqwEVEIlXtEEpAyKJ3cuue\nD+5t1ZFHNefoWL7sP9L0wIBzC/ffzVs4l3sLdNWll6fpwbPOal025/ZsQX9zkMu9Tz3ZcvNZe++d\npk8fbN6mDp7p2b/LOQ+3PrAmTR+w4Lg0PeI8fKL5pBD3ltW9FRxx+xoSMvJcApx/evNYQnqnnHt2\n83nY4yZO2MWWOxuY79Tjfa3r0S3DJRc2niI0eE62sMDAgmZIYdXq1S236Z082cnnwjTtfl5858NX\nV8udh/C63Lr62hePT9MvvPFGmr57xU/TtK+efeXx5evyhQjc/X/1xD9P07fd8i8t95NnINKWXzWf\n8DhpytQ0fcTc5vSw7ue9qHxdH7zQfKjx5N9vPp3HPefueS6qJ5quwEVEIqUGXEQkUuzkN+X9/f02\nNDQU/oaQouX5RjnjgIERtgVs05Nxn1n377j6yivS9OA5Tvgl4zGOuCX+IKACqjblqMQpkqf/dAvJ\n9WbWv+N6XYGLiERKDbiISKSq3QslRNY5THy3Z+42RUWVMoZB8hjRa8XN1z3egBltg8ImHnnm/tC8\nIdVQtXqoWnmqZtQrcJIfIvkAyf8h+RjJbyfr9yW5luSzJP+V5G7lF1dERLYLCaG8B+BoM/sYgAMB\nDJCcB+B7AK42s98F8CsAJ5ZXTBER2dGoDbg1vJ28nJAsBuBoALcn65cC+HwpJZRomFm6dPK9LpLp\nItkVVQ+Z0VmqUJ5IBH2JSbKH5MMANgFYAeB/AbxhZtsftb4BwEzPe08iOURyaHh4uIgyi4gIAhtw\nM9tmZgcC2AfAoQA+GpqBmS0xs34z6+/r62uzmCIisqNM3QjN7A0A9wA4DMCeJLf3YtkHwEveN4p0\niG65I2XOIsFCeqH0kdwzSU8EMB/AE2g05F9INlsE4M6yCikiIjsL6Qc+A8BSkj1oNPi3mdldJB8H\ncCvJiwE8BOC6EsspIiI76OhcKCSHAbwD4NWOZdp906DjrbOxdLxj6ViBah3vb5vZTl8idrQBBwCS\nQ60mZakrHW+9jaXjHUvHCsRxvJoLRUQkUmrARUQi1Y0GfEkX8uwmHW+9jaXjHUvHCkRwvB2PgYuI\nSDEUQhERiVRHG3CSAySfSqagXdzJvDuB5CyS95B8PJl699Rk/VSSK0g+k/yc0u2yFiWZJ+chkncl\nr2s7zTDJPUneTvJJkk+QPKzmdTuYfI4fJXlLMrV0beqX5PUkN5F81FnXsj7Z8PfJcT9C8uDulbyp\nYw14MhDoGgDHAdgfwAkk9+9U/h2yFcAZZrY/gHkATk6OcTGAlWY2B8DK5HVdnIrGyNzt6jzN8PcB\nLDezjwL4GBrHXcu6JTkTwCkA+s3sADSe8Pol1Kt+bwAwsMM6X30eB2BOspwE4AcdKuMudfIK/FAA\nz5rZc2b2awC3AljYwfxLZ2YbzezBJP0WGn/gM9E4zqXJZrWZepfkPgA+C+Da5DVR02mGSe4B4Egk\nI47N7NfJ3EC1rNvEeAATkzmPegFsRI3q18xWAXh9h9W++lwI4MZkeu01aMwFNaMzJfXrZAM+E8CL\nzmvvFLR1QHI2gIMArAUw3cw2Jr96GcD0LhWraH8H4Gw0H9a2FwKnGY7QvgCGAfxzEjK6luQk1LRu\nzewlAFcC+CUaDfdmAOtR3/rdzleflWy/9CVmCUhOBvAjAKeZ2Zvu76zR7Sf6rj8kPwdgk5mt73ZZ\nOmQ8gIMB/MDMDkJjSogR4ZK61C0AJLHfhWj84/otAJOwc7ih1mKoz0424C8BmOW8ruUUtCQnoNF4\n32xmdySrX9l+u5X83NSt8hXocAB/QvL/0AiHHY1GjLiu0wxvALDBzNYmr29Ho0GvY90CwLEAnjez\nYTN7H8AdaNR5Xet3O199VrL96mQDvg7AnORb7N3Q+EJkWQfzL10SA74OwBNmdpXzq2VoTLkL1GTq\nXTP7hpntY2az0ajL/zKzL6Om0wyb2csAXiS5X7LqGACPo4Z1m/glgHkke5PP9fbjrWX9Onz1uQzA\nV5LeKPMAbHZCLd3jToBf9gLgMwCeRuORbOd1Mu8OHd8n0LjlegTAw8nyGTRiwysBPAPgPwFM7XZZ\nCz7uowDclaQ/AuABAM8C+CGA3btdvgKP80AAQ0n9/hjAlDrXLYBvA3gSwKMAbgKwe53qF8AtaMT3\n30fjDutEX32i8bTOa5K26xdo9M7p+jFoJKaISKT0JaaISKTUgIuIREoNuIhIpNSAi4hESg24iEik\n1ICLiERKDbiISKTUgIuIROr/AXUpT4urK3NJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nSf-3jtD67q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "9340cb48-4b3b-473f-d972-39bd4f637758"
      },
      "source": [
        "pred = mlmodel.predict({\"image\": img})"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8fb9346d1ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/coremltools/models/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, useCPUOnly, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_macos_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model prediction is only supported on macOS version 10.13 or later.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Model prediction is only supported on macOS version 10.13 or later."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTB8H6pEEQnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}