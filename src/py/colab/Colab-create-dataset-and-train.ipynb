{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Colab-create-dataset-and-train.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koukyo1994/iOS-note-v2/blob/master/src/py/colab/Colab-create-dataset-and-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpTI0ZUNM0t_",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB3gkaOoM0uB",
        "colab_type": "code",
        "outputId": "230bb1a9-2168-4b1e-b4ba-d74f15e9b917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%sh\n",
        "apt -qq -y update >> /dev/null\n",
        "apt -qq -y install fonts-ipafont wamerican >> /dev/null\n",
        "pip install tensorflow-gpu==2.0.0 imgaug==0.2.6 coremltools==3.1 >> /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "ERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.2 which is incompatible.\n",
            "ERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\n",
            "ERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\n",
            "ERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpTZf0ZeM0uF",
        "colab_type": "code",
        "outputId": "9e1efa36-daab-4339-de87-fccddf4abb24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/koukyo1994/iOS-note-v2.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'iOS-note-v2'...\n",
            "remote: Enumerating objects: 199, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/199)\u001b[K\rremote: Counting objects:   1% (2/199)\u001b[K\rremote: Counting objects:   2% (4/199)\u001b[K\rremote: Counting objects:   3% (6/199)\u001b[K\rremote: Counting objects:   4% (8/199)\u001b[K\rremote: Counting objects:   5% (10/199)\u001b[K\rremote: Counting objects:   6% (12/199)\u001b[K\rremote: Counting objects:   7% (14/199)\u001b[K\rremote: Counting objects:   8% (16/199)\u001b[K\rremote: Counting objects:   9% (18/199)\u001b[K\rremote: Counting objects:  10% (20/199)\u001b[K\rremote: Counting objects:  11% (22/199)\u001b[K\rremote: Counting objects:  12% (24/199)\u001b[K\rremote: Counting objects:  13% (26/199)\u001b[K\rremote: Counting objects:  14% (28/199)\u001b[K\rremote: Counting objects:  15% (30/199)\u001b[K\rremote: Counting objects:  16% (32/199)\u001b[K\rremote: Counting objects:  17% (34/199)\u001b[K\rremote: Counting objects:  18% (36/199)\u001b[K\rremote: Counting objects:  19% (38/199)\u001b[K\rremote: Counting objects:  20% (40/199)\u001b[K\rremote: Counting objects:  21% (42/199)\u001b[K\rremote: Counting objects:  22% (44/199)\u001b[K\rremote: Counting objects:  23% (46/199)\u001b[K\rremote: Counting objects:  24% (48/199)\u001b[K\rremote: Counting objects:  25% (50/199)\u001b[K\rremote: Counting objects:  26% (52/199)\u001b[K\rremote: Counting objects:  27% (54/199)\u001b[K\rremote: Counting objects:  28% (56/199)\u001b[K\rremote: Counting objects:  29% (58/199)\u001b[K\rremote: Counting objects:  30% (60/199)\u001b[K\rremote: Counting objects:  31% (62/199)\u001b[K\rremote: Counting objects:  32% (64/199)\u001b[K\rremote: Counting objects:  33% (66/199)\u001b[K\rremote: Counting objects:  34% (68/199)\u001b[K\rremote: Counting objects:  35% (70/199)\u001b[K\rremote: Counting objects:  36% (72/199)\u001b[K\rremote: Counting objects:  37% (74/199)\u001b[K\rremote: Counting objects:  38% (76/199)\u001b[K\rremote: Counting objects:  39% (78/199)\u001b[K\rremote: Counting objects:  40% (80/199)\u001b[K\rremote: Counting objects:  41% (82/199)\u001b[K\rremote: Counting objects:  42% (84/199)\u001b[K\rremote: Counting objects:  43% (86/199)\u001b[K\rremote: Counting objects:  44% (88/199)\u001b[K\rremote: Counting objects:  45% (90/199)\u001b[K\rremote: Counting objects:  46% (92/199)\u001b[K\rremote: Counting objects:  47% (94/199)\u001b[K\rremote: Counting objects:  48% (96/199)\u001b[K\rremote: Counting objects:  49% (98/199)\u001b[K\rremote: Counting objects:  50% (100/199)\u001b[K\rremote: Counting objects:  51% (102/199)\u001b[K\rremote: Counting objects:  52% (104/199)\u001b[K\rremote: Counting objects:  53% (106/199)\u001b[K\rremote: Counting objects:  54% (108/199)\u001b[K\rremote: Counting objects:  55% (110/199)\u001b[K\rremote: Counting objects:  56% (112/199)\u001b[K\rremote: Counting objects:  57% (114/199)\u001b[K\rremote: Counting objects:  58% (116/199)\u001b[K\rremote: Counting objects:  59% (118/199)\u001b[K\rremote: Counting objects:  60% (120/199)\u001b[K\rremote: Counting objects:  61% (122/199)\u001b[K\rremote: Counting objects:  62% (124/199)\u001b[K\rremote: Counting objects:  63% (126/199)\u001b[K\rremote: Counting objects:  64% (128/199)\u001b[K\rremote: Counting objects:  65% (130/199)\u001b[K\rremote: Counting objects:  66% (132/199)\u001b[K\rremote: Counting objects:  67% (134/199)\u001b[K\rremote: Counting objects:  68% (136/199)\u001b[K\rremote: Counting objects:  69% (138/199)\u001b[K\rremote: Counting objects:  70% (140/199)\u001b[K\rremote: Counting objects:  71% (142/199)\u001b[K\rremote: Counting objects:  72% (144/199)\u001b[K\rremote: Counting objects:  73% (146/199)\u001b[K\rremote: Counting objects:  74% (148/199)\u001b[K\rremote: Counting objects:  75% (150/199)\u001b[K\rremote: Counting objects:  76% (152/199)\u001b[K\rremote: Counting objects:  77% (154/199)\u001b[K\rremote: Counting objects:  78% (156/199)\u001b[K\rremote: Counting objects:  79% (158/199)\u001b[K\rremote: Counting objects:  80% (160/199)\u001b[K\rremote: Counting objects:  81% (162/199)\u001b[K\rremote: Counting objects:  82% (164/199)\u001b[K\rremote: Counting objects:  83% (166/199)\u001b[K\rremote: Counting objects:  84% (168/199)\u001b[K\rremote: Counting objects:  85% (170/199)\u001b[K\rremote: Counting objects:  86% (172/199)\u001b[K\rremote: Counting objects:  87% (174/199)\u001b[K\rremote: Counting objects:  88% (176/199)\u001b[K\rremote: Counting objects:  89% (178/199)\u001b[K\rremote: Counting objects:  90% (180/199)\u001b[K\rremote: Counting objects:  91% (182/199)\u001b[K\rremote: Counting objects:  92% (184/199)\u001b[K\rremote: Counting objects:  93% (186/199)\u001b[K\rremote: Counting objects:  94% (188/199)\u001b[K\rremote: Counting objects:  95% (190/199)\u001b[K\rremote: Counting objects:  96% (192/199)\u001b[K\rremote: Counting objects:  97% (194/199)\u001b[K\rremote: Counting objects:  98% (196/199)\u001b[K\rremote: Counting objects:  99% (198/199)\u001b[K\rremote: Counting objects: 100% (199/199)\u001b[K\rremote: Counting objects: 100% (199/199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 199 (delta 62), reused 167 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (199/199), 2.33 MiB | 6.26 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cldi5yNjcCKf",
        "colab_type": "text"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klFTCJwfM0uI",
        "colab_type": "code",
        "outputId": "db37599c-8efc-426c-a861-eced11e294f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "%cd /content/iOS-note-v2/src/py\n",
        "!make create-dataset NSAMPLES=10000"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/iOS-note-v2/src/py\n",
            "chmod +x setup.sh\n",
            "make font-setup\n",
            "make[1]: Entering directory '/content/iOS-note-v2/src/py'\n",
            "./setup.sh\n",
            "alanis-hand\n",
            "architext\n",
            "ashcan-bb\n",
            "./setup.sh: 7: [: ashcanbb_bold.ttf: unexpected operator\n",
            "attack-of-the-cucumbers\n",
            "./setup.sh: 7: [: attack: unexpected operator\n",
            "attract-more-women\n",
            "blzee\n",
            "calligravity\n",
            "domestic-manners\n",
            "FH-GoodDogPlain-WTT\n",
            "james-almacen\n",
            "james-fajardo\n",
            "./setup.sh: 7: [: James: unexpected operator\n",
            "khand\n",
            "ladylike-bb\n",
            "mulders-handwriting\n",
            "mumsies\n",
            "Otto\n",
            "pecita\n",
            "quikhand\n",
            "Sophia\n",
            "two-turtle-doves\n",
            "make[1]: Leaving directory '/content/iOS-note-v2/src/py'\n",
            "python create_dataset.py --n_samples 10000\n",
            "100% 10000/10000 [03:25<00:00, 48.50it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muqkdWAPcFYN",
        "colab_type": "text"
      },
      "source": [
        "## Save Dataset in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTQ5ZsJwM0uK",
        "colab_type": "code",
        "outputId": "c025a4a9-0fe9-4dba-cbff-5c7d22159c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sL6bYdfM0uM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r data/images /content/gdrive/My\\ Drive/AppliedML/\n",
        "!cp data/labels.csv /content/gdrive/My\\ Drive/AppliedML/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juvw9y15cJbr",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKAcedl7UfFE",
        "colab_type": "code",
        "outputId": "2c5f2987-fd11-417f-f5c9-0fccb5d4ac1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Step 10 : Loss : 48.9250565\n",
            "Step 20 : Loss : 40.8672676\n",
            "Step 30 : Loss : 37.4945717\n",
            "Step 40 : Loss : 35.4135246\n",
            "Step 50 : Loss : 34.1190758\n",
            "Step 60 : Loss : 33.15131\n",
            "Step 70 : Loss : 32.2951965\n",
            "Step 80 : Loss : 31.5905361\n",
            "Step 90 : Loss : 31.056797\n",
            "Step 100 : Loss : 30.6019516\n",
            "Step 110 : Loss : 30.2320805\n",
            "Step 120 : Loss : 29.8586197\n",
            "Step 130 : Loss : 29.5164185\n",
            "Step 140 : Loss : 29.1522827\n",
            "Step 150 : Loss : 28.8922977\n",
            "Step 160 : Loss : 28.685009\n",
            "Step 170 : Loss : 28.4357529\n",
            "Step 180 : Loss : 28.1672039\n",
            "Step 190 : Loss : 27.9574528\n",
            "Step 200 : Loss : 27.7581615\n",
            "Step 210 : Loss : 27.5264721\n",
            "Step 220 : Loss : 27.3444366\n",
            "Step 230 : Loss : 27.1349487\n",
            "Step 240 : Loss : 26.928524\n",
            "Step 250 : Loss : 26.7255936\n",
            "Step 260 : Loss : 26.5385113\n",
            "Step 270 : Loss : 26.3631325\n",
            "Step 280 : Loss : 26.1924686\n",
            "Step 290 : Loss : 26.0341454\n",
            "Step 295 : Loss :  25.9743881\n",
            "Epoch 2\n",
            "Step 300 : Loss : 25.8895092\n",
            "Step 310 : Loss : 25.7306499\n",
            "Step 320 : Loss : 25.5678291\n",
            "Step 330 : Loss : 25.4006596\n",
            "Step 340 : Loss : 25.2302761\n",
            "Step 350 : Loss : 25.0589123\n",
            "Step 360 : Loss : 24.9051838\n",
            "Step 370 : Loss : 24.7242317\n",
            "Step 380 : Loss : 24.5423183\n",
            "Step 390 : Loss : 24.3806362\n",
            "Step 400 : Loss : 24.2343845\n",
            "Step 410 : Loss : 24.093029\n",
            "Step 420 : Loss : 23.9609394\n",
            "Step 430 : Loss : 23.7984161\n",
            "Step 440 : Loss : 23.6448154\n",
            "Step 450 : Loss : 23.513567\n",
            "Step 460 : Loss : 23.3624477\n",
            "Step 470 : Loss : 23.2070923\n",
            "Step 480 : Loss : 23.0559769\n",
            "Step 490 : Loss : 22.9086609\n",
            "Step 500 : Loss : 22.7766247\n",
            "Step 510 : Loss : 22.6299763\n",
            "Step 520 : Loss : 22.4936295\n",
            "Step 530 : Loss : 22.337574\n",
            "Step 540 : Loss : 22.1943092\n",
            "Step 550 : Loss : 22.0453949\n",
            "Step 560 : Loss : 21.9085789\n",
            "Step 570 : Loss : 21.7688351\n",
            "Step 580 : Loss : 21.6387119\n",
            "Step 590 : Loss : 21.4921436\n",
            "Step 590 : Loss :  21.4921436\n",
            "Epoch 3\n",
            "Step 600 : Loss : 21.3678551\n",
            "Step 610 : Loss : 21.2424622\n",
            "Step 620 : Loss : 21.1132946\n",
            "Step 630 : Loss : 20.9817276\n",
            "Step 640 : Loss : 20.8553047\n",
            "Step 650 : Loss : 20.7292194\n",
            "Step 660 : Loss : 20.5949535\n",
            "Step 670 : Loss : 20.4537659\n",
            "Step 680 : Loss : 20.3283539\n",
            "Step 690 : Loss : 20.2048683\n",
            "Step 700 : Loss : 20.0905457\n",
            "Step 710 : Loss : 19.9777985\n",
            "Step 720 : Loss : 19.863781\n",
            "Step 730 : Loss : 19.741436\n",
            "Step 740 : Loss : 19.6175919\n",
            "Step 750 : Loss : 19.5015926\n",
            "Step 760 : Loss : 19.3833828\n",
            "Step 770 : Loss : 19.263258\n",
            "Step 780 : Loss : 19.1446819\n",
            "Step 790 : Loss : 19.034977\n",
            "Step 800 : Loss : 18.925499\n",
            "Step 810 : Loss : 18.8185444\n",
            "Step 820 : Loss : 18.7063751\n",
            "Step 830 : Loss : 18.5898514\n",
            "Step 840 : Loss : 18.4782219\n",
            "Step 850 : Loss : 18.3690834\n",
            "Step 860 : Loss : 18.2590504\n",
            "Step 870 : Loss : 18.1500874\n",
            "Step 880 : Loss : 18.0494671\n",
            "Step 885 : Loss :  17.9887619\n",
            "Epoch 4\n",
            "Step 890 : Loss : 17.9421558\n",
            "Step 900 : Loss : 17.8452206\n",
            "Step 910 : Loss : 17.7467518\n",
            "Step 920 : Loss : 17.6428795\n",
            "Step 930 : Loss : 17.5433159\n",
            "Step 940 : Loss : 17.4367161\n",
            "Step 950 : Loss : 17.3363838\n",
            "Step 960 : Loss : 17.2292252\n",
            "Step 970 : Loss : 17.1201744\n",
            "Step 980 : Loss : 17.0187397\n",
            "Step 990 : Loss : 16.9200821\n",
            "Step 1000 : Loss : 16.8275261\n",
            "Step 1010 : Loss : 16.7382393\n",
            "Step 1020 : Loss : 16.6383076\n",
            "Step 1030 : Loss : 16.5380192\n",
            "Step 1040 : Loss : 16.4505787\n",
            "Step 1050 : Loss : 16.3537712\n",
            "Step 1060 : Loss : 16.2609158\n",
            "Step 1070 : Loss : 16.1652431\n",
            "Step 1080 : Loss : 16.0700264\n",
            "Step 1090 : Loss : 15.9837532\n",
            "Step 1100 : Loss : 15.8922138\n",
            "Step 1110 : Loss : 15.8069258\n",
            "Step 1120 : Loss : 15.7167635\n",
            "Step 1130 : Loss : 15.6277933\n",
            "Step 1140 : Loss : 15.5366573\n",
            "Step 1150 : Loss : 15.4461546\n",
            "Step 1160 : Loss : 15.3558273\n",
            "Step 1170 : Loss : 15.2710867\n",
            "Step 1180 : Loss : 15.1829147\n",
            "Step 1180 : Loss :  15.1829147\n",
            "Epoch 5\n",
            "Step 1190 : Loss : 15.1009846\n",
            "Step 1200 : Loss : 15.0182533\n",
            "Step 1210 : Loss : 14.9345818\n",
            "Step 1220 : Loss : 14.849925\n",
            "Step 1230 : Loss : 14.7690125\n",
            "Step 1240 : Loss : 14.6860132\n",
            "Step 1250 : Loss : 14.6029186\n",
            "Step 1260 : Loss : 14.5166435\n",
            "Step 1270 : Loss : 14.4344244\n",
            "Step 1280 : Loss : 14.3547316\n",
            "Step 1290 : Loss : 14.2780476\n",
            "Step 1300 : Loss : 14.2029114\n",
            "Step 1310 : Loss : 14.1255112\n",
            "Step 1320 : Loss : 14.0467386\n",
            "Step 1330 : Loss : 13.9692726\n",
            "Step 1340 : Loss : 13.893055\n",
            "Step 1350 : Loss : 13.8180408\n",
            "Step 1360 : Loss : 13.7404919\n",
            "Step 1370 : Loss : 13.6628847\n",
            "Step 1380 : Loss : 13.5886269\n",
            "Step 1390 : Loss : 13.5151682\n",
            "Step 1400 : Loss : 13.4452305\n",
            "Step 1410 : Loss : 13.3784008\n",
            "Step 1420 : Loss : 13.3077812\n",
            "Step 1430 : Loss : 13.2357368\n",
            "Step 1440 : Loss : 13.1627808\n",
            "Step 1450 : Loss : 13.0889988\n",
            "Step 1460 : Loss : 13.0183\n",
            "Step 1470 : Loss : 12.9495201\n",
            "Step 1475 : Loss :  12.9125967\n",
            "Epoch 6\n",
            "Step 1480 : Loss : 12.8778687\n",
            "Step 1490 : Loss : 12.8076725\n",
            "Step 1500 : Loss : 12.7388773\n",
            "Step 1510 : Loss : 12.6694565\n",
            "Step 1520 : Loss : 12.6026134\n",
            "Step 1530 : Loss : 12.5342827\n",
            "Step 1540 : Loss : 12.4683495\n",
            "Step 1550 : Loss : 12.4035883\n",
            "Step 1560 : Loss : 12.3368111\n",
            "Step 1570 : Loss : 12.272109\n",
            "Step 1580 : Loss : 12.2101727\n",
            "Step 1590 : Loss : 12.152854\n",
            "Step 1600 : Loss : 12.0974236\n",
            "Step 1610 : Loss : 12.0362339\n",
            "Step 1620 : Loss : 11.9741259\n",
            "Step 1630 : Loss : 11.9136734\n",
            "Step 1640 : Loss : 11.8522882\n",
            "Step 1650 : Loss : 11.7917681\n",
            "Step 1660 : Loss : 11.7315865\n",
            "Step 1670 : Loss : 11.6721401\n",
            "Step 1680 : Loss : 11.6165209\n",
            "Step 1690 : Loss : 11.5575428\n",
            "Step 1700 : Loss : 11.499342\n",
            "Step 1710 : Loss : 11.4429903\n",
            "Step 1720 : Loss : 11.3855181\n",
            "Step 1730 : Loss : 11.32798\n",
            "Step 1740 : Loss : 11.2706547\n",
            "Step 1750 : Loss : 11.2143869\n",
            "Step 1760 : Loss : 11.1598148\n",
            "Step 1770 : Loss : 11.1042318\n",
            "Step 1770 : Loss :  11.1042318\n",
            "Epoch 7\n",
            "Step 1780 : Loss : 11.0495491\n",
            "Step 1790 : Loss : 10.9944448\n",
            "Step 1800 : Loss : 10.9402456\n",
            "Step 1810 : Loss : 10.8864536\n",
            "Step 1820 : Loss : 10.8336277\n",
            "Step 1830 : Loss : 10.7808723\n",
            "Step 1840 : Loss : 10.7296829\n",
            "Step 1850 : Loss : 10.6780281\n",
            "Step 1860 : Loss : 10.6280737\n",
            "Step 1870 : Loss : 10.5776968\n",
            "Step 1880 : Loss : 10.528821\n",
            "Step 1890 : Loss : 10.4805384\n",
            "Step 1900 : Loss : 10.4330072\n",
            "Step 1910 : Loss : 10.3848267\n",
            "Step 1920 : Loss : 10.3367109\n",
            "Step 1930 : Loss : 10.2884541\n",
            "Step 1940 : Loss : 10.2408752\n",
            "Step 1950 : Loss : 10.1930618\n",
            "Step 1960 : Loss : 10.1460161\n",
            "Step 1970 : Loss : 10.1000681\n",
            "Step 1980 : Loss : 10.0554848\n",
            "Step 1990 : Loss : 10.0103531\n",
            "Step 2000 : Loss : 9.96576881\n",
            "Step 2010 : Loss : 9.92116261\n",
            "Step 2020 : Loss : 9.87671566\n",
            "Step 2030 : Loss : 9.83215523\n",
            "Step 2040 : Loss : 9.78767395\n",
            "Step 2050 : Loss : 9.74400139\n",
            "Step 2060 : Loss : 9.70150852\n",
            "Step 2065 : Loss :  9.67955208\n",
            "Epoch 8\n",
            "Step 2070 : Loss : 9.65873146\n",
            "Step 2080 : Loss : 9.61603069\n",
            "Step 2090 : Loss : 9.57367229\n",
            "Step 2100 : Loss : 9.53162098\n",
            "Step 2110 : Loss : 9.49012089\n",
            "Step 2120 : Loss : 9.44899464\n",
            "Step 2130 : Loss : 9.40803814\n",
            "Step 2140 : Loss : 9.36718178\n",
            "Step 2150 : Loss : 9.327178\n",
            "Step 2160 : Loss : 9.28785896\n",
            "Step 2170 : Loss : 9.24907494\n",
            "Step 2180 : Loss : 9.21014595\n",
            "Step 2190 : Loss : 9.17089558\n",
            "Step 2200 : Loss : 9.13139915\n",
            "Step 2210 : Loss : 9.09224796\n",
            "Step 2220 : Loss : 9.0542078\n",
            "Step 2230 : Loss : 9.0165081\n",
            "Step 2240 : Loss : 8.97963715\n",
            "Step 2250 : Loss : 8.94268894\n",
            "Step 2260 : Loss : 8.90548801\n",
            "Step 2270 : Loss : 8.86901\n",
            "Step 2280 : Loss : 8.83319\n",
            "Step 2290 : Loss : 8.79760838\n",
            "Step 2300 : Loss : 8.76137543\n",
            "Step 2310 : Loss : 8.72557068\n",
            "Step 2320 : Loss : 8.68978214\n",
            "Step 2330 : Loss : 8.65414715\n",
            "Step 2340 : Loss : 8.61886406\n",
            "Step 2350 : Loss : 8.58414268\n",
            "Step 2360 : Loss : 8.54999\n",
            "Step 2360 : Loss :  8.54999\n",
            "Epoch 9\n",
            "Step 2370 : Loss : 8.51654816\n",
            "Step 2380 : Loss : 8.48298359\n",
            "Step 2390 : Loss : 8.44943333\n",
            "Step 2400 : Loss : 8.41624069\n",
            "Step 2410 : Loss : 8.38321781\n",
            "Step 2420 : Loss : 8.35052586\n",
            "Step 2430 : Loss : 8.31814\n",
            "Step 2440 : Loss : 8.28608131\n",
            "Step 2450 : Loss : 8.25468349\n",
            "Step 2460 : Loss : 8.22339058\n",
            "Step 2470 : Loss : 8.19244289\n",
            "Step 2480 : Loss : 8.16151524\n",
            "Step 2490 : Loss : 8.1308651\n",
            "Step 2500 : Loss : 8.09978199\n",
            "Step 2510 : Loss : 8.06886101\n",
            "Step 2520 : Loss : 8.03807545\n",
            "Step 2530 : Loss : 8.00789833\n",
            "Step 2540 : Loss : 7.97827625\n",
            "Step 2550 : Loss : 7.94901133\n",
            "Step 2560 : Loss : 7.91943836\n",
            "Step 2570 : Loss : 7.8897934\n",
            "Step 2580 : Loss : 7.86047745\n",
            "Step 2590 : Loss : 7.83137226\n",
            "Step 2600 : Loss : 7.80222\n",
            "Step 2610 : Loss : 7.77331257\n",
            "Step 2620 : Loss : 7.74450731\n",
            "Step 2630 : Loss : 7.71581364\n",
            "Step 2640 : Loss : 7.68739796\n",
            "Step 2650 : Loss : 7.65941238\n",
            "Step 2655 : Loss :  7.64534616\n",
            "Epoch 10\n",
            "Step 2660 : Loss : 7.63174582\n",
            "Step 2670 : Loss : 7.60431862\n",
            "Step 2680 : Loss : 7.5773735\n",
            "Step 2690 : Loss : 7.55038929\n",
            "Step 2700 : Loss : 7.52390099\n",
            "Step 2710 : Loss : 7.49750137\n",
            "Step 2720 : Loss : 7.47114706\n",
            "Step 2730 : Loss : 7.44471788\n",
            "Step 2740 : Loss : 7.41850185\n",
            "Step 2750 : Loss : 7.39228392\n",
            "Step 2760 : Loss : 7.36613464\n",
            "Step 2770 : Loss : 7.34043312\n",
            "Step 2780 : Loss : 7.31515074\n",
            "Step 2790 : Loss : 7.2899971\n",
            "Step 2800 : Loss : 7.26481676\n",
            "Step 2810 : Loss : 7.23980331\n",
            "Step 2820 : Loss : 7.21485\n",
            "Step 2830 : Loss : 7.19008398\n",
            "Step 2840 : Loss : 7.16560793\n",
            "Step 2850 : Loss : 7.14121437\n",
            "Step 2860 : Loss : 7.11710501\n",
            "Step 2870 : Loss : 7.09286404\n",
            "Step 2880 : Loss : 7.06902218\n",
            "Step 2890 : Loss : 7.04530048\n",
            "Step 2900 : Loss : 7.02159071\n",
            "Step 2910 : Loss : 6.99801731\n",
            "Step 2920 : Loss : 6.97453976\n",
            "Step 2930 : Loss : 6.95117712\n",
            "Step 2940 : Loss : 6.92803621\n",
            "Step 2950 : Loss : 6.90501308\n",
            "Step 2950 : Loss :  6.90501308\n",
            "Epoch 11\n",
            "Step 2960 : Loss : 6.88215971\n",
            "Step 2970 : Loss : 6.85939264\n",
            "Step 2980 : Loss : 6.83682346\n",
            "Step 2990 : Loss : 6.81447601\n",
            "Step 3000 : Loss : 6.79226637\n",
            "Step 3010 : Loss : 6.77021408\n",
            "Step 3020 : Loss : 6.74830675\n",
            "Step 3030 : Loss : 6.72637939\n",
            "Step 3040 : Loss : 6.70477343\n",
            "Step 3050 : Loss : 6.68312931\n",
            "Step 3060 : Loss : 6.66156483\n",
            "Step 3070 : Loss : 6.64026737\n",
            "Step 3080 : Loss : 6.61904812\n",
            "Step 3090 : Loss : 6.59799957\n",
            "Step 3100 : Loss : 6.57704973\n",
            "Step 3110 : Loss : 6.55625439\n",
            "Step 3120 : Loss : 6.53554\n",
            "Step 3130 : Loss : 6.51495409\n",
            "Step 3140 : Loss : 6.49471951\n",
            "Step 3150 : Loss : 6.47456741\n",
            "Step 3160 : Loss : 6.45450449\n",
            "Step 3170 : Loss : 6.43453836\n",
            "Step 3180 : Loss : 6.41459513\n",
            "Step 3190 : Loss : 6.39487\n",
            "Step 3200 : Loss : 6.37516546\n",
            "Step 3210 : Loss : 6.35554123\n",
            "Step 3220 : Loss : 6.33604813\n",
            "Step 3230 : Loss : 6.31664133\n",
            "Step 3240 : Loss : 6.29744816\n",
            "Step 3245 : Loss :  6.28782368\n",
            "Epoch 12\n",
            "Step 3250 : Loss : 6.27825785\n",
            "Step 3260 : Loss : 6.25925779\n",
            "Step 3270 : Loss : 6.24031162\n",
            "Step 3280 : Loss : 6.22154045\n",
            "Step 3290 : Loss : 6.20288086\n",
            "Step 3300 : Loss : 6.18431568\n",
            "Step 3310 : Loss : 6.16580391\n",
            "Step 3320 : Loss : 6.14740372\n",
            "Step 3330 : Loss : 6.12927103\n",
            "Step 3340 : Loss : 6.11108351\n",
            "Step 3350 : Loss : 6.09299374\n",
            "Step 3360 : Loss : 6.07501173\n",
            "Step 3370 : Loss : 6.05718327\n",
            "Step 3380 : Loss : 6.03939199\n",
            "Step 3390 : Loss : 6.02170467\n",
            "Step 3400 : Loss : 6.00414181\n",
            "Step 3410 : Loss : 5.98668861\n",
            "Step 3420 : Loss : 5.96933842\n",
            "Step 3430 : Loss : 5.95221663\n",
            "Step 3440 : Loss : 5.93505\n",
            "Step 3450 : Loss : 5.91806507\n",
            "Step 3460 : Loss : 5.9010663\n",
            "Step 3470 : Loss : 5.88419104\n",
            "Step 3480 : Loss : 5.86737347\n",
            "Step 3490 : Loss : 5.85071754\n",
            "Step 3500 : Loss : 5.83411551\n",
            "Step 3510 : Loss : 5.81760168\n",
            "Step 3520 : Loss : 5.80117083\n",
            "Step 3530 : Loss : 5.78482819\n",
            "Step 3540 : Loss : 5.76856136\n",
            "Step 3540 : Loss :  5.76856136\n",
            "Epoch 13\n",
            "Step 3550 : Loss : 5.7524\n",
            "Step 3560 : Loss : 5.73633385\n",
            "Step 3570 : Loss : 5.72038746\n",
            "Step 3580 : Loss : 5.70451689\n",
            "Step 3590 : Loss : 5.6887269\n",
            "Step 3600 : Loss : 5.67307758\n",
            "Step 3610 : Loss : 5.65745974\n",
            "Step 3620 : Loss : 5.64193153\n",
            "Step 3630 : Loss : 5.62659645\n",
            "Step 3640 : Loss : 5.61122179\n",
            "Step 3650 : Loss : 5.59591341\n",
            "Step 3660 : Loss : 5.58069801\n",
            "Step 3670 : Loss : 5.56557798\n",
            "Step 3680 : Loss : 5.55051756\n",
            "Step 3690 : Loss : 5.53555059\n",
            "Step 3700 : Loss : 5.52069473\n",
            "Step 3710 : Loss : 5.50591898\n",
            "Step 3720 : Loss : 5.49119902\n",
            "Step 3730 : Loss : 5.47667551\n",
            "Step 3740 : Loss : 5.46209192\n",
            "Step 3750 : Loss : 5.44758129\n",
            "Step 3760 : Loss : 5.4331584\n",
            "Step 3770 : Loss : 5.41880322\n",
            "Step 3780 : Loss : 5.40455437\n",
            "Step 3790 : Loss : 5.39034939\n",
            "Step 3800 : Loss : 5.3762269\n",
            "Step 3810 : Loss : 5.36217356\n",
            "Step 3820 : Loss : 5.34819269\n",
            "Step 3830 : Loss : 5.33428383\n",
            "Step 3835 : Loss :  5.32734728\n",
            "Epoch 14\n",
            "Step 3840 : Loss : 5.32043362\n",
            "Step 3850 : Loss : 5.30665588\n",
            "Step 3860 : Loss : 5.29294968\n",
            "Step 3870 : Loss : 5.27937\n",
            "Step 3880 : Loss : 5.26582384\n",
            "Step 3890 : Loss : 5.2524066\n",
            "Step 3900 : Loss : 5.23901129\n",
            "Step 3910 : Loss : 5.22566128\n",
            "Step 3920 : Loss : 5.2124958\n",
            "Step 3930 : Loss : 5.1993\n",
            "Step 3940 : Loss : 5.18616581\n",
            "Step 3950 : Loss : 5.17307711\n",
            "Step 3960 : Loss : 5.16006231\n",
            "Step 3970 : Loss : 5.14710665\n",
            "Step 3980 : Loss : 5.13421059\n",
            "Step 3990 : Loss : 5.12138748\n",
            "Step 4000 : Loss : 5.10868931\n",
            "Step 4010 : Loss : 5.09601\n",
            "Step 4020 : Loss : 5.08350658\n",
            "Step 4030 : Loss : 5.07093334\n",
            "Step 4040 : Loss : 5.0584197\n",
            "Step 4050 : Loss : 5.04596472\n",
            "Step 4060 : Loss : 5.03357\n",
            "Step 4070 : Loss : 5.0212307\n",
            "Step 4080 : Loss : 5.00895405\n",
            "Step 4090 : Loss : 4.99673939\n",
            "Step 4100 : Loss : 4.98458529\n",
            "Step 4110 : Loss : 4.97248697\n",
            "Step 4120 : Loss : 4.96045065\n",
            "Step 4130 : Loss : 4.94846725\n",
            "Step 4130 : Loss :  4.94846725\n",
            "Epoch 15\n",
            "Step 4140 : Loss : 4.93654346\n",
            "Step 4150 : Loss : 4.92467308\n",
            "Step 4160 : Loss : 4.91286\n",
            "Step 4170 : Loss : 4.90116167\n",
            "Step 4180 : Loss : 4.88946486\n",
            "Step 4190 : Loss : 4.87791967\n",
            "Step 4200 : Loss : 4.86635\n",
            "Step 4210 : Loss : 4.85482\n",
            "Step 4220 : Loss : 4.84345627\n",
            "Step 4230 : Loss : 4.8320365\n",
            "Step 4240 : Loss : 4.8206687\n",
            "Step 4250 : Loss : 4.80935097\n",
            "Step 4260 : Loss : 4.79808664\n",
            "Step 4270 : Loss : 4.78687811\n",
            "Step 4280 : Loss : 4.77571535\n",
            "Step 4290 : Loss : 4.76460648\n",
            "Step 4300 : Loss : 4.75354862\n",
            "Step 4310 : Loss : 4.74254084\n",
            "Step 4320 : Loss : 4.7317\n",
            "Step 4330 : Loss : 4.72079372\n",
            "Step 4340 : Loss : 4.70993757\n",
            "Step 4350 : Loss : 4.69913054\n",
            "Step 4360 : Loss : 4.68837\n",
            "Step 4370 : Loss : 4.67766094\n",
            "Step 4380 : Loss : 4.66699839\n",
            "Step 4390 : Loss : 4.65638781\n",
            "Step 4400 : Loss : 4.64582443\n",
            "Step 4410 : Loss : 4.63530922\n",
            "Step 4420 : Loss : 4.62484169\n",
            "Step 4425 : Loss :  4.61962414\n",
            "Epoch 16\n",
            "Step 4430 : Loss : 4.61441851\n",
            "Step 4440 : Loss : 4.60404348\n",
            "Step 4450 : Loss : 4.59371424\n",
            "Step 4460 : Loss : 4.5834322\n",
            "Step 4470 : Loss : 4.57319307\n",
            "Step 4480 : Loss : 4.56301308\n",
            "Step 4490 : Loss : 4.55286837\n",
            "Step 4500 : Loss : 4.54278135\n",
            "Step 4510 : Loss : 4.53282356\n",
            "Step 4520 : Loss : 4.52281475\n",
            "Step 4530 : Loss : 4.51285315\n",
            "Step 4540 : Loss : 4.50292873\n",
            "Step 4550 : Loss : 4.49305105\n",
            "Step 4560 : Loss : 4.48321438\n",
            "Step 4570 : Loss : 4.47341871\n",
            "Step 4580 : Loss : 4.46366835\n",
            "Step 4590 : Loss : 4.45395947\n",
            "Step 4600 : Loss : 4.44429302\n",
            "Step 4610 : Loss : 4.43477726\n",
            "Step 4620 : Loss : 4.42519236\n",
            "Step 4630 : Loss : 4.41564941\n",
            "Step 4640 : Loss : 4.40614748\n",
            "Step 4650 : Loss : 4.39668608\n",
            "Step 4660 : Loss : 4.38726234\n",
            "Step 4670 : Loss : 4.37788153\n",
            "Step 4680 : Loss : 4.36854029\n",
            "Step 4690 : Loss : 4.35924\n",
            "Step 4700 : Loss : 4.34997845\n",
            "Step 4710 : Loss : 4.34075737\n",
            "Step 4720 : Loss : 4.33157301\n",
            "Step 4720 : Loss :  4.33157301\n",
            "Epoch 17\n",
            "Step 4730 : Loss : 4.32242966\n",
            "Step 4740 : Loss : 4.31332302\n",
            "Step 4750 : Loss : 4.30425453\n",
            "Step 4760 : Loss : 4.29522467\n",
            "Step 4770 : Loss : 4.28623152\n",
            "Step 4780 : Loss : 4.27727604\n",
            "Step 4790 : Loss : 4.26835585\n",
            "Step 4800 : Loss : 4.25947475\n",
            "Step 4810 : Loss : 4.25072479\n",
            "Step 4820 : Loss : 4.24191666\n",
            "Step 4830 : Loss : 4.23314524\n",
            "Step 4840 : Loss : 4.22440958\n",
            "Step 4850 : Loss : 4.21571\n",
            "Step 4860 : Loss : 4.20704651\n",
            "Step 4870 : Loss : 4.19841766\n",
            "Step 4880 : Loss : 4.18982458\n",
            "Step 4890 : Loss : 4.18126678\n",
            "Step 4900 : Loss : 4.17274332\n",
            "Step 4910 : Loss : 4.16435719\n",
            "Step 4920 : Loss : 4.15590286\n",
            "Step 4930 : Loss : 4.14748287\n",
            "Step 4940 : Loss : 4.13909674\n",
            "Step 4950 : Loss : 4.13074446\n",
            "Step 4960 : Loss : 4.12242603\n",
            "Step 4970 : Loss : 4.11414\n",
            "Step 4980 : Loss : 4.10588932\n",
            "Step 4990 : Loss : 4.09767103\n",
            "Step 5000 : Loss : 4.08948517\n",
            "Step 5010 : Loss : 4.08133364\n",
            "Step 5015 : Loss :  4.07726812\n",
            "Epoch 18\n",
            "Step 5020 : Loss : 4.07321215\n",
            "Step 5030 : Loss : 4.06512356\n",
            "Step 5040 : Loss : 4.05706692\n",
            "Step 5050 : Loss : 4.0490427\n",
            "Step 5060 : Loss : 4.041049\n",
            "Step 5070 : Loss : 4.03308678\n",
            "Step 5080 : Loss : 4.02515602\n",
            "Step 5090 : Loss : 4.01725626\n",
            "Step 5100 : Loss : 4.00947475\n",
            "Step 5110 : Loss : 4.00163603\n",
            "Step 5120 : Loss : 3.99382925\n",
            "Step 5130 : Loss : 3.9860518\n",
            "Step 5140 : Loss : 3.97830558\n",
            "Step 5150 : Loss : 3.97058773\n",
            "Step 5160 : Loss : 3.9629004\n",
            "Step 5170 : Loss : 3.95524359\n",
            "Step 5180 : Loss : 3.94761586\n",
            "Step 5190 : Loss : 3.9400177\n",
            "Step 5200 : Loss : 3.93254542\n",
            "Step 5210 : Loss : 3.92500448\n",
            "Step 5220 : Loss : 3.91749239\n",
            "Step 5230 : Loss : 3.91000986\n",
            "Step 5240 : Loss : 3.90255499\n",
            "Step 5250 : Loss : 3.89512825\n",
            "Step 5260 : Loss : 3.88773108\n",
            "Step 5270 : Loss : 3.88036132\n",
            "Step 5280 : Loss : 3.87302\n",
            "Step 5290 : Loss : 3.86570597\n",
            "Step 5300 : Loss : 3.85842\n",
            "Step 5310 : Loss : 3.85116\n",
            "Step 5310 : Loss :  3.85116\n",
            "Epoch 19\n",
            "Step 5320 : Loss : 3.84392905\n",
            "Step 5330 : Loss : 3.836725\n",
            "Step 5340 : Loss : 3.82954741\n",
            "Step 5350 : Loss : 3.82239628\n",
            "Step 5360 : Loss : 3.81527114\n",
            "Step 5370 : Loss : 3.80817246\n",
            "Step 5380 : Loss : 3.8011\n",
            "Step 5390 : Loss : 3.79405403\n",
            "Step 5400 : Loss : 3.78711867\n",
            "Step 5410 : Loss : 3.78012443\n",
            "Step 5420 : Loss : 3.77315617\n",
            "Step 5430 : Loss : 3.76621389\n",
            "Step 5440 : Loss : 3.75929666\n",
            "Step 5450 : Loss : 3.75240612\n",
            "Step 5460 : Loss : 3.74553895\n",
            "Step 5470 : Loss : 3.73869801\n",
            "Step 5480 : Loss : 3.73188186\n",
            "Step 5490 : Loss : 3.72509\n",
            "Step 5500 : Loss : 3.71841478\n",
            "Step 5510 : Loss : 3.71167278\n",
            "Step 5520 : Loss : 3.7049551\n",
            "Step 5530 : Loss : 3.69826102\n",
            "Step 5540 : Loss : 3.69159055\n",
            "Step 5550 : Loss : 3.68494487\n",
            "Step 5560 : Loss : 3.67832279\n",
            "Step 5570 : Loss : 3.67172551\n",
            "Step 5580 : Loss : 3.66515207\n",
            "Step 5590 : Loss : 3.65860176\n",
            "Step 5600 : Loss : 3.65207529\n",
            "Step 5605 : Loss :  3.64882016\n",
            "Epoch 20\n",
            "Step 5610 : Loss : 3.64557076\n",
            "Step 5620 : Loss : 3.63908958\n",
            "Step 5630 : Loss : 3.63263178\n",
            "Step 5640 : Loss : 3.62619686\n",
            "Step 5650 : Loss : 3.61978316\n",
            "Step 5660 : Loss : 3.61339259\n",
            "Step 5670 : Loss : 3.60702515\n",
            "Step 5680 : Loss : 3.60067987\n",
            "Step 5690 : Loss : 3.59443593\n",
            "Step 5700 : Loss : 3.588135\n",
            "Step 5710 : Loss : 3.58185673\n",
            "Step 5720 : Loss : 3.57560039\n",
            "Step 5730 : Loss : 3.56936526\n",
            "Step 5740 : Loss : 3.5631516\n",
            "Step 5750 : Loss : 3.55695891\n",
            "Step 5760 : Loss : 3.55078864\n",
            "Step 5770 : Loss : 3.54463959\n",
            "Step 5780 : Loss : 3.53851128\n",
            "Step 5790 : Loss : 3.53249168\n",
            "Step 5800 : Loss : 3.52640533\n",
            "Step 5810 : Loss : 3.52034068\n",
            "Step 5820 : Loss : 3.51429629\n",
            "Step 5830 : Loss : 3.50827336\n",
            "Step 5840 : Loss : 3.50227022\n",
            "Step 5850 : Loss : 3.49628806\n",
            "Step 5860 : Loss : 3.4903264\n",
            "Step 5870 : Loss : 3.48438573\n",
            "Step 5880 : Loss : 3.47846413\n",
            "Step 5890 : Loss : 3.47256351\n",
            "Step 5900 : Loss : 3.46668243\n",
            "Step 5900 : Loss :  3.46668243\n",
            "Epoch 21\n",
            "Step 5910 : Loss : 3.46082115\n",
            "Step 5920 : Loss : 3.4549787\n",
            "Step 5930 : Loss : 3.44915724\n",
            "Step 5940 : Loss : 3.44335508\n",
            "Step 5950 : Loss : 3.43757153\n",
            "Step 5960 : Loss : 3.43180799\n",
            "Step 5970 : Loss : 3.4260633\n",
            "Step 5980 : Loss : 3.42033792\n",
            "Step 5990 : Loss : 3.41470742\n",
            "Step 6000 : Loss : 3.40902042\n",
            "Step 6010 : Loss : 3.40335226\n",
            "Step 6020 : Loss : 3.39770222\n",
            "Step 6030 : Loss : 3.39207149\n",
            "Step 6040 : Loss : 3.38645911\n",
            "Step 6050 : Loss : 3.3808651\n",
            "Step 6060 : Loss : 3.37529016\n",
            "Step 6070 : Loss : 3.36973357\n",
            "Step 6080 : Loss : 3.36419487\n",
            "Step 6090 : Loss : 3.35874891\n",
            "Step 6100 : Loss : 3.35325027\n",
            "Step 6110 : Loss : 3.34776855\n",
            "Step 6120 : Loss : 3.34230494\n",
            "Step 6130 : Loss : 3.33685803\n",
            "Step 6140 : Loss : 3.33142877\n",
            "Step 6150 : Loss : 3.3260169\n",
            "Step 6160 : Loss : 3.32062316\n",
            "Step 6170 : Loss : 3.31524682\n",
            "Step 6180 : Loss : 3.30988693\n",
            "Step 6190 : Loss : 3.30454516\n",
            "Step 6195 : Loss :  3.30188\n",
            "Epoch 22\n",
            "Step 6200 : Loss : 3.29921961\n",
            "Step 6210 : Loss : 3.29391146\n",
            "Step 6220 : Loss : 3.28862071\n",
            "Step 6230 : Loss : 3.28334665\n",
            "Step 6240 : Loss : 3.27808833\n",
            "Step 6250 : Loss : 3.27284694\n",
            "Step 6260 : Loss : 3.26762271\n",
            "Step 6270 : Loss : 3.26241517\n",
            "Step 6280 : Loss : 3.25729561\n",
            "Step 6290 : Loss : 3.25212049\n",
            "Step 6300 : Loss : 3.24696279\n",
            "Step 6310 : Loss : 3.24182\n",
            "Step 6320 : Loss : 3.23669457\n",
            "Step 6330 : Loss : 3.23158479\n",
            "Step 6340 : Loss : 3.22649074\n",
            "Step 6350 : Loss : 3.22141314\n",
            "Step 6360 : Loss : 3.21635127\n",
            "Step 6370 : Loss : 3.21130538\n",
            "Step 6380 : Loss : 3.20627499\n",
            "Step 6390 : Loss : 3.20126057\n",
            "Step 6400 : Loss : 3.19626164\n",
            "Step 6410 : Loss : 3.1912787\n",
            "Step 6420 : Loss : 3.18631077\n",
            "Step 6430 : Loss : 3.18135834\n",
            "Step 6440 : Loss : 3.1764214\n",
            "Step 6450 : Loss : 3.17149973\n",
            "Step 6460 : Loss : 3.16659379\n",
            "Step 6470 : Loss : 3.16170239\n",
            "Step 6480 : Loss : 3.1568265\n",
            "Step 6490 : Loss : 3.15196514\n",
            "Step 6490 : Loss :  3.15196514\n",
            "Epoch 23\n",
            "Step 6500 : Loss : 3.14711928\n",
            "Step 6510 : Loss : 3.14228797\n",
            "Step 6520 : Loss : 3.13747144\n",
            "Step 6530 : Loss : 3.13267\n",
            "Step 6540 : Loss : 3.12788272\n",
            "Step 6550 : Loss : 3.12311029\n",
            "Step 6560 : Loss : 3.11835241\n",
            "Step 6570 : Loss : 3.11360908\n",
            "Step 6580 : Loss : 3.10894895\n",
            "Step 6590 : Loss : 3.1042347\n",
            "Step 6600 : Loss : 3.09953427\n",
            "Step 6610 : Loss : 3.09484792\n",
            "Step 6620 : Loss : 3.09017587\n",
            "Step 6630 : Loss : 3.08551812\n",
            "Step 6640 : Loss : 3.08087397\n",
            "Step 6650 : Loss : 3.07624435\n",
            "Step 6660 : Loss : 3.07162833\n",
            "Step 6670 : Loss : 3.06702614\n",
            "Step 6680 : Loss : 3.06243777\n",
            "Step 6690 : Loss : 3.057863\n",
            "Step 6700 : Loss : 3.05330205\n",
            "Step 6710 : Loss : 3.04875445\n",
            "Step 6720 : Loss : 3.04422045\n",
            "Step 6730 : Loss : 3.0397\n",
            "Step 6740 : Loss : 3.03519297\n",
            "Step 6750 : Loss : 3.03069925\n",
            "Step 6760 : Loss : 3.02621889\n",
            "Step 6770 : Loss : 3.02175188\n",
            "Step 6780 : Loss : 3.01729774\n",
            "Step 6785 : Loss :  3.01507545\n",
            "Epoch 24\n",
            "Step 6790 : Loss : 3.01285672\n",
            "Step 6800 : Loss : 3.00842881\n",
            "Step 6810 : Loss : 3.00401402\n",
            "Step 6820 : Loss : 2.99961233\n",
            "Step 6830 : Loss : 2.99522328\n",
            "Step 6840 : Loss : 2.99084711\n",
            "Step 6850 : Loss : 2.98648381\n",
            "Step 6860 : Loss : 2.98213315\n",
            "Step 6870 : Loss : 2.97786117\n",
            "Step 6880 : Loss : 2.97353578\n",
            "Step 6890 : Loss : 2.96922302\n",
            "Step 6900 : Loss : 2.96492267\n",
            "Step 6910 : Loss : 2.96063471\n",
            "Step 6920 : Loss : 2.95635915\n",
            "Step 6930 : Loss : 2.95209599\n",
            "Step 6940 : Loss : 2.94784498\n",
            "Step 6950 : Loss : 2.94360638\n",
            "Step 6960 : Loss : 2.93938\n",
            "Step 6970 : Loss : 2.93516541\n",
            "Step 6980 : Loss : 2.93096328\n",
            "Step 6990 : Loss : 2.92677283\n",
            "Step 7000 : Loss : 2.92259455\n",
            "Step 7010 : Loss : 2.91842818\n",
            "Step 7020 : Loss : 2.91427374\n",
            "Step 7030 : Loss : 2.91013098\n",
            "Step 7040 : Loss : 2.90600014\n",
            "Step 7050 : Loss : 2.90188074\n",
            "Step 7060 : Loss : 2.89777327\n",
            "Step 7070 : Loss : 2.89367723\n",
            "Step 7080 : Loss : 2.88959265\n",
            "Step 7080 : Loss :  2.88959265\n",
            "Epoch 25\n",
            "Step 7090 : Loss : 2.88552\n",
            "Step 7100 : Loss : 2.88145852\n",
            "Step 7110 : Loss : 2.8774085\n",
            "Step 7120 : Loss : 2.87337\n",
            "Step 7130 : Loss : 2.8693428\n",
            "Step 7140 : Loss : 2.86532688\n",
            "Step 7150 : Loss : 2.86132216\n",
            "Step 7160 : Loss : 2.85732841\n",
            "Step 7170 : Loss : 2.85340905\n",
            "Step 7180 : Loss : 2.84943748\n",
            "Step 7190 : Loss : 2.84547687\n",
            "Step 7200 : Loss : 2.84152746\n",
            "Step 7210 : Loss : 2.83758879\n",
            "Step 7220 : Loss : 2.83366132\n",
            "Step 7230 : Loss : 2.82974482\n",
            "Step 7240 : Loss : 2.82583904\n",
            "Step 7250 : Loss : 2.82194376\n",
            "Step 7260 : Loss : 2.81805944\n",
            "Step 7270 : Loss : 2.81418586\n",
            "Step 7280 : Loss : 2.81032252\n",
            "Step 7290 : Loss : 2.80647016\n",
            "Step 7300 : Loss : 2.80262852\n",
            "Step 7310 : Loss : 2.79879689\n",
            "Step 7320 : Loss : 2.79497552\n",
            "Step 7330 : Loss : 2.79116488\n",
            "Step 7340 : Loss : 2.78736472\n",
            "Step 7350 : Loss : 2.78357506\n",
            "Step 7360 : Loss : 2.77979589\n",
            "Step 7370 : Loss : 2.77602673\n",
            "Step 7375 : Loss :  2.7741456\n",
            "Epoch 26\n",
            "Step 7380 : Loss : 2.77226758\n",
            "Step 7390 : Loss : 2.76851869\n",
            "Step 7400 : Loss : 2.76477981\n",
            "Step 7410 : Loss : 2.76105118\n",
            "Step 7420 : Loss : 2.75733209\n",
            "Step 7430 : Loss : 2.75362253\n",
            "Step 7440 : Loss : 2.74992418\n",
            "Step 7450 : Loss : 2.74623513\n",
            "Step 7460 : Loss : 2.74261642\n",
            "Step 7470 : Loss : 2.73894715\n",
            "Step 7480 : Loss : 2.73528767\n",
            "Step 7490 : Loss : 2.73163795\n",
            "Step 7500 : Loss : 2.7279985\n",
            "Step 7510 : Loss : 2.7243681\n",
            "Step 7520 : Loss : 2.72074747\n",
            "Step 7530 : Loss : 2.71713638\n",
            "Step 7540 : Loss : 2.71353507\n",
            "Step 7550 : Loss : 2.70994329\n",
            "Step 7560 : Loss : 2.70636129\n",
            "Step 7570 : Loss : 2.70278859\n",
            "Step 7580 : Loss : 2.69922495\n",
            "Step 7590 : Loss : 2.69567084\n",
            "Step 7600 : Loss : 2.6921258\n",
            "Step 7610 : Loss : 2.68858981\n",
            "Step 7620 : Loss : 2.68506384\n",
            "Step 7630 : Loss : 2.68154693\n",
            "Step 7640 : Loss : 2.67803931\n",
            "Step 7650 : Loss : 2.67454076\n",
            "Step 7660 : Loss : 2.6710515\n",
            "Step 7670 : Loss : 2.66757131\n",
            "Step 7670 : Loss :  2.66757131\n",
            "Epoch 27\n",
            "Step 7680 : Loss : 2.66410017\n",
            "Step 7690 : Loss : 2.66063809\n",
            "Step 7700 : Loss : 2.65718436\n",
            "Step 7710 : Loss : 2.65374\n",
            "Step 7720 : Loss : 2.65030408\n",
            "Step 7730 : Loss : 2.64687729\n",
            "Step 7740 : Loss : 2.6434586\n",
            "Step 7750 : Loss : 2.64004946\n",
            "Step 7760 : Loss : 2.63670778\n",
            "Step 7770 : Loss : 2.63331556\n",
            "Step 7780 : Loss : 2.6299324\n",
            "Step 7790 : Loss : 2.62655878\n",
            "Step 7800 : Loss : 2.62319303\n",
            "Step 7810 : Loss : 2.61983609\n",
            "Step 7820 : Loss : 2.61648726\n",
            "Step 7830 : Loss : 2.61314726\n",
            "Step 7840 : Loss : 2.60981584\n",
            "Step 7850 : Loss : 2.60649276\n",
            "Step 7860 : Loss : 2.6031785\n",
            "Step 7870 : Loss : 2.59987259\n",
            "Step 7880 : Loss : 2.59657502\n",
            "Step 7890 : Loss : 2.59328556\n",
            "Step 7900 : Loss : 2.59000421\n",
            "Step 7910 : Loss : 2.5867312\n",
            "Step 7920 : Loss : 2.58346629\n",
            "Step 7930 : Loss : 2.58021069\n",
            "Step 7940 : Loss : 2.57696295\n",
            "Step 7950 : Loss : 2.57372355\n",
            "Step 7960 : Loss : 2.57049227\n",
            "Step 7965 : Loss :  2.5688796\n",
            "Epoch 28\n",
            "Step 7970 : Loss : 2.56726861\n",
            "Step 7980 : Loss : 2.56405258\n",
            "Step 7990 : Loss : 2.56084514\n",
            "Step 8000 : Loss : 2.5576458\n",
            "Step 8010 : Loss : 2.55445337\n",
            "Step 8020 : Loss : 2.55126905\n",
            "Step 8030 : Loss : 2.54809332\n",
            "Step 8040 : Loss : 2.54492521\n",
            "Step 8050 : Loss : 2.54182124\n",
            "Step 8060 : Loss : 2.53866863\n",
            "Step 8070 : Loss : 2.53552461\n",
            "Step 8080 : Loss : 2.53238773\n",
            "Step 8090 : Loss : 2.52925944\n",
            "Step 8100 : Loss : 2.52613783\n",
            "Step 8110 : Loss : 2.52302361\n",
            "Step 8120 : Loss : 2.51991749\n",
            "Step 8130 : Loss : 2.516819\n",
            "Step 8140 : Loss : 2.51372766\n",
            "Step 8150 : Loss : 2.51064467\n",
            "Step 8160 : Loss : 2.50756884\n",
            "Step 8170 : Loss : 2.50450063\n",
            "Step 8180 : Loss : 2.50143957\n",
            "Step 8190 : Loss : 2.49838638\n",
            "Step 8200 : Loss : 2.49534\n",
            "Step 8210 : Loss : 2.49230123\n",
            "Step 8220 : Loss : 2.48927021\n",
            "Step 8230 : Loss : 2.48624706\n",
            "Step 8240 : Loss : 2.48323035\n",
            "Step 8250 : Loss : 2.48022151\n",
            "Step 8260 : Loss : 2.47721958\n",
            "Step 8260 : Loss :  2.47721958\n",
            "Epoch 29\n",
            "Step 8270 : Loss : 2.47422504\n",
            "Step 8280 : Loss : 2.47123742\n",
            "Step 8290 : Loss : 2.46825743\n",
            "Step 8300 : Loss : 2.46528459\n",
            "Step 8310 : Loss : 2.46231818\n",
            "Step 8320 : Loss : 2.45935893\n",
            "Step 8330 : Loss : 2.45640683\n",
            "Step 8340 : Loss : 2.45346212\n",
            "Step 8350 : Loss : 2.45057893\n",
            "Step 8360 : Loss : 2.44764853\n",
            "Step 8370 : Loss : 2.4447248\n",
            "Step 8380 : Loss : 2.44180775\n",
            "Step 8390 : Loss : 2.43889809\n",
            "Step 8400 : Loss : 2.43599486\n",
            "Step 8410 : Loss : 2.43309855\n",
            "Step 8420 : Loss : 2.4302094\n",
            "Step 8430 : Loss : 2.42732739\n",
            "Step 8440 : Loss : 2.42445183\n",
            "Step 8450 : Loss : 2.42158318\n",
            "Step 8460 : Loss : 2.41872072\n",
            "Step 8470 : Loss : 2.41586518\n",
            "Step 8480 : Loss : 2.4130168\n",
            "Step 8490 : Loss : 2.41017509\n",
            "Step 8500 : Loss : 2.40734\n",
            "Step 8510 : Loss : 2.40451145\n",
            "Step 8520 : Loss : 2.40168929\n",
            "Step 8530 : Loss : 2.39887381\n",
            "Step 8540 : Loss : 2.39606524\n",
            "Step 8550 : Loss : 2.39326286\n",
            "Step 8555 : Loss :  2.3918643\n",
            "Epoch 30\n",
            "Step 8560 : Loss : 2.39046741\n",
            "Step 8570 : Loss : 2.38767838\n",
            "Step 8580 : Loss : 2.38489604\n",
            "Step 8590 : Loss : 2.38212037\n",
            "Step 8600 : Loss : 2.37935042\n",
            "Step 8610 : Loss : 2.37658691\n",
            "Step 8620 : Loss : 2.37383\n",
            "Step 8630 : Loss : 2.37107944\n",
            "Step 8640 : Loss : 2.3683877\n",
            "Step 8650 : Loss : 2.36565\n",
            "Step 8660 : Loss : 2.36291862\n",
            "Step 8670 : Loss : 2.36019325\n",
            "Step 8680 : Loss : 2.35747433\n",
            "Step 8690 : Loss : 2.3547616\n",
            "Step 8700 : Loss : 2.35205507\n",
            "Step 8710 : Loss : 2.34935498\n",
            "Step 8720 : Loss : 2.34666085\n",
            "Step 8730 : Loss : 2.34397316\n",
            "Step 8740 : Loss : 2.34129119\n",
            "Step 8750 : Loss : 2.33861542\n",
            "Step 8760 : Loss : 2.33594584\n",
            "Step 8770 : Loss : 2.33328247\n",
            "Step 8780 : Loss : 2.33062482\n",
            "Step 8790 : Loss : 2.32797337\n",
            "Step 8800 : Loss : 2.32532811\n",
            "Step 8810 : Loss : 2.32268858\n",
            "Step 8820 : Loss : 2.32005548\n",
            "Step 8830 : Loss : 2.31742787\n",
            "Step 8840 : Loss : 2.3148067\n",
            "Step 8850 : Loss : 2.31219101\n",
            "Step 8850 : Loss :  2.31219101\n",
            "Epoch 31\n",
            "Step 8860 : Loss : 2.30958152\n",
            "Step 8870 : Loss : 2.30697775\n",
            "Step 8880 : Loss : 2.3043797\n",
            "Step 8890 : Loss : 2.30178761\n",
            "Step 8900 : Loss : 2.29920149\n",
            "Step 8910 : Loss : 2.29662085\n",
            "Step 8920 : Loss : 2.29404616\n",
            "Step 8930 : Loss : 2.29147744\n",
            "Step 8940 : Loss : 2.28896499\n",
            "Step 8950 : Loss : 2.28640771\n",
            "Step 8960 : Loss : 2.28385592\n",
            "Step 8970 : Loss : 2.28130984\n",
            "Step 8980 : Loss : 2.27876949\n",
            "Step 8990 : Loss : 2.27623463\n",
            "Step 9000 : Loss : 2.27370548\n",
            "Step 9010 : Loss : 2.2711823\n",
            "Step 9020 : Loss : 2.26866436\n",
            "Step 9030 : Loss : 2.26615191\n",
            "Step 9040 : Loss : 2.26364517\n",
            "Step 9050 : Loss : 2.26114392\n",
            "Step 9060 : Loss : 2.25864816\n",
            "Step 9070 : Loss : 2.25615788\n",
            "Step 9080 : Loss : 2.25367308\n",
            "Step 9090 : Loss : 2.25119376\n",
            "Step 9100 : Loss : 2.24872\n",
            "Step 9110 : Loss : 2.24625158\n",
            "Step 9120 : Loss : 2.24378848\n",
            "Step 9130 : Loss : 2.24133086\n",
            "Step 9140 : Loss : 2.23887873\n",
            "Step 9145 : Loss :  2.23765469\n",
            "Epoch 32\n",
            "Step 9150 : Loss : 2.23643184\n",
            "Step 9160 : Loss : 2.23399043\n",
            "Step 9170 : Loss : 2.23155427\n",
            "Step 9180 : Loss : 2.22912335\n",
            "Step 9190 : Loss : 2.22669768\n",
            "Step 9200 : Loss : 2.22427726\n",
            "Step 9210 : Loss : 2.22186232\n",
            "Step 9220 : Loss : 2.21945238\n",
            "Step 9230 : Loss : 2.21709704\n",
            "Step 9240 : Loss : 2.21469736\n",
            "Step 9250 : Loss : 2.2123034\n",
            "Step 9260 : Loss : 2.20991421\n",
            "Step 9270 : Loss : 2.2075305\n",
            "Step 9280 : Loss : 2.20515156\n",
            "Step 9290 : Loss : 2.20277786\n",
            "Step 9300 : Loss : 2.20040941\n",
            "Step 9310 : Loss : 2.19804573\n",
            "Step 9320 : Loss : 2.19568729\n",
            "Step 9330 : Loss : 2.1933341\n",
            "Step 9340 : Loss : 2.19098568\n",
            "Step 9350 : Loss : 2.1886425\n",
            "Step 9360 : Loss : 2.18630409\n",
            "Step 9370 : Loss : 2.18397093\n",
            "Step 9380 : Loss : 2.18164253\n",
            "Step 9390 : Loss : 2.17931914\n",
            "Step 9400 : Loss : 2.17700076\n",
            "Step 9410 : Loss : 2.17468715\n",
            "Step 9420 : Loss : 2.17237854\n",
            "Step 9430 : Loss : 2.17007494\n",
            "Step 9440 : Loss : 2.16777611\n",
            "Step 9440 : Loss :  2.16777611\n",
            "Epoch 33\n",
            "Step 9450 : Loss : 2.16548228\n",
            "Step 9460 : Loss : 2.16319299\n",
            "Step 9470 : Loss : 2.16090894\n",
            "Step 9480 : Loss : 2.15862942\n",
            "Step 9490 : Loss : 2.15635467\n",
            "Step 9500 : Loss : 2.15408492\n",
            "Step 9510 : Loss : 2.15182\n",
            "Step 9520 : Loss : 2.1495595\n",
            "Step 9530 : Loss : 2.1473515\n",
            "Step 9540 : Loss : 2.14510059\n",
            "Step 9550 : Loss : 2.14285445\n",
            "Step 9560 : Loss : 2.14061284\n",
            "Step 9570 : Loss : 2.13837624\n",
            "Step 9580 : Loss : 2.13614392\n",
            "Step 9590 : Loss : 2.13391662\n",
            "Step 9600 : Loss : 2.13169384\n",
            "Step 9610 : Loss : 2.12947559\n",
            "Step 9620 : Loss : 2.12726188\n",
            "Step 9630 : Loss : 2.12505293\n",
            "Step 9640 : Loss : 2.12284851\n",
            "Step 9650 : Loss : 2.12064862\n",
            "Step 9660 : Loss : 2.1184535\n",
            "Step 9670 : Loss : 2.11626267\n",
            "Step 9680 : Loss : 2.11407638\n",
            "Step 9690 : Loss : 2.11189461\n",
            "Step 9700 : Loss : 2.10971761\n",
            "Step 9710 : Loss : 2.10754466\n",
            "Step 9720 : Loss : 2.10537648\n",
            "Step 9730 : Loss : 2.10321283\n",
            "Step 9735 : Loss :  2.10213256\n",
            "Epoch 34\n",
            "Step 9740 : Loss : 2.10105348\n",
            "Step 9750 : Loss : 2.09889841\n",
            "Step 9760 : Loss : 2.09674788\n",
            "Step 9770 : Loss : 2.09460187\n",
            "Step 9780 : Loss : 2.09246016\n",
            "Step 9790 : Loss : 2.09032273\n",
            "Step 9800 : Loss : 2.08818984\n",
            "Step 9810 : Loss : 2.08606124\n",
            "Step 9820 : Loss : 2.08398294\n",
            "Step 9830 : Loss : 2.08186293\n",
            "Step 9840 : Loss : 2.0797472\n",
            "Step 9850 : Loss : 2.07763577\n",
            "Step 9860 : Loss : 2.07552862\n",
            "Step 9870 : Loss : 2.07342577\n",
            "Step 9880 : Loss : 2.07132721\n",
            "Step 9890 : Loss : 2.06923294\n",
            "Step 9900 : Loss : 2.06714272\n",
            "Step 9910 : Loss : 2.0650568\n",
            "Step 9920 : Loss : 2.06297517\n",
            "Step 9930 : Loss : 2.06089759\n",
            "Step 9940 : Loss : 2.0588243\n",
            "Step 9950 : Loss : 2.05675507\n",
            "Step 9960 : Loss : 2.05469012\n",
            "Step 9970 : Loss : 2.05262923\n",
            "Step 9980 : Loss : 2.0505724\n",
            "Step 9990 : Loss : 2.04851985\n",
            "Step 10000 : Loss : 2.04647136\n",
            "Step 10010 : Loss : 2.04442692\n",
            "Step 10020 : Loss : 2.04238653\n",
            "Step 10030 : Loss : 2.0403502\n",
            "Step 10030 : Loss :  2.0403502\n",
            "Epoch 35\n",
            "Step 10040 : Loss : 2.03831792\n",
            "Step 10050 : Loss : 2.03629\n",
            "Step 10060 : Loss : 2.03426576\n",
            "Step 10070 : Loss : 2.03224564\n",
            "Step 10080 : Loss : 2.03022957\n",
            "Step 10090 : Loss : 2.02821732\n",
            "Step 10100 : Loss : 2.02620912\n",
            "Step 10110 : Loss : 2.02420497\n",
            "Step 10120 : Loss : 2.0222497\n",
            "Step 10130 : Loss : 2.02025342\n",
            "Step 10140 : Loss : 2.01826096\n",
            "Step 10150 : Loss : 2.01627254\n",
            "Step 10160 : Loss : 2.01428795\n",
            "Step 10170 : Loss : 2.01230741\n",
            "Step 10180 : Loss : 2.01033068\n",
            "Step 10190 : Loss : 2.00835776\n",
            "Step 10200 : Loss : 2.0063889\n",
            "Step 10210 : Loss : 2.00442362\n",
            "Step 10220 : Loss : 2.00246239\n",
            "Step 10230 : Loss : 2.00050497\n",
            "Step 10240 : Loss : 1.99855137\n",
            "Step 10250 : Loss : 1.99660158\n",
            "Step 10260 : Loss : 1.99465561\n",
            "Step 10270 : Loss : 1.99271333\n",
            "Step 10280 : Loss : 1.99077487\n",
            "Step 10290 : Loss : 1.98884022\n",
            "Step 10300 : Loss : 1.98690927\n",
            "Step 10310 : Loss : 1.98498213\n",
            "Step 10320 : Loss : 1.98305869\n",
            "Step 10325 : Loss :  1.98209846\n",
            "Epoch 36\n",
            "Step 10330 : Loss : 1.98113906\n",
            "Step 10340 : Loss : 1.97922301\n",
            "Step 10350 : Loss : 1.97731078\n",
            "Step 10360 : Loss : 1.97540212\n",
            "Step 10370 : Loss : 1.97349715\n",
            "Step 10380 : Loss : 1.971596\n",
            "Step 10390 : Loss : 1.96969831\n",
            "Step 10400 : Loss : 1.96780443\n",
            "Step 10410 : Loss : 1.96595764\n",
            "Step 10420 : Loss : 1.96407092\n",
            "Step 10430 : Loss : 1.96218789\n",
            "Step 10440 : Loss : 1.96030831\n",
            "Step 10450 : Loss : 1.95843244\n",
            "Step 10460 : Loss : 1.95656013\n",
            "Step 10470 : Loss : 1.95469141\n",
            "Step 10480 : Loss : 1.95282626\n",
            "Step 10490 : Loss : 1.95096469\n",
            "Step 10500 : Loss : 1.94910657\n",
            "Step 10510 : Loss : 1.94725204\n",
            "Step 10520 : Loss : 1.94540107\n",
            "Step 10530 : Loss : 1.94355357\n",
            "Step 10540 : Loss : 1.94170964\n",
            "Step 10550 : Loss : 1.93986917\n",
            "Step 10560 : Loss : 1.93803215\n",
            "Step 10570 : Loss : 1.93619859\n",
            "Step 10580 : Loss : 1.93436849\n",
            "Step 10590 : Loss : 1.93254197\n",
            "Step 10600 : Loss : 1.93071878\n",
            "Step 10610 : Loss : 1.92889905\n",
            "Step 10620 : Loss : 1.92708278\n",
            "Step 10620 : Loss :  1.92708278\n",
            "Epoch 37\n",
            "Step 10630 : Loss : 1.92527\n",
            "Step 10640 : Loss : 1.92346048\n",
            "Step 10650 : Loss : 1.92165434\n",
            "Step 10660 : Loss : 1.91985166\n",
            "Step 10670 : Loss : 1.91805243\n",
            "Step 10680 : Loss : 1.91625643\n",
            "Step 10690 : Loss : 1.91446388\n",
            "Step 10700 : Loss : 1.91267467\n",
            "Step 10710 : Loss : 1.91093111\n",
            "Step 10720 : Loss : 1.90914857\n",
            "Step 10730 : Loss : 1.90736926\n",
            "Step 10740 : Loss : 1.90559328\n",
            "Step 10750 : Loss : 1.90382063\n",
            "Step 10760 : Loss : 1.90205133\n",
            "Step 10770 : Loss : 1.90028524\n",
            "Step 10780 : Loss : 1.8985225\n",
            "Step 10790 : Loss : 1.89676297\n",
            "Step 10800 : Loss : 1.89500666\n",
            "Step 10810 : Loss : 1.89325368\n",
            "Step 10820 : Loss : 1.89150393\n",
            "Step 10830 : Loss : 1.88975739\n",
            "Step 10840 : Loss : 1.88801408\n",
            "Step 10850 : Loss : 1.88627398\n",
            "Step 10860 : Loss : 1.88453698\n",
            "Step 10870 : Loss : 1.88280332\n",
            "Step 10880 : Loss : 1.88107276\n",
            "Step 10890 : Loss : 1.87934554\n",
            "Step 10900 : Loss : 1.87762129\n",
            "Step 10910 : Loss : 1.87590027\n",
            "Step 10915 : Loss :  1.87504101\n",
            "Epoch 38\n",
            "Step 10920 : Loss : 1.87418246\n",
            "Step 10930 : Loss : 1.87246776\n",
            "Step 10940 : Loss : 1.87075615\n",
            "Step 10950 : Loss : 1.86904764\n",
            "Step 10960 : Loss : 1.86734235\n",
            "Step 10970 : Loss : 1.86564016\n",
            "Step 10980 : Loss : 1.86394095\n",
            "Step 10990 : Loss : 1.86224496\n",
            "Step 11000 : Loss : 1.8605932\n",
            "Step 11010 : Loss : 1.85890329\n",
            "Step 11020 : Loss : 1.85721648\n",
            "Step 11030 : Loss : 1.85553265\n",
            "Step 11040 : Loss : 1.85385191\n",
            "Step 11050 : Loss : 1.85217428\n",
            "Step 11060 : Loss : 1.85049963\n",
            "Step 11070 : Loss : 1.84882796\n",
            "Step 11080 : Loss : 1.84715939\n",
            "Step 11090 : Loss : 1.84549367\n",
            "Step 11100 : Loss : 1.84383106\n",
            "Step 11110 : Loss : 1.84217155\n",
            "Step 11120 : Loss : 1.8405149\n",
            "Step 11130 : Loss : 1.83886123\n",
            "Step 11140 : Loss : 1.83721054\n",
            "Step 11150 : Loss : 1.83556283\n",
            "Step 11160 : Loss : 1.83391809\n",
            "Step 11170 : Loss : 1.83227623\n",
            "Step 11180 : Loss : 1.83063734\n",
            "Step 11190 : Loss : 1.82900143\n",
            "Step 11200 : Loss : 1.82736838\n",
            "Step 11210 : Loss : 1.82573819\n",
            "Step 11210 : Loss :  1.82573819\n",
            "Epoch 39\n",
            "Step 11220 : Loss : 1.82411098\n",
            "Step 11230 : Loss : 1.82248664\n",
            "Step 11240 : Loss : 1.82086527\n",
            "Step 11250 : Loss : 1.81924665\n",
            "Step 11260 : Loss : 1.81763101\n",
            "Step 11270 : Loss : 1.81601822\n",
            "Step 11280 : Loss : 1.8144083\n",
            "Step 11290 : Loss : 1.81280124\n",
            "Step 11300 : Loss : 1.81123698\n",
            "Step 11310 : Loss : 1.80963564\n",
            "Step 11320 : Loss : 1.80803692\n",
            "Step 11330 : Loss : 1.80644119\n",
            "Step 11340 : Loss : 1.80484819\n",
            "Step 11350 : Loss : 1.80325806\n",
            "Step 11360 : Loss : 1.80167067\n",
            "Step 11370 : Loss : 1.80008602\n",
            "Step 11380 : Loss : 1.79850423\n",
            "Step 11390 : Loss : 1.79692519\n",
            "Step 11400 : Loss : 1.795349\n",
            "Step 11410 : Loss : 1.79377556\n",
            "Step 11420 : Loss : 1.79220474\n",
            "Step 11430 : Loss : 1.79063678\n",
            "Step 11440 : Loss : 1.78907156\n",
            "Step 11450 : Loss : 1.78750908\n",
            "Step 11460 : Loss : 1.78594923\n",
            "Step 11470 : Loss : 1.78439224\n",
            "Step 11480 : Loss : 1.78283787\n",
            "Step 11490 : Loss : 1.78128624\n",
            "Step 11500 : Loss : 1.77973723\n",
            "Step 11505 : Loss :  1.7789638\n",
            "Epoch 40\n",
            "Step 11510 : Loss : 1.77819097\n",
            "Step 11520 : Loss : 1.77664745\n",
            "Step 11530 : Loss : 1.77510655\n",
            "Step 11540 : Loss : 1.77356827\n",
            "Step 11550 : Loss : 1.77203274\n",
            "Step 11560 : Loss : 1.77049983\n",
            "Step 11570 : Loss : 1.76896966\n",
            "Step 11580 : Loss : 1.76744199\n",
            "Step 11590 : Loss : 1.76595616\n",
            "Step 11600 : Loss : 1.76443374\n",
            "Step 11610 : Loss : 1.76291406\n",
            "Step 11620 : Loss : 1.76139688\n",
            "Step 11630 : Loss : 1.75988233\n",
            "Step 11640 : Loss : 1.7583704\n",
            "Step 11650 : Loss : 1.75686109\n",
            "Step 11660 : Loss : 1.75535429\n",
            "Step 11670 : Loss : 1.75385022\n",
            "Step 11680 : Loss : 1.75234854\n",
            "Step 11690 : Loss : 1.7508496\n",
            "Step 11700 : Loss : 1.74935317\n",
            "Step 11710 : Loss : 1.74785924\n",
            "Step 11720 : Loss : 1.74636793\n",
            "Step 11730 : Loss : 1.74487913\n",
            "Step 11740 : Loss : 1.74339283\n",
            "Step 11750 : Loss : 1.74190903\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 37, in <module>\n",
            "    train(dataset)\n",
            "  File \"train.py\", line 30, in train\n",
            "    train_step(images, labels)\n",
            "  File \"train.py\", line 25, in train_step\n",
            "    gradients = tape.gradient(loss, model.trainable_variables)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py\", line 1014, in gradient\n",
            "    unconnected_gradients=unconnected_gradients)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/imperative_grad.py\", line 76, in imperative_grad\n",
            "    compat.as_str(unconnected_gradients.value))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py\", line 138, in _gradient_function\n",
            "    return grad_fn(mock_op, *out_grads)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_grad.py\", line 606, in _Conv2DGrad\n",
            "    data_format=data_format)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py\", line 1203, in conv2d_backprop_filter\n",
            "    _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
            "  File \"<string>\", line 3, in raise_from\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1024,1024,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2DBackpropFilter]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Qw7A9qcVs7",
        "colab_type": "code",
        "outputId": "009a1970-a198-40f4-e8c3-f3e9f5d61c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "labels = pd.read_csv(\"data/labels.csv\")\n",
        "labels.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>text</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cfcd208495d565ef66e7dff9f98764da</td>\n",
              "      <td>catbird</td>\n",
              "      <td>95</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>c4ca4238a0b923820dcc509a6f75849b</td>\n",
              "      <td>Grünewald</td>\n",
              "      <td>106</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c81e728d9d4c2f636f067f89cc14862c</td>\n",
              "      <td>birding</td>\n",
              "      <td>55</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>eccbc87e4b5ce2fe28308fd9f2a7baf3</td>\n",
              "      <td>metastasize</td>\n",
              "      <td>117</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a87ff679a2f3e71d9181a67b7542122c</td>\n",
              "      <td>large</td>\n",
              "      <td>67</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           image_id         text  width  height\n",
              "0  cfcd208495d565ef66e7dff9f98764da      catbird     95      32\n",
              "1  c4ca4238a0b923820dcc509a6f75849b    Grünewald    106      32\n",
              "2  c81e728d9d4c2f636f067f89cc14862c      birding     55      32\n",
              "3  eccbc87e4b5ce2fe28308fd9f2a7baf3  metastasize    117      32\n",
              "4  a87ff679a2f3e71d9181a67b7542122c        large     67      32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYwrmJAri0Vu",
        "colab_type": "code",
        "outputId": "1d184514-e8a1-48c5-d01d-0aacdf0801f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "labels.text[labels.text.isnull()]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23163    NaN\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOHQsJkhi8Ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}