{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Colab-create-dataset-and-train.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koukyo1994/iOS-note-v2/blob/master/src/py/colab/Colab-create-dataset-and-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpTI0ZUNM0t_",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB3gkaOoM0uB",
        "colab_type": "code",
        "outputId": "29283e21-a2b6-42ac-c66c-eacf0669bf3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%sh\n",
        "apt -qq -y update >> /dev/null\n",
        "apt -qq -y install fonts-ipafont wamerican >> /dev/null\n",
        "pip install tensorflow-gpu==2.0.0 imgaug==0.2.6 coremltools==3.1 >> /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "ERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.2 which is incompatible.\n",
            "ERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\n",
            "ERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\n",
            "ERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpTZf0ZeM0uF",
        "colab_type": "code",
        "outputId": "c5dea139-9076-474a-df3a-bb520dcab3b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/koukyo1994/iOS-note-v2.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'iOS-note-v2'...\n",
            "remote: Enumerating objects: 216, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/216)\u001b[K\rremote: Counting objects:   1% (3/216)\u001b[K\rremote: Counting objects:   2% (5/216)\u001b[K\rremote: Counting objects:   3% (7/216)\u001b[K\rremote: Counting objects:   4% (9/216)\u001b[K\rremote: Counting objects:   5% (11/216)\u001b[K\rremote: Counting objects:   6% (13/216)\u001b[K\rremote: Counting objects:   7% (16/216)\u001b[K\rremote: Counting objects:   8% (18/216)\u001b[K\rremote: Counting objects:   9% (20/216)\u001b[K\rremote: Counting objects:  10% (22/216)\u001b[K\rremote: Counting objects:  11% (24/216)\u001b[K\rremote: Counting objects:  12% (26/216)\u001b[K\rremote: Counting objects:  13% (29/216)\u001b[K\rremote: Counting objects:  14% (31/216)\u001b[K\rremote: Counting objects:  15% (33/216)\u001b[K\rremote: Counting objects:  16% (35/216)\u001b[K\rremote: Counting objects:  17% (37/216)\u001b[K\rremote: Counting objects:  18% (39/216)\u001b[K\rremote: Counting objects:  19% (42/216)\u001b[K\rremote: Counting objects:  20% (44/216)\u001b[K\rremote: Counting objects:  21% (46/216)\u001b[K\rremote: Counting objects:  22% (48/216)\u001b[K\rremote: Counting objects:  23% (50/216)\u001b[K\rremote: Counting objects:  24% (52/216)\u001b[K\rremote: Counting objects:  25% (54/216)\u001b[K\rremote: Counting objects:  26% (57/216)\u001b[K\rremote: Counting objects:  27% (59/216)\u001b[K\rremote: Counting objects:  28% (61/216)\u001b[K\rremote: Counting objects:  29% (63/216)\u001b[K\rremote: Counting objects:  30% (65/216)\u001b[K\rremote: Counting objects:  31% (67/216)\u001b[K\rremote: Counting objects:  32% (70/216)\u001b[K\rremote: Counting objects:  33% (72/216)\u001b[K\rremote: Counting objects:  34% (74/216)\u001b[K\rremote: Counting objects:  35% (76/216)\u001b[K\rremote: Counting objects:  36% (78/216)\u001b[K\rremote: Counting objects:  37% (80/216)\u001b[K\rremote: Counting objects:  38% (83/216)\u001b[K\rremote: Counting objects:  39% (85/216)\u001b[K\rremote: Counting objects:  40% (87/216)\u001b[K\rremote: Counting objects:  41% (89/216)\u001b[K\rremote: Counting objects:  42% (91/216)\u001b[K\rremote: Counting objects:  43% (93/216)\u001b[K\rremote: Counting objects:  44% (96/216)\u001b[K\rremote: Counting objects:  45% (98/216)\u001b[K\rremote: Counting objects:  46% (100/216)\rremote: Counting objects:  47% (102/216)\u001b[K\rremote: Counting objects:  48% (104/216)\u001b[K\rremote: Counting objects:  49% (106/216)\u001b[K\rremote: Counting objects:  50% (108/216)\u001b[K\rremote: Counting objects:  51% (111/216)\u001b[K\rremote: Counting objects:  52% (113/216)\u001b[K\rremote: Counting objects:  53% (115/216)\u001b[K\rremote: Counting objects:  54% (117/216)\u001b[K\rremote: Counting objects:  55% (119/216)\u001b[K\rremote: Counting objects:  56% (121/216)\u001b[K\rremote: Counting objects:  57% (124/216)\u001b[K\rremote: Counting objects:  58% (126/216)\u001b[K\rremote: Counting objects:  59% (128/216)\u001b[K\rremote: Counting objects:  60% (130/216)\u001b[K\rremote: Counting objects:  61% (132/216)\u001b[K\rremote: Counting objects:  62% (134/216)\u001b[K\rremote: Counting objects:  63% (137/216)\u001b[K\rremote: Counting objects:  64% (139/216)\u001b[K\rremote: Counting objects:  65% (141/216)\u001b[K\rremote: Counting objects:  66% (143/216)\u001b[K\rremote: Counting objects:  67% (145/216)\u001b[K\rremote: Counting objects:  68% (147/216)\u001b[K\rremote: Counting objects:  69% (150/216)\u001b[K\rremote: Counting objects:  70% (152/216)\u001b[K\rremote: Counting objects:  71% (154/216)\u001b[K\rremote: Counting objects:  72% (156/216)\u001b[K\rremote: Counting objects:  73% (158/216)\u001b[K\rremote: Counting objects:  74% (160/216)\u001b[K\rremote: Counting objects:  75% (162/216)\u001b[K\rremote: Counting objects:  76% (165/216)\u001b[K\rremote: Counting objects:  77% (167/216)\u001b[K\rremote: Counting objects:  78% (169/216)\u001b[K\rremote: Counting objects:  79% (171/216)\u001b[K\rremote: Counting objects:  80% (173/216)\u001b[K\rremote: Counting objects:  81% (175/216)\u001b[K\rremote: Counting objects:  82% (178/216)\u001b[K\rremote: Counting objects:  83% (180/216)\u001b[K\rremote: Counting objects:  84% (182/216)\u001b[K\rremote: Counting objects:  85% (184/216)\u001b[K\rremote: Counting objects:  86% (186/216)\u001b[K\rremote: Counting objects:  87% (188/216)\u001b[K\rremote: Counting objects:  88% (191/216)\u001b[K\rremote: Counting objects:  89% (193/216)\u001b[K\rremote: Counting objects:  90% (195/216)\u001b[K\rremote: Counting objects:  91% (197/216)\u001b[K\rremote: Counting objects:  92% (199/216)\u001b[K\rremote: Counting objects:  93% (201/216)\u001b[K\rremote: Counting objects:  94% (204/216)\u001b[K\rremote: Counting objects:  95% (206/216)\u001b[K\rremote: Counting objects:  96% (208/216)\u001b[K\rremote: Counting objects:  97% (210/216)\u001b[K\rremote: Counting objects:  98% (212/216)\u001b[K\rremote: Counting objects:  99% (214/216)\u001b[K\rremote: Counting objects: 100% (216/216)\u001b[K\rremote: Counting objects: 100% (216/216), done.\u001b[K\n",
            "remote: Compressing objects: 100% (174/174), done.\u001b[K\n",
            "remote: Total 216 (delta 75), reused 173 (delta 39), pack-reused 0\n",
            "Receiving objects: 100% (216/216), 2.34 MiB | 1.97 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cldi5yNjcCKf",
        "colab_type": "text"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klFTCJwfM0uI",
        "colab_type": "code",
        "outputId": "d180970d-b2d9-4f4e-ee6a-66446e9392da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "%cd /content/iOS-note-v2/src/py\n",
        "!make create-dataset NSAMPLES=20000"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/iOS-note-v2/src/py\n",
            "chmod +x setup.sh\n",
            "make font-setup\n",
            "make[1]: Entering directory '/content/iOS-note-v2/src/py'\n",
            "./setup.sh\n",
            "alanis-hand\n",
            "architext\n",
            "ashcan-bb\n",
            "./setup.sh: 7: [: ashcanbb_bold.ttf: unexpected operator\n",
            "attack-of-the-cucumbers\n",
            "./setup.sh: 7: [: attack: unexpected operator\n",
            "attract-more-women\n",
            "blzee\n",
            "calligravity\n",
            "domestic-manners\n",
            "FH-GoodDogPlain-WTT\n",
            "james-almacen\n",
            "james-fajardo\n",
            "./setup.sh: 7: [: James: unexpected operator\n",
            "khand\n",
            "ladylike-bb\n",
            "mulders-handwriting\n",
            "mumsies\n",
            "Otto\n",
            "pecita\n",
            "quikhand\n",
            "Sophia\n",
            "two-turtle-doves\n",
            "make[1]: Leaving directory '/content/iOS-note-v2/src/py'\n",
            "python create_dataset.py --n_samples 20000\n",
            "100% 20000/20000 [07:23<00:00, 42.49it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muqkdWAPcFYN",
        "colab_type": "text"
      },
      "source": [
        "## Save Dataset in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTQ5ZsJwM0uK",
        "colab_type": "code",
        "outputId": "2a78c3a6-7fba-4fa8-d10a-70d2e75898af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sL6bYdfM0uM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r data/images /content/gdrive/My\\ Drive/AppliedML/\n",
        "!cp data/labels.csv /content/gdrive/My\\ Drive/AppliedML/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juvw9y15cJbr",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKAcedl7UfFE",
        "colab_type": "code",
        "outputId": "e98210d3-d498-4d9d-c5b4-32011b1acd51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 9 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 10 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 10 : Loss : 55.3300858\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 20 : Loss : 45.3969803\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 30 : Loss : 41.1996346\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 40 : Loss : 38.578373\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 50 : Loss : 36.631691\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 60 : Loss : 35.1621323\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 70 : Loss : 34.0582466\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 80 : Loss : 33.0239334\n",
            "WARNING:tensorflow:8 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 90 : Loss : 32.2904396\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 100 : Loss : 31.638855\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 110 : Loss : 31.0868053\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 120 : Loss : 30.6201859\n",
            "WARNING:tensorflow:7 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 130 : Loss : 30.2792091\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 140 : Loss : 29.9107037\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 14 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 150 : Loss : 29.5984821\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:8 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 160 : Loss : 29.2350616\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 14 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 170 : Loss : 28.9162369\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 180 : Loss : 28.6364288\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 190 : Loss : 28.35853\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 200 : Loss : 28.0941\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 210 : Loss : 27.858408\n",
            "WARNING:tensorflow:7 out of the last 15 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 220 : Loss : 27.6097355\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 230 : Loss : 27.3565464\n",
            "Step 240 : Loss : 27.1228943\n",
            "WARNING:tensorflow:5 out of the last 16 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 19 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 250 : Loss : 26.8974762\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 14 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 260 : Loss : 26.6937618\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 270 : Loss : 26.500885\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 15 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 280 : Loss : 26.2761116\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 290 : Loss : 26.0805874\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 300 : Loss : 25.8846283\n",
            "Step 310 : Loss : 25.6690483\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 18 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 320 : Loss : 25.528141\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 330 : Loss : 25.3320484\n",
            "Step 340 : Loss : 25.1396198\n",
            "Step 350 : Loss : 24.9700775\n",
            "WARNING:tensorflow:5 out of the last 18 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 360 : Loss : 24.7984276\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 370 : Loss : 24.641058\n",
            "Step 380 : Loss : 24.4572\n",
            "Step 390 : Loss : 24.3033447\n",
            "Step 400 : Loss : 24.1455879\n",
            "Step 410 : Loss : 23.9760513\n",
            "WARNING:tensorflow:5 out of the last 26 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 420 : Loss : 23.8078899\n",
            "WARNING:tensorflow:6 out of the last 31 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 15 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 430 : Loss : 23.6562748\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 440 : Loss : 23.5147572\n",
            "Step 450 : Loss : 23.3517036\n",
            "Step 460 : Loss : 23.198698\n",
            "Step 470 : Loss : 23.027813\n",
            "Step 480 : Loss : 22.8626537\n",
            "Step 490 : Loss : 22.7037258\n",
            "Step 500 : Loss : 22.5508709\n",
            "Step 510 : Loss : 22.4144115\n",
            "WARNING:tensorflow:5 out of the last 20 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 21 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 520 : Loss : 22.2779236\n",
            "Step 530 : Loss : 22.1270199\n",
            "WARNING:tensorflow:5 out of the last 15 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 540 : Loss : 21.9832439\n",
            "Step 550 : Loss : 21.8354702\n",
            "WARNING:tensorflow:5 out of the last 20 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 560 : Loss : 21.6975689\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 570 : Loss : 21.5657082\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 580 : Loss : 21.4292202\n",
            "Step 590 : Loss : 21.2974491\n",
            "WARNING:tensorflow:5 out of the last 16 calls to <function train_step at 0x7fb062a0f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Step 593 : Loss :  21.2625656\n",
            "Epoch 2\n",
            "Step 600 : Loss : 21.1686172\n",
            "Step 610 : Loss : 21.0365391\n",
            "Step 620 : Loss : 20.9105721\n",
            "Step 630 : Loss : 20.8031921\n",
            "Step 640 : Loss : 20.6868439\n",
            "Step 650 : Loss : 20.5608597\n",
            "Step 660 : Loss : 20.4389172\n",
            "Step 670 : Loss : 20.3082943\n",
            "Step 680 : Loss : 20.1956291\n",
            "Step 690 : Loss : 20.0775223\n",
            "Step 700 : Loss : 19.9706173\n",
            "Step 710 : Loss : 19.8659916\n",
            "Step 720 : Loss : 19.7550564\n",
            "Step 730 : Loss : 19.6451912\n",
            "Step 740 : Loss : 19.5471935\n",
            "Step 750 : Loss : 19.4436398\n",
            "Step 760 : Loss : 19.3299313\n",
            "Step 770 : Loss : 19.2198029\n",
            "Step 780 : Loss : 19.1221714\n",
            "Step 790 : Loss : 19.0232525\n",
            "Step 800 : Loss : 18.9238968\n",
            "Step 810 : Loss : 18.8262634\n",
            "Step 820 : Loss : 18.7279339\n",
            "Step 830 : Loss : 18.6193256\n",
            "Step 840 : Loss : 18.5152607\n",
            "Step 850 : Loss : 18.4227257\n",
            "Step 860 : Loss : 18.3361034\n",
            "Step 870 : Loss : 18.2433\n",
            "Step 880 : Loss : 18.1512833\n",
            "Step 890 : Loss : 18.0566044\n",
            "Step 900 : Loss : 17.960104\n",
            "Step 910 : Loss : 17.8883419\n",
            "Step 920 : Loss : 17.8002529\n",
            "Step 930 : Loss : 17.7078247\n",
            "Step 940 : Loss : 17.6194153\n",
            "Step 950 : Loss : 17.5309353\n",
            "Step 960 : Loss : 17.4550495\n",
            "Step 970 : Loss : 17.3732\n",
            "Step 980 : Loss : 17.2934608\n",
            "Step 990 : Loss : 17.21768\n",
            "Step 1000 : Loss : 17.1437378\n",
            "Step 1010 : Loss : 17.0563812\n",
            "Step 1020 : Loss : 16.9718304\n",
            "Step 1030 : Loss : 16.8998928\n",
            "Step 1040 : Loss : 16.8222561\n",
            "Step 1050 : Loss : 16.743681\n",
            "Step 1060 : Loss : 16.6604671\n",
            "Step 1070 : Loss : 16.5792217\n",
            "Step 1080 : Loss : 16.5052528\n",
            "Step 1090 : Loss : 16.4304256\n",
            "Step 1100 : Loss : 16.3591022\n",
            "Step 1110 : Loss : 16.2851944\n",
            "Step 1120 : Loss : 16.2154465\n",
            "Step 1130 : Loss : 16.1442566\n",
            "Step 1140 : Loss : 16.0751839\n",
            "Step 1150 : Loss : 16.0047569\n",
            "Step 1160 : Loss : 15.9402781\n",
            "Step 1170 : Loss : 15.86796\n",
            "Step 1180 : Loss : 15.8014202\n",
            "Step 1186 : Loss :  15.7611322\n",
            "Epoch 3\n",
            "Step 1190 : Loss : 15.7369633\n",
            "Step 1200 : Loss : 15.6648645\n",
            "Step 1210 : Loss : 15.5984955\n",
            "Step 1220 : Loss : 15.5389948\n",
            "Step 1230 : Loss : 15.4800224\n",
            "Step 1240 : Loss : 15.412158\n",
            "Step 1250 : Loss : 15.3493967\n",
            "Step 1260 : Loss : 15.285243\n",
            "Step 1270 : Loss : 15.2202234\n",
            "Step 1280 : Loss : 15.1595564\n",
            "Step 1290 : Loss : 15.097908\n",
            "Step 1300 : Loss : 15.0345793\n",
            "Step 1310 : Loss : 14.9770994\n",
            "Step 1320 : Loss : 14.9181967\n",
            "Step 1330 : Loss : 14.8608818\n",
            "Step 1340 : Loss : 14.8042326\n",
            "Step 1350 : Loss : 14.7458334\n",
            "Step 1360 : Loss : 14.682929\n",
            "Step 1370 : Loss : 14.6280069\n",
            "Step 1380 : Loss : 14.5722885\n",
            "Step 1390 : Loss : 14.5193348\n",
            "Step 1400 : Loss : 14.4596109\n",
            "Step 1410 : Loss : 14.408946\n",
            "Step 1420 : Loss : 14.3504238\n",
            "Step 1430 : Loss : 14.2924528\n",
            "Step 1440 : Loss : 14.2359171\n",
            "Step 1450 : Loss : 14.1838312\n",
            "Step 1460 : Loss : 14.1322041\n",
            "Step 1470 : Loss : 14.0748615\n",
            "Step 1480 : Loss : 14.0242376\n",
            "Step 1490 : Loss : 13.9705658\n",
            "Step 1500 : Loss : 13.9222746\n",
            "Step 1510 : Loss : 13.8741989\n",
            "Step 1520 : Loss : 13.8192892\n",
            "Step 1530 : Loss : 13.7689028\n",
            "Step 1540 : Loss : 13.7133303\n",
            "Step 1550 : Loss : 13.6658306\n",
            "Step 1560 : Loss : 13.6176157\n",
            "Step 1570 : Loss : 13.5708437\n",
            "Step 1580 : Loss : 13.5249138\n",
            "Step 1590 : Loss : 13.4787636\n",
            "Step 1600 : Loss : 13.4293079\n",
            "Step 1610 : Loss : 13.3823633\n",
            "Step 1620 : Loss : 13.3355274\n",
            "Step 1630 : Loss : 13.2929535\n",
            "Step 1640 : Loss : 13.2440796\n",
            "Step 1650 : Loss : 13.1959829\n",
            "Step 1660 : Loss : 13.1453247\n",
            "Step 1670 : Loss : 13.0977869\n",
            "Step 1680 : Loss : 13.0521097\n",
            "Step 1690 : Loss : 13.005723\n",
            "Step 1700 : Loss : 12.9618473\n",
            "Step 1710 : Loss : 12.9169788\n",
            "Step 1720 : Loss : 12.8707657\n",
            "Step 1730 : Loss : 12.8282461\n",
            "Step 1740 : Loss : 12.7838573\n",
            "Step 1750 : Loss : 12.7426262\n",
            "Step 1760 : Loss : 12.6983299\n",
            "Step 1770 : Loss : 12.6527166\n",
            "Step 1779 : Loss :  12.6137781\n",
            "Epoch 4\n",
            "Step 1780 : Loss : 12.6092281\n",
            "Step 1790 : Loss : 12.5648813\n",
            "Step 1800 : Loss : 12.5195894\n",
            "Step 1810 : Loss : 12.4827747\n",
            "Step 1820 : Loss : 12.4436741\n",
            "Step 1830 : Loss : 12.3990803\n",
            "Step 1840 : Loss : 12.3557529\n",
            "Step 1850 : Loss : 12.317379\n",
            "Step 1860 : Loss : 12.2732239\n",
            "Step 1870 : Loss : 12.2316103\n",
            "Step 1880 : Loss : 12.1915846\n",
            "Step 1890 : Loss : 12.1501465\n",
            "Step 1900 : Loss : 12.1089077\n",
            "Step 1910 : Loss : 12.0685577\n",
            "Step 1920 : Loss : 12.0304041\n",
            "Step 1930 : Loss : 11.9916344\n",
            "Step 1940 : Loss : 11.9525242\n",
            "Step 1950 : Loss : 11.9111557\n",
            "Step 1960 : Loss : 11.8710699\n",
            "Step 1970 : Loss : 11.8326902\n",
            "Step 1980 : Loss : 11.7952938\n",
            "Step 1990 : Loss : 11.7557087\n",
            "Step 2000 : Loss : 11.7188482\n",
            "Step 2010 : Loss : 11.681591\n",
            "Step 2020 : Loss : 11.6424332\n",
            "Step 2030 : Loss : 11.6033802\n",
            "Step 2040 : Loss : 11.5686741\n",
            "Step 2050 : Loss : 11.5320024\n",
            "Step 2060 : Loss : 11.4928608\n",
            "Step 2070 : Loss : 11.4560995\n",
            "Step 2080 : Loss : 11.4206238\n",
            "Step 2090 : Loss : 11.383646\n",
            "Step 2100 : Loss : 11.351819\n",
            "Step 2110 : Loss : 11.3152561\n",
            "Step 2120 : Loss : 11.2785816\n",
            "Step 2130 : Loss : 11.2406597\n",
            "Step 2140 : Loss : 11.2045193\n",
            "Step 2150 : Loss : 11.1708975\n",
            "Step 2160 : Loss : 11.1351461\n",
            "Step 2170 : Loss : 11.1032429\n",
            "Step 2180 : Loss : 11.070446\n",
            "Step 2190 : Loss : 11.0380955\n",
            "Step 2200 : Loss : 11.0060263\n",
            "Step 2210 : Loss : 10.9732647\n",
            "Step 2220 : Loss : 10.9425526\n",
            "Step 2230 : Loss : 10.9074516\n",
            "Step 2240 : Loss : 10.8745184\n",
            "Step 2250 : Loss : 10.8377266\n",
            "Step 2260 : Loss : 10.8037577\n",
            "Step 2270 : Loss : 10.7701225\n",
            "Step 2280 : Loss : 10.7360125\n",
            "Step 2290 : Loss : 10.7035685\n",
            "Step 2300 : Loss : 10.6705856\n",
            "Step 2310 : Loss : 10.6379147\n",
            "Step 2320 : Loss : 10.6039648\n",
            "Step 2330 : Loss : 10.5734549\n",
            "Step 2340 : Loss : 10.540329\n",
            "Step 2350 : Loss : 10.5087862\n",
            "Step 2360 : Loss : 10.4757519\n",
            "Step 2370 : Loss : 10.4446163\n",
            "Step 2372 : Loss :  10.4379644\n",
            "Epoch 5\n",
            "Step 2380 : Loss : 10.4116392\n",
            "Step 2390 : Loss : 10.3791122\n",
            "Step 2400 : Loss : 10.3483362\n",
            "Step 2410 : Loss : 10.3190022\n",
            "Step 2420 : Loss : 10.2863398\n",
            "Step 2430 : Loss : 10.254981\n",
            "Step 2440 : Loss : 10.2255239\n",
            "Step 2450 : Loss : 10.1933603\n",
            "Step 2460 : Loss : 10.1624479\n",
            "Step 2470 : Loss : 10.1319771\n",
            "Step 2480 : Loss : 10.1026907\n",
            "Step 2490 : Loss : 10.0737209\n",
            "Step 2500 : Loss : 10.0451403\n",
            "Step 2510 : Loss : 10.0164948\n",
            "Step 2520 : Loss : 9.98908\n",
            "Step 2530 : Loss : 9.96029663\n",
            "Step 2540 : Loss : 9.93041611\n",
            "Step 2550 : Loss : 9.90134144\n",
            "Step 2560 : Loss : 9.87343884\n",
            "Step 2570 : Loss : 9.84516048\n",
            "Step 2580 : Loss : 9.8164\n",
            "Step 2590 : Loss : 9.78867531\n",
            "Step 2600 : Loss : 9.76386642\n",
            "Step 2610 : Loss : 9.73551369\n",
            "Step 2620 : Loss : 9.70845318\n",
            "Step 2630 : Loss : 9.68193054\n",
            "Step 2640 : Loss : 9.65521049\n",
            "Step 2650 : Loss : 9.62754631\n",
            "Step 2660 : Loss : 9.60001\n",
            "Step 2670 : Loss : 9.5727253\n",
            "Step 2680 : Loss : 9.54490185\n",
            "Step 2690 : Loss : 9.51982784\n",
            "Step 2700 : Loss : 9.49263191\n",
            "Step 2710 : Loss : 9.46583557\n",
            "Step 2720 : Loss : 9.43865776\n",
            "Step 2730 : Loss : 9.41117859\n",
            "Step 2740 : Loss : 9.38453293\n",
            "Step 2750 : Loss : 9.35852814\n",
            "Step 2760 : Loss : 9.33261681\n",
            "Step 2770 : Loss : 9.30835247\n",
            "Step 2780 : Loss : 9.28429794\n",
            "Step 2790 : Loss : 9.26013088\n",
            "Step 2800 : Loss : 9.23402309\n",
            "Step 2810 : Loss : 9.2107687\n",
            "Step 2820 : Loss : 9.1855545\n",
            "Step 2830 : Loss : 9.16038227\n",
            "Step 2840 : Loss : 9.13380527\n",
            "Step 2850 : Loss : 9.10764503\n",
            "Step 2860 : Loss : 9.0827446\n",
            "Step 2870 : Loss : 9.05724812\n",
            "Step 2880 : Loss : 9.0323925\n",
            "Step 2890 : Loss : 9.00650597\n",
            "Step 2900 : Loss : 8.98148441\n",
            "Step 2910 : Loss : 8.9566\n",
            "Step 2920 : Loss : 8.9321022\n",
            "Step 2930 : Loss : 8.90717411\n",
            "Step 2940 : Loss : 8.88326359\n",
            "Step 2950 : Loss : 8.85866737\n",
            "Step 2960 : Loss : 8.8352108\n",
            "Step 2965 : Loss :  8.82348061\n",
            "Epoch 6\n",
            "Step 2970 : Loss : 8.81168938\n",
            "Step 2980 : Loss : 8.78744507\n",
            "Step 2990 : Loss : 8.76330376\n",
            "Step 3000 : Loss : 8.74062157\n",
            "Step 3010 : Loss : 8.71657848\n",
            "Step 3020 : Loss : 8.69292641\n",
            "Step 3030 : Loss : 8.66986465\n",
            "Step 3040 : Loss : 8.64708233\n",
            "Step 3050 : Loss : 8.62389183\n",
            "Step 3060 : Loss : 8.60103321\n",
            "Step 3070 : Loss : 8.57844162\n",
            "Step 3080 : Loss : 8.55548763\n",
            "Step 3090 : Loss : 8.53395367\n",
            "Step 3100 : Loss : 8.51238537\n",
            "Step 3110 : Loss : 8.4911\n",
            "Step 3120 : Loss : 8.4693718\n",
            "Step 3130 : Loss : 8.44926929\n",
            "Step 3140 : Loss : 8.42749786\n",
            "Step 3150 : Loss : 8.40674686\n",
            "Step 3160 : Loss : 8.38563251\n",
            "Step 3170 : Loss : 8.36454201\n",
            "Step 3180 : Loss : 8.34268379\n",
            "Step 3190 : Loss : 8.32300472\n",
            "Step 3200 : Loss : 8.30216122\n",
            "Step 3210 : Loss : 8.28134346\n",
            "Step 3220 : Loss : 8.26151848\n",
            "Step 3230 : Loss : 8.2416935\n",
            "Step 3240 : Loss : 8.22170353\n",
            "Step 3250 : Loss : 8.20041752\n",
            "Step 3260 : Loss : 8.1801157\n",
            "Step 3270 : Loss : 8.15887547\n",
            "Step 3280 : Loss : 8.13825607\n",
            "Step 3290 : Loss : 8.11808\n",
            "Step 3300 : Loss : 8.09780598\n",
            "Step 3310 : Loss : 8.07795811\n",
            "Step 3320 : Loss : 8.05694\n",
            "Step 3330 : Loss : 8.0369\n",
            "Step 3340 : Loss : 8.01678848\n",
            "Step 3350 : Loss : 7.99659538\n",
            "Step 3360 : Loss : 7.97699594\n",
            "Step 3370 : Loss : 7.95798826\n",
            "Step 3380 : Loss : 7.93843555\n",
            "Step 3390 : Loss : 7.91886282\n",
            "Step 3400 : Loss : 7.90032816\n",
            "Step 3410 : Loss : 7.88118696\n",
            "Step 3420 : Loss : 7.86167955\n",
            "Step 3430 : Loss : 7.84235048\n",
            "Step 3440 : Loss : 7.82285213\n",
            "Step 3450 : Loss : 7.80409718\n",
            "Step 3460 : Loss : 7.78552961\n",
            "Step 3470 : Loss : 7.76745558\n",
            "Step 3480 : Loss : 7.74899721\n",
            "Step 3490 : Loss : 7.73087502\n",
            "Step 3500 : Loss : 7.71241\n",
            "Step 3510 : Loss : 7.69500446\n",
            "Step 3520 : Loss : 7.67667055\n",
            "Step 3530 : Loss : 7.65895414\n",
            "Step 3540 : Loss : 7.64075804\n",
            "Step 3550 : Loss : 7.622715\n",
            "Step 3558 : Loss :  7.60844803\n",
            "Epoch 7\n",
            "Step 3560 : Loss : 7.60473728\n",
            "Step 3570 : Loss : 7.586308\n",
            "Step 3580 : Loss : 7.56824493\n",
            "Step 3590 : Loss : 7.55071211\n",
            "Step 3600 : Loss : 7.5331316\n",
            "Step 3610 : Loss : 7.5156455\n",
            "Step 3620 : Loss : 7.49784\n",
            "Step 3630 : Loss : 7.48116159\n",
            "Step 3640 : Loss : 7.46403885\n",
            "Step 3650 : Loss : 7.44710732\n",
            "Step 3660 : Loss : 7.4304204\n",
            "Step 3670 : Loss : 7.41320562\n",
            "Step 3680 : Loss : 7.39595318\n",
            "Step 3690 : Loss : 7.37892056\n",
            "Step 3700 : Loss : 7.36234379\n",
            "Step 3710 : Loss : 7.3460536\n",
            "Step 3720 : Loss : 7.32956219\n",
            "Step 3730 : Loss : 7.31272793\n",
            "Step 3740 : Loss : 7.29623413\n",
            "Step 3750 : Loss : 7.28040791\n",
            "Step 3760 : Loss : 7.26456881\n",
            "Step 3770 : Loss : 7.24871302\n",
            "Step 3780 : Loss : 7.23304272\n",
            "Step 3790 : Loss : 7.21728659\n",
            "Step 3800 : Loss : 7.20075369\n",
            "Step 3810 : Loss : 7.18447876\n",
            "Step 3820 : Loss : 7.16884947\n",
            "Step 3830 : Loss : 7.15306139\n",
            "Step 3840 : Loss : 7.13673353\n",
            "Step 3850 : Loss : 7.1212182\n",
            "Step 3860 : Loss : 7.10521\n",
            "Step 3870 : Loss : 7.0889864\n",
            "Step 3880 : Loss : 7.07346249\n",
            "Step 3890 : Loss : 7.05760527\n",
            "Step 3900 : Loss : 7.0426259\n",
            "Step 3910 : Loss : 7.02727\n",
            "Step 3920 : Loss : 7.01243162\n",
            "Step 3930 : Loss : 6.99796867\n",
            "Step 3940 : Loss : 6.98301125\n",
            "Step 3950 : Loss : 6.96818733\n",
            "Step 3960 : Loss : 6.95302868\n",
            "Step 3970 : Loss : 6.93809557\n",
            "Step 3980 : Loss : 6.923594\n",
            "Step 3990 : Loss : 6.9090209\n",
            "Step 4000 : Loss : 6.89459896\n",
            "Step 4010 : Loss : 6.87941408\n",
            "Step 4020 : Loss : 6.86459064\n",
            "Step 4030 : Loss : 6.84945488\n",
            "Step 4040 : Loss : 6.83441162\n",
            "Step 4050 : Loss : 6.81988621\n",
            "Step 4060 : Loss : 6.80483341\n",
            "Step 4070 : Loss : 6.79026651\n",
            "Step 4080 : Loss : 6.7757082\n",
            "Step 4090 : Loss : 6.76173496\n",
            "Step 4100 : Loss : 6.7472744\n",
            "Step 4110 : Loss : 6.73287582\n",
            "Step 4120 : Loss : 6.71886873\n",
            "Step 4130 : Loss : 6.70495367\n",
            "Step 4140 : Loss : 6.69092512\n",
            "Step 4150 : Loss : 6.67701578\n",
            "Step 4151 : Loss :  6.67552853\n",
            "Epoch 8\n",
            "Step 4160 : Loss : 6.66315413\n",
            "Step 4170 : Loss : 6.64964199\n",
            "Step 4180 : Loss : 6.63570309\n",
            "Step 4190 : Loss : 6.62197638\n",
            "Step 4200 : Loss : 6.60823441\n",
            "Step 4210 : Loss : 6.59478092\n",
            "Step 4220 : Loss : 6.58130741\n",
            "Step 4230 : Loss : 6.56793308\n",
            "Step 4240 : Loss : 6.55453968\n",
            "Step 4250 : Loss : 6.54130459\n",
            "Step 4260 : Loss : 6.52803755\n",
            "Step 4270 : Loss : 6.51496649\n",
            "Step 4280 : Loss : 6.50212097\n",
            "Step 4290 : Loss : 6.48900127\n",
            "Step 4300 : Loss : 6.47612\n",
            "Step 4310 : Loss : 6.46292257\n",
            "Step 4320 : Loss : 6.44976282\n",
            "Step 4330 : Loss : 6.43644047\n",
            "Step 4340 : Loss : 6.42360401\n",
            "Step 4350 : Loss : 6.41112804\n",
            "Step 4360 : Loss : 6.39863729\n",
            "Step 4370 : Loss : 6.3858614\n",
            "Step 4380 : Loss : 6.37348795\n",
            "Step 4390 : Loss : 6.36050177\n",
            "Step 4400 : Loss : 6.34779596\n",
            "Step 4410 : Loss : 6.33527613\n",
            "Step 4420 : Loss : 6.32266617\n",
            "Step 4430 : Loss : 6.31010437\n",
            "Step 4440 : Loss : 6.29773426\n",
            "Step 4450 : Loss : 6.28522968\n",
            "Step 4460 : Loss : 6.27254438\n",
            "Step 4470 : Loss : 6.26039267\n",
            "Step 4480 : Loss : 6.24765587\n",
            "Step 4490 : Loss : 6.23545599\n",
            "Step 4500 : Loss : 6.22337294\n",
            "Step 4510 : Loss : 6.21110868\n",
            "Step 4520 : Loss : 6.19945431\n",
            "Step 4530 : Loss : 6.18726778\n",
            "Step 4540 : Loss : 6.17516851\n",
            "Step 4550 : Loss : 6.16328621\n",
            "Step 4560 : Loss : 6.15122843\n",
            "Step 4570 : Loss : 6.13913\n",
            "Step 4580 : Loss : 6.12704849\n",
            "Step 4590 : Loss : 6.1157465\n",
            "Step 4600 : Loss : 6.10387039\n",
            "Step 4610 : Loss : 6.09209156\n",
            "Step 4620 : Loss : 6.07991695\n",
            "Step 4630 : Loss : 6.06806421\n",
            "Step 4640 : Loss : 6.05619383\n",
            "Step 4650 : Loss : 6.04427242\n",
            "Step 4660 : Loss : 6.03261137\n",
            "Step 4670 : Loss : 6.0207839\n",
            "Step 4680 : Loss : 6.00930738\n",
            "Step 4690 : Loss : 5.99760056\n",
            "Step 4700 : Loss : 5.98608494\n",
            "Step 4710 : Loss : 5.97461176\n",
            "Step 4720 : Loss : 5.96352863\n",
            "Step 4730 : Loss : 5.95219612\n",
            "Step 4740 : Loss : 5.94085503\n",
            "Step 4744 : Loss :  5.93638802\n",
            "Epoch 9\n",
            "Step 4750 : Loss : 5.92949152\n",
            "Step 4760 : Loss : 5.91837835\n",
            "Step 4770 : Loss : 5.90718794\n",
            "Step 4780 : Loss : 5.89614296\n",
            "Step 4790 : Loss : 5.88522482\n",
            "Step 4800 : Loss : 5.87434912\n",
            "Step 4810 : Loss : 5.86341953\n",
            "Step 4820 : Loss : 5.85241413\n",
            "Step 4830 : Loss : 5.84137106\n",
            "Step 4840 : Loss : 5.83040857\n",
            "Step 4850 : Loss : 5.81970501\n",
            "Step 4860 : Loss : 5.80898476\n",
            "Step 4870 : Loss : 5.79843569\n",
            "Step 4880 : Loss : 5.78787279\n",
            "Step 4890 : Loss : 5.77728\n",
            "Step 4900 : Loss : 5.76635695\n",
            "Step 4910 : Loss : 5.7556262\n",
            "Step 4920 : Loss : 5.74497843\n",
            "Step 4930 : Loss : 5.7343092\n",
            "Step 4940 : Loss : 5.7239275\n",
            "Step 4950 : Loss : 5.71332169\n",
            "Step 4960 : Loss : 5.70284033\n",
            "Step 4970 : Loss : 5.69269\n",
            "Step 4980 : Loss : 5.68243456\n",
            "Step 4990 : Loss : 5.6721096\n",
            "Step 5000 : Loss : 5.66191149\n",
            "Step 5010 : Loss : 5.65172195\n",
            "Step 5020 : Loss : 5.64159393\n",
            "Step 5030 : Loss : 5.63136101\n",
            "Step 5040 : Loss : 5.62147903\n",
            "Step 5050 : Loss : 5.61135721\n",
            "Step 5060 : Loss : 5.6013813\n",
            "Step 5070 : Loss : 5.59142685\n",
            "Step 5080 : Loss : 5.58141661\n",
            "Step 5090 : Loss : 5.57176\n",
            "Step 5100 : Loss : 5.5618453\n",
            "Step 5110 : Loss : 5.55220556\n",
            "Step 5120 : Loss : 5.54250622\n",
            "Step 5130 : Loss : 5.53276443\n",
            "Step 5140 : Loss : 5.5232\n",
            "Step 5150 : Loss : 5.51364183\n",
            "Step 5160 : Loss : 5.50426722\n",
            "Step 5170 : Loss : 5.49460793\n",
            "Step 5180 : Loss : 5.4854517\n",
            "Step 5190 : Loss : 5.47605276\n",
            "Step 5200 : Loss : 5.46670294\n",
            "Step 5210 : Loss : 5.45744944\n",
            "Step 5220 : Loss : 5.44790077\n",
            "Step 5230 : Loss : 5.43847561\n",
            "Step 5240 : Loss : 5.42902946\n",
            "Step 5250 : Loss : 5.4197793\n",
            "Step 5260 : Loss : 5.4102\n",
            "Step 5270 : Loss : 5.40082216\n",
            "Step 5280 : Loss : 5.39134884\n",
            "Step 5290 : Loss : 5.38198\n",
            "Step 5300 : Loss : 5.37267303\n",
            "Step 5310 : Loss : 5.36337376\n",
            "Step 5320 : Loss : 5.35415936\n",
            "Step 5330 : Loss : 5.34511185\n",
            "Step 5337 : Loss :  5.33870506\n",
            "Epoch 10\n",
            "Step 5340 : Loss : 5.33593082\n",
            "Step 5350 : Loss : 5.32668877\n",
            "Step 5360 : Loss : 5.31751633\n",
            "Step 5370 : Loss : 5.30836678\n",
            "Step 5380 : Loss : 5.29916668\n",
            "Step 5390 : Loss : 5.2900877\n",
            "Step 5400 : Loss : 5.28096151\n",
            "Step 5410 : Loss : 5.27194643\n",
            "Step 5420 : Loss : 5.26281548\n",
            "Step 5430 : Loss : 5.25387907\n",
            "Step 5440 : Loss : 5.24487448\n",
            "Step 5450 : Loss : 5.23600054\n",
            "Step 5460 : Loss : 5.22739697\n",
            "Step 5470 : Loss : 5.21878958\n",
            "Step 5480 : Loss : 5.21009493\n",
            "Step 5490 : Loss : 5.20128679\n",
            "Step 5500 : Loss : 5.1927247\n",
            "Step 5510 : Loss : 5.18390179\n",
            "Step 5520 : Loss : 5.17531347\n",
            "Step 5530 : Loss : 5.16666126\n",
            "Step 5540 : Loss : 5.15797615\n",
            "Step 5550 : Loss : 5.14941883\n",
            "Step 5560 : Loss : 5.14082623\n",
            "Step 5570 : Loss : 5.13241434\n",
            "Step 5580 : Loss : 5.12379503\n",
            "Step 5590 : Loss : 5.11535645\n",
            "Step 5600 : Loss : 5.10695887\n",
            "Step 5610 : Loss : 5.09837627\n",
            "Step 5620 : Loss : 5.08984566\n",
            "Step 5630 : Loss : 5.08164549\n",
            "Step 5640 : Loss : 5.07325459\n",
            "Step 5650 : Loss : 5.06501341\n",
            "Step 5660 : Loss : 5.05688477\n",
            "Step 5670 : Loss : 5.04854345\n",
            "Step 5680 : Loss : 5.04016304\n",
            "Step 5690 : Loss : 5.03183556\n",
            "Step 5700 : Loss : 5.02362633\n",
            "Step 5710 : Loss : 5.01568365\n",
            "Step 5720 : Loss : 5.00758171\n",
            "Step 5730 : Loss : 4.99956083\n",
            "Step 5740 : Loss : 4.99162245\n",
            "Step 5750 : Loss : 4.98373079\n",
            "Step 5760 : Loss : 4.97588921\n",
            "Step 5770 : Loss : 4.96805191\n",
            "Step 5780 : Loss : 4.96038961\n",
            "Step 5790 : Loss : 4.95253325\n",
            "Step 5800 : Loss : 4.9445591\n",
            "Step 5810 : Loss : 4.93676233\n",
            "Step 5820 : Loss : 4.92922688\n",
            "Step 5830 : Loss : 4.9215374\n",
            "Step 5840 : Loss : 4.91388273\n",
            "Step 5850 : Loss : 4.90636\n",
            "Step 5860 : Loss : 4.89865065\n",
            "Step 5870 : Loss : 4.89090252\n",
            "Step 5880 : Loss : 4.88318348\n",
            "Step 5890 : Loss : 4.87546587\n",
            "Step 5900 : Loss : 4.86776829\n",
            "Step 5910 : Loss : 4.86004448\n",
            "Step 5920 : Loss : 4.85225821\n",
            "Step 5930 : Loss : 4.84454632\n",
            "Step 5930 : Loss :  4.84454632\n",
            "Epoch 11\n",
            "Step 5940 : Loss : 4.83688593\n",
            "Step 5950 : Loss : 4.82926083\n",
            "Step 5960 : Loss : 4.82171631\n",
            "Step 5970 : Loss : 4.81410694\n",
            "Step 5980 : Loss : 4.80657578\n",
            "Step 5990 : Loss : 4.79909658\n",
            "Step 6000 : Loss : 4.7916007\n",
            "Step 6010 : Loss : 4.78401709\n",
            "Step 6020 : Loss : 4.77646303\n",
            "Step 6030 : Loss : 4.76901817\n",
            "Step 6040 : Loss : 4.76161146\n",
            "Step 6050 : Loss : 4.75424719\n",
            "Step 6060 : Loss : 4.74682713\n",
            "Step 6070 : Loss : 4.7395134\n",
            "Step 6080 : Loss : 4.73218346\n",
            "Step 6090 : Loss : 4.72487211\n",
            "Step 6100 : Loss : 4.71767759\n",
            "Step 6110 : Loss : 4.7104764\n",
            "Step 6120 : Loss : 4.70320702\n",
            "Step 6130 : Loss : 4.69603109\n",
            "Step 6140 : Loss : 4.68880701\n",
            "Step 6150 : Loss : 4.68161058\n",
            "Step 6160 : Loss : 4.67453432\n",
            "Step 6170 : Loss : 4.66755295\n",
            "Step 6180 : Loss : 4.66045\n",
            "Step 6190 : Loss : 4.65340805\n",
            "Step 6200 : Loss : 4.64636898\n",
            "Step 6210 : Loss : 4.63933754\n",
            "Step 6220 : Loss : 4.63249588\n",
            "Step 6230 : Loss : 4.62578297\n",
            "Step 6240 : Loss : 4.6189785\n",
            "Step 6250 : Loss : 4.6124568\n",
            "Step 6260 : Loss : 4.60573244\n",
            "Step 6270 : Loss : 4.59893084\n",
            "Step 6280 : Loss : 4.59207201\n",
            "Step 6290 : Loss : 4.58529615\n",
            "Step 6300 : Loss : 4.57851791\n",
            "Step 6310 : Loss : 4.57174\n",
            "Step 6320 : Loss : 4.5650754\n",
            "Step 6330 : Loss : 4.55845547\n",
            "Step 6340 : Loss : 4.55188513\n",
            "Step 6350 : Loss : 4.54529572\n",
            "Step 6360 : Loss : 4.53872919\n",
            "Step 6370 : Loss : 4.53246832\n",
            "Step 6380 : Loss : 4.52605629\n",
            "Step 6390 : Loss : 4.51972342\n",
            "Step 6400 : Loss : 4.51316309\n",
            "Step 6410 : Loss : 4.50668097\n",
            "Step 6420 : Loss : 4.50037861\n",
            "Step 6430 : Loss : 4.49402618\n",
            "Step 6440 : Loss : 4.48785\n",
            "Step 6450 : Loss : 4.48162079\n",
            "Step 6460 : Loss : 4.47534513\n",
            "Step 6470 : Loss : 4.46912575\n",
            "Step 6480 : Loss : 4.46288681\n",
            "Step 6490 : Loss : 4.45658541\n",
            "Step 6500 : Loss : 4.45030499\n",
            "Step 6510 : Loss : 4.44412899\n",
            "Step 6520 : Loss : 4.43798351\n",
            "Step 6523 : Loss :  4.43606806\n",
            "Epoch 12\n",
            "Step 6530 : Loss : 4.43173027\n",
            "Step 6540 : Loss : 4.42550087\n",
            "Step 6550 : Loss : 4.41940594\n",
            "Step 6560 : Loss : 4.41320705\n",
            "Step 6570 : Loss : 4.40706873\n",
            "Step 6580 : Loss : 4.40106726\n",
            "Step 6590 : Loss : 4.39495611\n",
            "Step 6600 : Loss : 4.38880396\n",
            "Step 6610 : Loss : 4.38267612\n",
            "Step 6620 : Loss : 4.37657404\n",
            "Step 6630 : Loss : 4.37069845\n",
            "Step 6640 : Loss : 4.36456347\n",
            "Step 6650 : Loss : 4.35859442\n",
            "Step 6660 : Loss : 4.35266399\n",
            "Step 6670 : Loss : 4.3467021\n",
            "Step 6680 : Loss : 4.3406682\n",
            "Step 6690 : Loss : 4.33466768\n",
            "Step 6700 : Loss : 4.32878256\n",
            "Step 6710 : Loss : 4.32291174\n",
            "Step 6720 : Loss : 4.31701422\n",
            "Step 6730 : Loss : 4.31117678\n",
            "Step 6740 : Loss : 4.30532694\n",
            "Step 6750 : Loss : 4.29944944\n",
            "Step 6760 : Loss : 4.29351711\n",
            "Step 6770 : Loss : 4.28758097\n",
            "Step 6780 : Loss : 4.28174305\n",
            "Step 6790 : Loss : 4.2759943\n",
            "Step 6800 : Loss : 4.27032423\n",
            "Step 6810 : Loss : 4.26459837\n",
            "Step 6820 : Loss : 4.25903082\n",
            "Step 6830 : Loss : 4.25331831\n",
            "Step 6840 : Loss : 4.24797058\n",
            "Step 6850 : Loss : 4.24234056\n",
            "Step 6860 : Loss : 4.23685694\n",
            "Step 6870 : Loss : 4.23119783\n",
            "Step 6880 : Loss : 4.22548056\n",
            "Step 6890 : Loss : 4.21992826\n",
            "Step 6900 : Loss : 4.21434307\n",
            "Step 6910 : Loss : 4.20887852\n",
            "Step 6920 : Loss : 4.20335865\n",
            "Step 6930 : Loss : 4.1978755\n",
            "Step 6940 : Loss : 4.19232225\n",
            "Step 6950 : Loss : 4.18668699\n",
            "Step 6960 : Loss : 4.18120337\n",
            "Step 6970 : Loss : 4.17569733\n",
            "Step 6980 : Loss : 4.17024946\n",
            "Step 6990 : Loss : 4.16469049\n",
            "Step 7000 : Loss : 4.1591835\n",
            "Step 7010 : Loss : 4.15382\n",
            "Step 7020 : Loss : 4.14851284\n",
            "Step 7030 : Loss : 4.14322424\n",
            "Step 7040 : Loss : 4.13816404\n",
            "Step 7050 : Loss : 4.13288069\n",
            "Step 7060 : Loss : 4.12761641\n",
            "Step 7070 : Loss : 4.12244\n",
            "Step 7080 : Loss : 4.11717129\n",
            "Step 7090 : Loss : 4.11214399\n",
            "Step 7100 : Loss : 4.10694885\n",
            "Step 7110 : Loss : 4.10184193\n",
            "Step 7116 : Loss :  4.09869146\n",
            "Epoch 13\n",
            "Step 7120 : Loss : 4.0966239\n",
            "Step 7130 : Loss : 4.09137106\n",
            "Step 7140 : Loss : 4.08621788\n",
            "Step 7150 : Loss : 4.08097124\n",
            "Step 7160 : Loss : 4.07577372\n",
            "Step 7170 : Loss : 4.07051563\n",
            "Step 7180 : Loss : 4.06548071\n",
            "Step 7190 : Loss : 4.06027603\n",
            "Step 7200 : Loss : 4.0552\n",
            "Step 7210 : Loss : 4.05016518\n",
            "Step 7220 : Loss : 4.04506\n",
            "Step 7230 : Loss : 4.04002333\n",
            "Step 7240 : Loss : 4.03491354\n",
            "Step 7250 : Loss : 4.0300827\n",
            "Step 7260 : Loss : 4.02512932\n",
            "Step 7270 : Loss : 4.02016878\n",
            "Step 7280 : Loss : 4.01522207\n",
            "Step 7290 : Loss : 4.01023865\n",
            "Step 7300 : Loss : 4.00524378\n",
            "Step 7310 : Loss : 4.00044632\n",
            "Step 7320 : Loss : 3.99550104\n",
            "Step 7330 : Loss : 3.99049902\n",
            "Step 7340 : Loss : 3.98565\n",
            "Step 7350 : Loss : 3.98074555\n",
            "Step 7360 : Loss : 3.97584867\n",
            "Step 7370 : Loss : 3.9709909\n",
            "Step 7380 : Loss : 3.96609354\n",
            "Step 7390 : Loss : 3.96119595\n",
            "Step 7400 : Loss : 3.95641708\n",
            "Step 7410 : Loss : 3.95163536\n",
            "Step 7420 : Loss : 3.94694614\n",
            "Step 7430 : Loss : 3.94216299\n",
            "Step 7440 : Loss : 3.93757033\n",
            "Step 7450 : Loss : 3.93286371\n",
            "Step 7460 : Loss : 3.92812848\n",
            "Step 7470 : Loss : 3.92331314\n",
            "Step 7480 : Loss : 3.91855097\n",
            "Step 7490 : Loss : 3.91374612\n",
            "Step 7500 : Loss : 3.90905809\n",
            "Step 7510 : Loss : 3.90434074\n",
            "Step 7520 : Loss : 3.89971542\n",
            "Step 7530 : Loss : 3.89504528\n",
            "Step 7540 : Loss : 3.89041877\n",
            "Step 7550 : Loss : 3.88573241\n",
            "Step 7560 : Loss : 3.88114023\n",
            "Step 7570 : Loss : 3.87657714\n",
            "Step 7580 : Loss : 3.87194395\n",
            "Step 7590 : Loss : 3.86728597\n",
            "Step 7600 : Loss : 3.86281395\n",
            "Step 7610 : Loss : 3.85834694\n",
            "Step 7620 : Loss : 3.85384321\n",
            "Step 7630 : Loss : 3.84949923\n",
            "Step 7640 : Loss : 3.84505534\n",
            "Step 7650 : Loss : 3.84057426\n",
            "Step 7660 : Loss : 3.83612752\n",
            "Step 7670 : Loss : 3.83167815\n",
            "Step 7680 : Loss : 3.82725859\n",
            "Step 7690 : Loss : 3.82283139\n",
            "Step 7700 : Loss : 3.81840706\n",
            "Step 7709 : Loss :  3.81445479\n",
            "Epoch 14\n",
            "Step 7710 : Loss : 3.81406379\n",
            "Step 7720 : Loss : 3.80960512\n",
            "Step 7730 : Loss : 3.80520439\n",
            "Step 7740 : Loss : 3.80100179\n",
            "Step 7750 : Loss : 3.79669476\n",
            "Step 7760 : Loss : 3.7923007\n",
            "Step 7770 : Loss : 3.78821898\n",
            "Step 7780 : Loss : 3.78383\n",
            "Step 7790 : Loss : 3.77939582\n",
            "Step 7800 : Loss : 3.77508688\n",
            "Step 7810 : Loss : 3.77074409\n",
            "Step 7820 : Loss : 3.76643205\n",
            "Step 7830 : Loss : 3.76211953\n",
            "Step 7840 : Loss : 3.75780129\n",
            "Step 7850 : Loss : 3.75358558\n",
            "Step 7860 : Loss : 3.74941516\n",
            "Step 7870 : Loss : 3.74524069\n",
            "Step 7880 : Loss : 3.74100089\n",
            "Step 7890 : Loss : 3.73673129\n",
            "Step 7900 : Loss : 3.73237896\n",
            "Step 7910 : Loss : 3.72812319\n",
            "Step 7920 : Loss : 3.72374511\n",
            "Step 7930 : Loss : 3.7194972\n",
            "Step 7940 : Loss : 3.71515\n",
            "Step 7950 : Loss : 3.71079564\n",
            "Step 7960 : Loss : 3.7065022\n",
            "Step 7970 : Loss : 3.7023077\n",
            "Step 7980 : Loss : 3.698102\n",
            "Step 7990 : Loss : 3.69386435\n",
            "Step 8000 : Loss : 3.68965697\n",
            "Step 8010 : Loss : 3.68547201\n",
            "Step 8020 : Loss : 3.68123388\n",
            "Step 8030 : Loss : 3.67714429\n",
            "Step 8040 : Loss : 3.67289877\n",
            "Step 8050 : Loss : 3.66864038\n",
            "Step 8060 : Loss : 3.66460276\n",
            "Step 8070 : Loss : 3.66045833\n",
            "Step 8080 : Loss : 3.65634346\n",
            "Step 8090 : Loss : 3.65216088\n",
            "Step 8100 : Loss : 3.64807701\n",
            "Step 8110 : Loss : 3.64396405\n",
            "Step 8120 : Loss : 3.63978601\n",
            "Step 8130 : Loss : 3.63564062\n",
            "Step 8140 : Loss : 3.63152647\n",
            "Step 8150 : Loss : 3.62754631\n",
            "Step 8160 : Loss : 3.62344074\n",
            "Step 8170 : Loss : 3.61941123\n",
            "Step 8180 : Loss : 3.61538124\n",
            "Step 8190 : Loss : 3.61149812\n",
            "Step 8200 : Loss : 3.6075182\n",
            "Step 8210 : Loss : 3.60347462\n",
            "Step 8220 : Loss : 3.59945416\n",
            "Step 8230 : Loss : 3.59561872\n",
            "Step 8240 : Loss : 3.59180665\n",
            "Step 8250 : Loss : 3.58799839\n",
            "Step 8260 : Loss : 3.58409071\n",
            "Step 8270 : Loss : 3.58022428\n",
            "Step 8280 : Loss : 3.57634926\n",
            "Step 8290 : Loss : 3.57243657\n",
            "Step 8300 : Loss : 3.56871581\n",
            "Step 8302 : Loss :  3.56797886\n",
            "Epoch 15\n",
            "Step 8310 : Loss : 3.56490612\n",
            "Step 8320 : Loss : 3.56125426\n",
            "Step 8330 : Loss : 3.55749965\n",
            "Step 8340 : Loss : 3.5538547\n",
            "Step 8350 : Loss : 3.55003881\n",
            "Step 8360 : Loss : 3.54636073\n",
            "Step 8370 : Loss : 3.54254889\n",
            "Step 8380 : Loss : 3.53878379\n",
            "Step 8390 : Loss : 3.53499031\n",
            "Step 8400 : Loss : 3.53116655\n",
            "Step 8410 : Loss : 3.52742362\n",
            "Step 8420 : Loss : 3.5235796\n",
            "Step 8430 : Loss : 3.51985788\n",
            "Step 8440 : Loss : 3.51616144\n",
            "Step 8450 : Loss : 3.51251435\n",
            "Step 8460 : Loss : 3.50881362\n",
            "Step 8470 : Loss : 3.50502539\n",
            "Step 8480 : Loss : 3.50138664\n",
            "Step 8490 : Loss : 3.49771786\n",
            "Step 8500 : Loss : 3.49404955\n",
            "Step 8510 : Loss : 3.49032331\n",
            "Step 8520 : Loss : 3.48662734\n",
            "Step 8530 : Loss : 3.48287368\n",
            "Step 8540 : Loss : 3.47911787\n",
            "Step 8550 : Loss : 3.47537851\n",
            "Step 8560 : Loss : 3.47170281\n",
            "Step 8570 : Loss : 3.46798\n",
            "Step 8580 : Loss : 3.46423054\n",
            "Step 8590 : Loss : 3.46054387\n",
            "Step 8600 : Loss : 3.45697474\n",
            "Step 8610 : Loss : 3.45337272\n",
            "Step 8620 : Loss : 3.44984579\n",
            "Step 8630 : Loss : 3.44630027\n",
            "Step 8640 : Loss : 3.44273162\n",
            "Step 8650 : Loss : 3.43913245\n",
            "Step 8660 : Loss : 3.43553472\n",
            "Step 8670 : Loss : 3.43195248\n",
            "Step 8680 : Loss : 3.42836356\n",
            "Step 8690 : Loss : 3.4247725\n",
            "Step 8700 : Loss : 3.42122126\n",
            "Step 8710 : Loss : 3.41775084\n",
            "Step 8720 : Loss : 3.41436219\n",
            "Step 8730 : Loss : 3.41097212\n",
            "Step 8740 : Loss : 3.40756536\n",
            "Step 8750 : Loss : 3.40412045\n",
            "Step 8760 : Loss : 3.40072751\n",
            "Step 8770 : Loss : 3.39720106\n",
            "Step 8780 : Loss : 3.39369822\n",
            "Step 8790 : Loss : 3.39028811\n",
            "Step 8800 : Loss : 3.38673663\n",
            "Step 8810 : Loss : 3.38329029\n",
            "Step 8820 : Loss : 3.3797648\n",
            "Step 8830 : Loss : 3.37631249\n",
            "Step 8840 : Loss : 3.37281799\n",
            "Step 8850 : Loss : 3.36934781\n",
            "Step 8860 : Loss : 3.36591029\n",
            "Step 8870 : Loss : 3.36249185\n",
            "Step 8880 : Loss : 3.35903525\n",
            "Step 8890 : Loss : 3.35559034\n",
            "Step 8895 : Loss :  3.35391569\n",
            "Epoch 16\n",
            "Step 8900 : Loss : 3.35223746\n",
            "Step 8910 : Loss : 3.34882116\n",
            "Step 8920 : Loss : 3.34552145\n",
            "Step 8930 : Loss : 3.34213495\n",
            "Step 8940 : Loss : 3.3387692\n",
            "Step 8950 : Loss : 3.33533049\n",
            "Step 8960 : Loss : 3.33196783\n",
            "Step 8970 : Loss : 3.32863522\n",
            "Step 8980 : Loss : 3.32527399\n",
            "Step 8990 : Loss : 3.32181311\n",
            "Step 9000 : Loss : 3.31843758\n",
            "Step 9010 : Loss : 3.31513119\n",
            "Step 9020 : Loss : 3.3118732\n",
            "Step 9030 : Loss : 3.30853868\n",
            "Step 9040 : Loss : 3.30518651\n",
            "Step 9050 : Loss : 3.30191922\n",
            "Step 9060 : Loss : 3.29866457\n",
            "Step 9070 : Loss : 3.29542708\n",
            "Step 9080 : Loss : 3.29230952\n",
            "Step 9090 : Loss : 3.28914547\n",
            "Step 9100 : Loss : 3.28600764\n",
            "Step 9110 : Loss : 3.28280616\n",
            "Step 9120 : Loss : 3.27969503\n",
            "Step 9130 : Loss : 3.27648735\n",
            "Step 9140 : Loss : 3.2733283\n",
            "Step 9150 : Loss : 3.27011228\n",
            "Step 9160 : Loss : 3.2669332\n",
            "Step 9170 : Loss : 3.26390481\n",
            "Step 9180 : Loss : 3.26083398\n",
            "Step 9190 : Loss : 3.25778031\n",
            "Step 9200 : Loss : 3.25453281\n",
            "Step 9210 : Loss : 3.2515707\n",
            "Step 9220 : Loss : 3.24848771\n",
            "Step 9230 : Loss : 3.24533749\n",
            "Step 9240 : Loss : 3.24229598\n",
            "Step 9250 : Loss : 3.23912048\n",
            "Step 9260 : Loss : 3.23598528\n",
            "Step 9270 : Loss : 3.23285913\n",
            "Step 9280 : Loss : 3.22991109\n",
            "Step 9290 : Loss : 3.22689843\n",
            "Step 9300 : Loss : 3.22385693\n",
            "Step 9310 : Loss : 3.22079635\n",
            "Step 9320 : Loss : 3.21781921\n",
            "Step 9330 : Loss : 3.21467185\n",
            "Step 9340 : Loss : 3.21163511\n",
            "Step 9350 : Loss : 3.20857048\n",
            "Step 9360 : Loss : 3.20546508\n",
            "Step 9370 : Loss : 3.20246506\n",
            "Step 9380 : Loss : 3.19940925\n",
            "Step 9390 : Loss : 3.19645023\n",
            "Step 9400 : Loss : 3.19345093\n",
            "Step 9410 : Loss : 3.19031596\n",
            "Step 9420 : Loss : 3.1873126\n",
            "Step 9430 : Loss : 3.18426037\n",
            "Step 9440 : Loss : 3.1811769\n",
            "Step 9450 : Loss : 3.17815042\n",
            "Step 9460 : Loss : 3.17514896\n",
            "Step 9470 : Loss : 3.17204714\n",
            "Step 9480 : Loss : 3.16897535\n",
            "Step 9488 : Loss :  3.16658616\n",
            "Epoch 17\n",
            "Step 9490 : Loss : 3.16596055\n",
            "Step 9500 : Loss : 3.16294622\n",
            "Step 9510 : Loss : 3.16000628\n",
            "Step 9520 : Loss : 3.15719128\n",
            "Step 9530 : Loss : 3.15423346\n",
            "Step 9540 : Loss : 3.1512692\n",
            "Step 9550 : Loss : 3.1483\n",
            "Step 9560 : Loss : 3.14534\n",
            "Step 9570 : Loss : 3.14230442\n",
            "Step 9580 : Loss : 3.13928962\n",
            "Step 9590 : Loss : 3.13627958\n",
            "Step 9600 : Loss : 3.13320661\n",
            "Step 9610 : Loss : 3.13024974\n",
            "Step 9620 : Loss : 3.12730575\n",
            "Step 9630 : Loss : 3.12441587\n",
            "Step 9640 : Loss : 3.12142491\n",
            "Step 9650 : Loss : 3.11853909\n",
            "Step 9660 : Loss : 3.11564589\n",
            "Step 9670 : Loss : 3.11271214\n",
            "Step 9680 : Loss : 3.10986567\n",
            "Step 9690 : Loss : 3.1069026\n",
            "Step 9700 : Loss : 3.10391831\n",
            "Step 9710 : Loss : 3.10097194\n",
            "Step 9720 : Loss : 3.09806705\n",
            "Step 9730 : Loss : 3.09520841\n",
            "Step 9740 : Loss : 3.09227347\n",
            "Step 9750 : Loss : 3.08939505\n",
            "Step 9760 : Loss : 3.08652639\n",
            "Step 9770 : Loss : 3.08364248\n",
            "Step 9780 : Loss : 3.0808413\n",
            "Step 9790 : Loss : 3.07797909\n",
            "Step 9800 : Loss : 3.0751369\n",
            "Step 9810 : Loss : 3.07231212\n",
            "Step 9820 : Loss : 3.06948471\n",
            "Step 9830 : Loss : 3.0667\n",
            "Step 9840 : Loss : 3.06386256\n",
            "Step 9850 : Loss : 3.06104112\n",
            "Step 9860 : Loss : 3.05823278\n",
            "Step 9870 : Loss : 3.05547118\n",
            "Step 9880 : Loss : 3.05260038\n",
            "Step 9890 : Loss : 3.04993057\n",
            "Step 9900 : Loss : 3.04715896\n",
            "Step 9910 : Loss : 3.04439974\n",
            "Step 9920 : Loss : 3.04170775\n",
            "Step 9930 : Loss : 3.03897762\n",
            "Step 9940 : Loss : 3.03624749\n",
            "Step 9950 : Loss : 3.03344941\n",
            "Step 9960 : Loss : 3.03066945\n",
            "Step 9970 : Loss : 3.02788305\n",
            "Step 9980 : Loss : 3.02518749\n",
            "Step 9990 : Loss : 3.02245688\n",
            "Step 10000 : Loss : 3.01978374\n",
            "Step 10010 : Loss : 3.01713276\n",
            "Step 10020 : Loss : 3.0144043\n",
            "Step 10030 : Loss : 3.01168561\n",
            "Step 10040 : Loss : 3.00894165\n",
            "Step 10050 : Loss : 3.00617599\n",
            "Step 10060 : Loss : 3.00340915\n",
            "Step 10070 : Loss : 3.00060081\n",
            "Step 10080 : Loss : 2.99791598\n",
            "Step 10081 : Loss :  2.9976244\n",
            "Epoch 18\n",
            "Step 10090 : Loss : 2.99517322\n",
            "Step 10100 : Loss : 2.99238825\n",
            "Step 10110 : Loss : 2.98970842\n",
            "Step 10120 : Loss : 2.98712659\n",
            "Step 10130 : Loss : 2.98440814\n",
            "Step 10140 : Loss : 2.9817605\n",
            "Step 10150 : Loss : 2.97902584\n",
            "Step 10160 : Loss : 2.97637534\n",
            "Step 10170 : Loss : 2.97366524\n",
            "Step 10180 : Loss : 2.97096729\n",
            "Step 10190 : Loss : 2.96830058\n",
            "Step 10200 : Loss : 2.96559501\n",
            "Step 10210 : Loss : 2.96290183\n",
            "Step 10220 : Loss : 2.96019602\n",
            "Step 10230 : Loss : 2.95752048\n",
            "Step 10240 : Loss : 2.95481896\n",
            "Step 10250 : Loss : 2.95215559\n",
            "Step 10260 : Loss : 2.94947076\n",
            "Step 10270 : Loss : 2.94686222\n",
            "Step 10280 : Loss : 2.94419074\n",
            "Step 10290 : Loss : 2.94155216\n",
            "Step 10300 : Loss : 2.9388833\n",
            "Step 10310 : Loss : 2.93624\n",
            "Step 10320 : Loss : 2.93349648\n",
            "Step 10330 : Loss : 2.93083358\n",
            "Step 10340 : Loss : 2.92818785\n",
            "Step 10350 : Loss : 2.92557645\n",
            "Step 10360 : Loss : 2.92294216\n",
            "Step 10370 : Loss : 2.9203198\n",
            "Step 10380 : Loss : 2.91776514\n",
            "Step 10390 : Loss : 2.91520381\n",
            "Step 10400 : Loss : 2.91276622\n",
            "Step 10410 : Loss : 2.91019154\n",
            "Step 10420 : Loss : 2.90765309\n",
            "Step 10430 : Loss : 2.90513754\n",
            "Step 10440 : Loss : 2.90269089\n",
            "Step 10450 : Loss : 2.90016937\n",
            "Step 10460 : Loss : 2.89760256\n",
            "Step 10470 : Loss : 2.89511466\n",
            "Step 10480 : Loss : 2.89261866\n",
            "Step 10490 : Loss : 2.89012194\n",
            "Step 10500 : Loss : 2.88757229\n",
            "Step 10510 : Loss : 2.88511825\n",
            "Step 10520 : Loss : 2.88274\n",
            "Step 10530 : Loss : 2.88022232\n",
            "Step 10540 : Loss : 2.8777132\n",
            "Step 10550 : Loss : 2.87522745\n",
            "Step 10560 : Loss : 2.8728056\n",
            "Step 10570 : Loss : 2.87036657\n",
            "Step 10580 : Loss : 2.86795449\n",
            "Step 10590 : Loss : 2.86561561\n",
            "Step 10600 : Loss : 2.86315084\n",
            "Step 10610 : Loss : 2.86083364\n",
            "Step 10620 : Loss : 2.85838056\n",
            "Step 10630 : Loss : 2.85601473\n",
            "Step 10640 : Loss : 2.8536613\n",
            "Step 10650 : Loss : 2.85126019\n",
            "Step 10660 : Loss : 2.84882903\n",
            "Step 10670 : Loss : 2.84636521\n",
            "Step 10674 : Loss :  2.84541154\n",
            "Epoch 19\n",
            "Step 10680 : Loss : 2.84399462\n",
            "Step 10690 : Loss : 2.84159756\n",
            "Step 10700 : Loss : 2.83935452\n",
            "Step 10710 : Loss : 2.83710718\n",
            "Step 10720 : Loss : 2.8346858\n",
            "Step 10730 : Loss : 2.83240151\n",
            "Step 10740 : Loss : 2.83014679\n",
            "Step 10750 : Loss : 2.82783318\n",
            "Step 10760 : Loss : 2.82549548\n",
            "Step 10770 : Loss : 2.82315183\n",
            "Step 10780 : Loss : 2.82081318\n",
            "Step 10790 : Loss : 2.81849098\n",
            "Step 10800 : Loss : 2.81609845\n",
            "Step 10810 : Loss : 2.81377745\n",
            "Step 10820 : Loss : 2.81139755\n",
            "Step 10830 : Loss : 2.80907464\n",
            "Step 10840 : Loss : 2.80672932\n",
            "Step 10850 : Loss : 2.80429697\n",
            "Step 10860 : Loss : 2.80192256\n",
            "Step 10870 : Loss : 2.79954219\n",
            "Step 10880 : Loss : 2.79719567\n",
            "Step 10890 : Loss : 2.79481316\n",
            "Step 10900 : Loss : 2.79248524\n",
            "Step 10910 : Loss : 2.79012012\n",
            "Step 10920 : Loss : 2.78773427\n",
            "Step 10930 : Loss : 2.78537416\n",
            "Step 10940 : Loss : 2.78301692\n",
            "Step 10950 : Loss : 2.78062248\n",
            "Step 10960 : Loss : 2.77823853\n",
            "Step 10970 : Loss : 2.77591729\n",
            "Step 10980 : Loss : 2.77352405\n",
            "Step 10990 : Loss : 2.77121425\n",
            "Step 11000 : Loss : 2.76889777\n",
            "Step 11010 : Loss : 2.7665844\n",
            "Step 11020 : Loss : 2.76428223\n",
            "Step 11030 : Loss : 2.76203775\n",
            "Step 11040 : Loss : 2.75979328\n",
            "Step 11050 : Loss : 2.75760031\n",
            "Step 11060 : Loss : 2.75537086\n",
            "Step 11070 : Loss : 2.75305223\n",
            "Step 11080 : Loss : 2.75079489\n",
            "Step 11090 : Loss : 2.74854755\n",
            "Step 11100 : Loss : 2.74630642\n",
            "Step 11110 : Loss : 2.74411345\n",
            "Step 11120 : Loss : 2.74185085\n",
            "Step 11130 : Loss : 2.7395606\n",
            "Step 11140 : Loss : 2.73734093\n",
            "Step 11150 : Loss : 2.73504329\n",
            "Step 11160 : Loss : 2.73291564\n",
            "Step 11170 : Loss : 2.73068261\n",
            "Step 11180 : Loss : 2.72845721\n",
            "Step 11190 : Loss : 2.72627473\n",
            "Step 11200 : Loss : 2.72412801\n",
            "Step 11210 : Loss : 2.72203\n",
            "Step 11220 : Loss : 2.71984434\n",
            "Step 11230 : Loss : 2.71768117\n",
            "Step 11240 : Loss : 2.71549058\n",
            "Step 11250 : Loss : 2.7133472\n",
            "Step 11260 : Loss : 2.71129656\n",
            "Step 11267 : Loss :  2.70976305\n",
            "Epoch 20\n",
            "Step 11270 : Loss : 2.70917463\n",
            "Step 11280 : Loss : 2.70693278\n",
            "Step 11290 : Loss : 2.70480824\n",
            "Step 11300 : Loss : 2.70273376\n",
            "Step 11310 : Loss : 2.70063925\n",
            "Step 11320 : Loss : 2.69849229\n",
            "Step 11330 : Loss : 2.69636345\n",
            "Step 11340 : Loss : 2.69423103\n",
            "Step 11350 : Loss : 2.69212556\n",
            "Step 11360 : Loss : 2.68999362\n",
            "Step 11370 : Loss : 2.6879344\n",
            "Step 11380 : Loss : 2.68579841\n",
            "Step 11390 : Loss : 2.6837194\n",
            "Step 11400 : Loss : 2.6816442\n",
            "Step 11410 : Loss : 2.67947936\n",
            "Step 11420 : Loss : 2.67733598\n",
            "Step 11430 : Loss : 2.67515\n",
            "Step 11440 : Loss : 2.67294908\n",
            "Step 11450 : Loss : 2.67080593\n",
            "Step 11460 : Loss : 2.66871953\n",
            "Step 11470 : Loss : 2.66665769\n",
            "Step 11480 : Loss : 2.66452742\n",
            "Step 11490 : Loss : 2.66252494\n",
            "Step 11500 : Loss : 2.66040587\n",
            "Step 11510 : Loss : 2.65826702\n",
            "Step 11520 : Loss : 2.65615988\n",
            "Step 11530 : Loss : 2.65411\n",
            "Step 11540 : Loss : 2.65205\n",
            "Step 11550 : Loss : 2.64999127\n",
            "Step 11560 : Loss : 2.64795208\n",
            "Step 11570 : Loss : 2.64586091\n",
            "Step 11580 : Loss : 2.64381623\n",
            "Step 11590 : Loss : 2.64190912\n",
            "Step 11600 : Loss : 2.6398716\n",
            "Step 11610 : Loss : 2.63780355\n",
            "Step 11620 : Loss : 2.63566327\n",
            "Step 11630 : Loss : 2.63365817\n",
            "Step 11640 : Loss : 2.63160849\n",
            "Step 11650 : Loss : 2.62957907\n",
            "Step 11660 : Loss : 2.62757277\n",
            "Step 11670 : Loss : 2.6255\n",
            "Step 11680 : Loss : 2.62353587\n",
            "Step 11690 : Loss : 2.62150216\n",
            "Step 11700 : Loss : 2.61948657\n",
            "Step 11710 : Loss : 2.61746597\n",
            "Step 11720 : Loss : 2.61544824\n",
            "Step 11730 : Loss : 2.61338043\n",
            "Step 11740 : Loss : 2.6113317\n",
            "Step 11750 : Loss : 2.60929298\n",
            "Step 11760 : Loss : 2.60728264\n",
            "Step 11770 : Loss : 2.60529923\n",
            "Step 11780 : Loss : 2.60331631\n",
            "Step 11790 : Loss : 2.60132551\n",
            "Step 11800 : Loss : 2.59931493\n",
            "Step 11810 : Loss : 2.59728932\n",
            "Step 11820 : Loss : 2.59528399\n",
            "Step 11830 : Loss : 2.59334469\n",
            "Step 11840 : Loss : 2.59134364\n",
            "Step 11850 : Loss : 2.58931136\n",
            "Step 11860 : Loss : 2.58732295\n",
            "Step 11860 : Loss :  2.58732295\n",
            "Epoch 21\n",
            "Step 11870 : Loss : 2.58538127\n",
            "Step 11880 : Loss : 2.58334827\n",
            "Step 11890 : Loss : 2.58139062\n",
            "Step 11900 : Loss : 2.57941055\n",
            "Step 11910 : Loss : 2.57749176\n",
            "Step 11920 : Loss : 2.57554126\n",
            "Step 11930 : Loss : 2.57359076\n",
            "Step 11940 : Loss : 2.5715971\n",
            "Step 11950 : Loss : 2.569628\n",
            "Step 11960 : Loss : 2.56762671\n",
            "Step 11970 : Loss : 2.56570029\n",
            "Step 11980 : Loss : 2.5637219\n",
            "Step 11990 : Loss : 2.56174421\n",
            "Step 12000 : Loss : 2.5597856\n",
            "Step 12010 : Loss : 2.55783129\n",
            "Step 12020 : Loss : 2.55585814\n",
            "Step 12030 : Loss : 2.55388832\n",
            "Step 12040 : Loss : 2.55192518\n",
            "Step 12050 : Loss : 2.5499866\n",
            "Step 12060 : Loss : 2.54804707\n",
            "Step 12070 : Loss : 2.54610372\n",
            "Step 12080 : Loss : 2.54415822\n",
            "Step 12090 : Loss : 2.54224539\n",
            "Step 12100 : Loss : 2.54027963\n",
            "Step 12110 : Loss : 2.53840733\n",
            "Step 12120 : Loss : 2.53659511\n",
            "Step 12130 : Loss : 2.53471947\n",
            "Step 12140 : Loss : 2.53287601\n",
            "Step 12150 : Loss : 2.53094769\n",
            "Step 12160 : Loss : 2.52907324\n",
            "Step 12170 : Loss : 2.52719665\n",
            "Step 12180 : Loss : 2.52541447\n",
            "Step 12190 : Loss : 2.52368402\n",
            "Step 12200 : Loss : 2.52191615\n",
            "Step 12210 : Loss : 2.52006817\n",
            "Step 12220 : Loss : 2.51828098\n",
            "Step 12230 : Loss : 2.51652098\n",
            "Step 12240 : Loss : 2.51471758\n",
            "Step 12250 : Loss : 2.51297951\n",
            "Step 12260 : Loss : 2.51115584\n",
            "Step 12270 : Loss : 2.50938225\n",
            "Step 12280 : Loss : 2.50755811\n",
            "Step 12290 : Loss : 2.50571251\n",
            "Step 12300 : Loss : 2.50393701\n",
            "Step 12310 : Loss : 2.50214744\n",
            "Step 12320 : Loss : 2.50033855\n",
            "Step 12330 : Loss : 2.49848318\n",
            "Step 12340 : Loss : 2.49666667\n",
            "Step 12350 : Loss : 2.49483418\n",
            "Step 12360 : Loss : 2.49304914\n",
            "Step 12370 : Loss : 2.49126339\n",
            "Step 12380 : Loss : 2.48948503\n",
            "Step 12390 : Loss : 2.48769498\n",
            "Step 12400 : Loss : 2.48595667\n",
            "Step 12410 : Loss : 2.48425889\n",
            "Step 12420 : Loss : 2.48252916\n",
            "Step 12430 : Loss : 2.48081541\n",
            "Step 12440 : Loss : 2.47901511\n",
            "Step 12450 : Loss : 2.47729\n",
            "Step 12453 : Loss :  2.47679353\n",
            "Epoch 22\n",
            "Step 12460 : Loss : 2.47555089\n",
            "Step 12470 : Loss : 2.47381878\n",
            "Step 12480 : Loss : 2.47209811\n",
            "Step 12490 : Loss : 2.47044039\n",
            "Step 12500 : Loss : 2.46870661\n",
            "Step 12510 : Loss : 2.46698213\n",
            "Step 12520 : Loss : 2.46517396\n",
            "Step 12530 : Loss : 2.46341538\n",
            "Step 12540 : Loss : 2.46160889\n",
            "Step 12550 : Loss : 2.4598949\n",
            "Step 12560 : Loss : 2.45821953\n",
            "Step 12570 : Loss : 2.45649219\n",
            "Step 12580 : Loss : 2.45472097\n",
            "Step 12590 : Loss : 2.45304656\n",
            "Step 12600 : Loss : 2.45139194\n",
            "Step 12610 : Loss : 2.44967914\n",
            "Step 12620 : Loss : 2.4479959\n",
            "Step 12630 : Loss : 2.44627309\n",
            "Step 12640 : Loss : 2.44463253\n",
            "Step 12650 : Loss : 2.44294477\n",
            "Step 12660 : Loss : 2.44127488\n",
            "Step 12670 : Loss : 2.43960595\n",
            "Step 12680 : Loss : 2.43793797\n",
            "Step 12690 : Loss : 2.43627214\n",
            "Step 12700 : Loss : 2.43459487\n",
            "Step 12710 : Loss : 2.43286\n",
            "Step 12720 : Loss : 2.43119144\n",
            "Step 12730 : Loss : 2.42949867\n",
            "Step 12740 : Loss : 2.42785335\n",
            "Step 12750 : Loss : 2.42616153\n",
            "Step 12760 : Loss : 2.42442727\n",
            "Step 12770 : Loss : 2.42275071\n",
            "Step 12780 : Loss : 2.42103672\n",
            "Step 12790 : Loss : 2.4193368\n",
            "Step 12800 : Loss : 2.41759896\n",
            "Step 12810 : Loss : 2.41595745\n",
            "Step 12820 : Loss : 2.41431379\n",
            "Step 12830 : Loss : 2.41270375\n",
            "Step 12840 : Loss : 2.41107631\n",
            "Step 12850 : Loss : 2.40941668\n",
            "Step 12860 : Loss : 2.40774345\n",
            "Step 12870 : Loss : 2.40605235\n",
            "Step 12880 : Loss : 2.40442157\n",
            "Step 12890 : Loss : 2.40281844\n",
            "Step 12900 : Loss : 2.40122914\n",
            "Step 12910 : Loss : 2.39957237\n",
            "Step 12920 : Loss : 2.397928\n",
            "Step 12930 : Loss : 2.39626074\n",
            "Step 12940 : Loss : 2.39459372\n",
            "Step 12950 : Loss : 2.39299822\n",
            "Step 12960 : Loss : 2.39136672\n",
            "Step 12970 : Loss : 2.3896873\n",
            "Step 12980 : Loss : 2.38808322\n",
            "Step 12990 : Loss : 2.38644505\n",
            "Step 13000 : Loss : 2.38477445\n",
            "Step 13010 : Loss : 2.3831315\n",
            "Step 13020 : Loss : 2.38148594\n",
            "Step 13030 : Loss : 2.37979722\n",
            "Step 13040 : Loss : 2.37818384\n",
            "Step 13046 : Loss :  2.37718987\n",
            "Epoch 23\n",
            "Step 13050 : Loss : 2.37656665\n",
            "Step 13060 : Loss : 2.37494564\n",
            "Step 13070 : Loss : 2.37329769\n",
            "Step 13080 : Loss : 2.37163687\n",
            "Step 13090 : Loss : 2.37002659\n",
            "Step 13100 : Loss : 2.36842847\n",
            "Step 13110 : Loss : 2.36686921\n",
            "Step 13120 : Loss : 2.36521149\n",
            "Step 13130 : Loss : 2.36354756\n",
            "Step 13140 : Loss : 2.36192346\n",
            "Step 13150 : Loss : 2.36032772\n",
            "Step 13160 : Loss : 2.35872221\n",
            "Step 13170 : Loss : 2.35715413\n",
            "Step 13180 : Loss : 2.35553789\n",
            "Step 13190 : Loss : 2.35393882\n",
            "Step 13200 : Loss : 2.35231304\n",
            "Step 13210 : Loss : 2.35071921\n",
            "Step 13220 : Loss : 2.3490839\n",
            "Step 13230 : Loss : 2.34749317\n",
            "Step 13240 : Loss : 2.34589696\n",
            "Step 13250 : Loss : 2.34431124\n",
            "Step 13260 : Loss : 2.34267926\n",
            "Step 13270 : Loss : 2.34107137\n",
            "Step 13280 : Loss : 2.33944917\n",
            "Step 13290 : Loss : 2.33791542\n",
            "Step 13300 : Loss : 2.33631659\n",
            "Step 13310 : Loss : 2.33474\n",
            "Step 13320 : Loss : 2.33319855\n",
            "Step 13330 : Loss : 2.3316505\n",
            "Step 13340 : Loss : 2.33014226\n",
            "Step 13350 : Loss : 2.3285532\n",
            "Step 13360 : Loss : 2.32695675\n",
            "Step 13370 : Loss : 2.32537031\n",
            "Step 13380 : Loss : 2.32376218\n",
            "Step 13390 : Loss : 2.32213879\n",
            "Step 13400 : Loss : 2.32058215\n",
            "Step 13410 : Loss : 2.31900764\n",
            "Step 13420 : Loss : 2.31743884\n",
            "Step 13430 : Loss : 2.31583214\n",
            "Step 13440 : Loss : 2.31432819\n",
            "Step 13450 : Loss : 2.3127811\n",
            "Step 13460 : Loss : 2.31116772\n",
            "Step 13470 : Loss : 2.30957961\n",
            "Step 13480 : Loss : 2.30805206\n",
            "Step 13490 : Loss : 2.30651283\n",
            "Step 13500 : Loss : 2.30493879\n",
            "Step 13510 : Loss : 2.30341554\n",
            "Step 13520 : Loss : 2.30185699\n",
            "Step 13530 : Loss : 2.30026865\n",
            "Step 13540 : Loss : 2.29866433\n",
            "Step 13550 : Loss : 2.29711246\n",
            "Step 13560 : Loss : 2.29556108\n",
            "Step 13570 : Loss : 2.29398823\n",
            "Step 13580 : Loss : 2.29240799\n",
            "Step 13590 : Loss : 2.29087567\n",
            "Step 13600 : Loss : 2.28930306\n",
            "Step 13610 : Loss : 2.28776884\n",
            "Step 13620 : Loss : 2.28624415\n",
            "Step 13630 : Loss : 2.28470182\n",
            "Step 13639 : Loss :  2.28333044\n",
            "Epoch 24\n",
            "Step 13640 : Loss : 2.2831738\n",
            "Step 13650 : Loss : 2.28162909\n",
            "Step 13660 : Loss : 2.28012133\n",
            "Step 13670 : Loss : 2.27856\n",
            "Step 13680 : Loss : 2.27701807\n",
            "Step 13690 : Loss : 2.27549195\n",
            "Step 13700 : Loss : 2.2740047\n",
            "Step 13710 : Loss : 2.27245116\n",
            "Step 13720 : Loss : 2.27090764\n",
            "Step 13730 : Loss : 2.26937604\n",
            "Step 13740 : Loss : 2.2678442\n",
            "Step 13750 : Loss : 2.26629138\n",
            "Step 13760 : Loss : 2.26475954\n",
            "Step 13770 : Loss : 2.2632525\n",
            "Step 13780 : Loss : 2.26174331\n",
            "Step 13790 : Loss : 2.2602253\n",
            "Step 13800 : Loss : 2.25869036\n",
            "Step 13810 : Loss : 2.2571733\n",
            "Step 13820 : Loss : 2.25567746\n",
            "Step 13830 : Loss : 2.25421691\n",
            "Step 13840 : Loss : 2.25270987\n",
            "Step 13850 : Loss : 2.25125194\n",
            "Step 13860 : Loss : 2.24980092\n",
            "Step 13870 : Loss : 2.248317\n",
            "Step 13880 : Loss : 2.24683213\n",
            "Step 13890 : Loss : 2.24530339\n",
            "Step 13900 : Loss : 2.24379587\n",
            "Step 13910 : Loss : 2.24231672\n",
            "Step 13920 : Loss : 2.24083352\n",
            "Step 13930 : Loss : 2.23936343\n",
            "Step 13940 : Loss : 2.23790455\n",
            "Step 13950 : Loss : 2.23639655\n",
            "Step 13960 : Loss : 2.23494\n",
            "Step 13970 : Loss : 2.23346949\n",
            "Step 13980 : Loss : 2.23201942\n",
            "Step 13990 : Loss : 2.23060322\n",
            "Step 14000 : Loss : 2.22919035\n",
            "Step 14010 : Loss : 2.22778773\n",
            "Step 14020 : Loss : 2.22634506\n",
            "Step 14030 : Loss : 2.22501159\n",
            "Step 14040 : Loss : 2.2236681\n",
            "Step 14050 : Loss : 2.22226667\n",
            "Step 14060 : Loss : 2.22081304\n",
            "Step 14070 : Loss : 2.21935797\n",
            "Step 14080 : Loss : 2.21800041\n",
            "Step 14090 : Loss : 2.21657681\n",
            "Step 14100 : Loss : 2.21510816\n",
            "Step 14110 : Loss : 2.21361685\n",
            "Step 14120 : Loss : 2.21219873\n",
            "Step 14130 : Loss : 2.21082377\n",
            "Step 14140 : Loss : 2.20941806\n",
            "Step 14150 : Loss : 2.20799279\n",
            "Step 14160 : Loss : 2.20659\n",
            "Step 14170 : Loss : 2.20521212\n",
            "Step 14180 : Loss : 2.20380497\n",
            "Step 14190 : Loss : 2.20242405\n",
            "Step 14200 : Loss : 2.20102406\n",
            "Step 14210 : Loss : 2.19960213\n",
            "Step 14220 : Loss : 2.19821525\n",
            "Step 14230 : Loss : 2.19677949\n",
            "Step 14232 : Loss :  2.19648933\n",
            "Epoch 25\n",
            "Step 14240 : Loss : 2.19538188\n",
            "Step 14250 : Loss : 2.1939888\n",
            "Step 14260 : Loss : 2.19262052\n",
            "Step 14270 : Loss : 2.19123054\n",
            "Step 14280 : Loss : 2.18985271\n",
            "Step 14290 : Loss : 2.18847561\n",
            "Step 14300 : Loss : 2.18708563\n",
            "Step 14310 : Loss : 2.18565679\n",
            "Step 14320 : Loss : 2.18428683\n",
            "Step 14330 : Loss : 2.18285251\n",
            "Step 14340 : Loss : 2.18145299\n",
            "Step 14350 : Loss : 2.18006754\n",
            "Step 14360 : Loss : 2.17869163\n",
            "Step 14370 : Loss : 2.17735052\n",
            "Step 14380 : Loss : 2.17599797\n",
            "Step 14390 : Loss : 2.17462397\n",
            "Step 14400 : Loss : 2.17323089\n",
            "Step 14410 : Loss : 2.17186856\n",
            "Step 14420 : Loss : 2.1705265\n",
            "Step 14430 : Loss : 2.16916871\n",
            "Step 14440 : Loss : 2.16780448\n",
            "Step 14450 : Loss : 2.16642022\n",
            "Step 14460 : Loss : 2.16501641\n",
            "Step 14470 : Loss : 2.16363668\n",
            "Step 14480 : Loss : 2.16226697\n",
            "Step 14490 : Loss : 2.16091967\n",
            "Step 14500 : Loss : 2.1595397\n",
            "Step 14510 : Loss : 2.15816116\n",
            "Step 14520 : Loss : 2.15679455\n",
            "Step 14530 : Loss : 2.15542793\n",
            "Step 14540 : Loss : 2.1540482\n",
            "Step 14550 : Loss : 2.15270352\n",
            "Step 14560 : Loss : 2.15132213\n",
            "Step 14570 : Loss : 2.15000153\n",
            "Step 14580 : Loss : 2.14865923\n",
            "Step 14590 : Loss : 2.14739823\n",
            "Step 14600 : Loss : 2.1461091\n",
            "Step 14610 : Loss : 2.14486194\n",
            "Step 14620 : Loss : 2.14358568\n",
            "Step 14630 : Loss : 2.14227509\n",
            "Step 14640 : Loss : 2.14095736\n",
            "Step 14650 : Loss : 2.13966012\n",
            "Step 14660 : Loss : 2.13838577\n",
            "Step 14670 : Loss : 2.13710427\n",
            "Step 14680 : Loss : 2.13583779\n",
            "Step 14690 : Loss : 2.13459444\n",
            "Step 14700 : Loss : 2.13333631\n",
            "Step 14710 : Loss : 2.13198709\n",
            "Step 14720 : Loss : 2.13072848\n",
            "Step 14730 : Loss : 2.12944031\n",
            "Step 14740 : Loss : 2.12809515\n",
            "Step 14750 : Loss : 2.12677455\n",
            "Step 14760 : Loss : 2.12547064\n",
            "Step 14770 : Loss : 2.12414551\n",
            "Step 14780 : Loss : 2.12280607\n",
            "Step 14790 : Loss : 2.12151074\n",
            "Step 14800 : Loss : 2.12017703\n",
            "Step 14810 : Loss : 2.11880541\n",
            "Step 14820 : Loss : 2.11751294\n",
            "Step 14825 : Loss :  2.11685276\n",
            "Epoch 26\n",
            "Step 14830 : Loss : 2.11617684\n",
            "Step 14840 : Loss : 2.11485267\n",
            "Step 14850 : Loss : 2.11356282\n",
            "Step 14860 : Loss : 2.11225867\n",
            "Step 14870 : Loss : 2.11096716\n",
            "Step 14880 : Loss : 2.10962129\n",
            "Step 14890 : Loss : 2.10832405\n",
            "Step 14900 : Loss : 2.10698771\n",
            "Step 14910 : Loss : 2.10566974\n",
            "Step 14920 : Loss : 2.10435629\n",
            "Step 14930 : Loss : 2.10306764\n",
            "Step 14940 : Loss : 2.10176277\n",
            "Step 14950 : Loss : 2.10053396\n",
            "Step 14960 : Loss : 2.09928846\n",
            "Step 14970 : Loss : 2.0980618\n",
            "Step 14980 : Loss : 2.09680128\n",
            "Step 14990 : Loss : 2.09553218\n",
            "Step 15000 : Loss : 2.0943048\n",
            "Step 15010 : Loss : 2.09310412\n",
            "Step 15020 : Loss : 2.09187055\n",
            "Step 15030 : Loss : 2.0906117\n",
            "Step 15040 : Loss : 2.08935165\n",
            "Step 15050 : Loss : 2.0881114\n",
            "Step 15060 : Loss : 2.08686543\n",
            "Step 15070 : Loss : 2.08561397\n",
            "Step 15080 : Loss : 2.08441663\n",
            "Step 15090 : Loss : 2.08318615\n",
            "Step 15100 : Loss : 2.08194852\n",
            "Step 15110 : Loss : 2.08072042\n",
            "Step 15120 : Loss : 2.07945561\n",
            "Step 15130 : Loss : 2.07824755\n",
            "Step 15140 : Loss : 2.07709646\n",
            "Step 15150 : Loss : 2.07587719\n",
            "Step 15160 : Loss : 2.07461309\n",
            "Step 15170 : Loss : 2.07339787\n",
            "Step 15180 : Loss : 2.07212758\n",
            "Step 15190 : Loss : 2.07094479\n",
            "Step 15200 : Loss : 2.06974387\n",
            "Step 15210 : Loss : 2.06853986\n",
            "Step 15220 : Loss : 2.0673933\n",
            "Step 15230 : Loss : 2.06625628\n",
            "Step 15240 : Loss : 2.06510615\n",
            "Step 15250 : Loss : 2.06391621\n",
            "Step 15260 : Loss : 2.06273723\n",
            "Step 15270 : Loss : 2.06153488\n",
            "Step 15280 : Loss : 2.06028914\n",
            "Step 15290 : Loss : 2.05908465\n",
            "Step 15300 : Loss : 2.05788922\n",
            "Step 15310 : Loss : 2.05671763\n",
            "Step 15320 : Loss : 2.05554\n",
            "Step 15330 : Loss : 2.05435038\n",
            "Step 15340 : Loss : 2.05310559\n",
            "Step 15350 : Loss : 2.05190277\n",
            "Step 15360 : Loss : 2.05068135\n",
            "Step 15370 : Loss : 2.04949832\n",
            "Step 15380 : Loss : 2.0483017\n",
            "Step 15390 : Loss : 2.04719663\n",
            "Step 15400 : Loss : 2.04609442\n",
            "Step 15410 : Loss : 2.04493117\n",
            "Step 15418 : Loss :  2.04400611\n",
            "Epoch 27\n",
            "Step 15420 : Loss : 2.04378128\n",
            "Step 15430 : Loss : 2.04258275\n",
            "Step 15440 : Loss : 2.04140687\n",
            "Step 15450 : Loss : 2.04025102\n",
            "Step 15460 : Loss : 2.03906226\n",
            "Step 15470 : Loss : 2.03782582\n",
            "Step 15480 : Loss : 2.0366559\n",
            "Step 15490 : Loss : 2.03544664\n",
            "Step 15500 : Loss : 2.03421831\n",
            "Step 15510 : Loss : 2.03301716\n",
            "Step 15520 : Loss : 2.03179836\n",
            "Step 15530 : Loss : 2.03063321\n",
            "Step 15540 : Loss : 2.02942896\n",
            "Step 15550 : Loss : 2.02825522\n",
            "Step 15560 : Loss : 2.02708077\n",
            "Step 15570 : Loss : 2.02591825\n",
            "Step 15580 : Loss : 2.02471328\n",
            "Step 15590 : Loss : 2.02352715\n",
            "Step 15600 : Loss : 2.02231359\n",
            "Step 15610 : Loss : 2.02111268\n",
            "Step 15620 : Loss : 2.01999545\n",
            "Step 15630 : Loss : 2.01883578\n",
            "Step 15640 : Loss : 2.01767135\n",
            "Step 15650 : Loss : 2.01651764\n",
            "Step 15660 : Loss : 2.01531982\n",
            "Step 15670 : Loss : 2.01415491\n",
            "Step 15680 : Loss : 2.01305747\n",
            "Step 15690 : Loss : 2.0119369\n",
            "Step 15700 : Loss : 2.01083207\n",
            "Step 15710 : Loss : 2.0096662\n",
            "Step 15720 : Loss : 2.00857687\n",
            "Step 15730 : Loss : 2.00753403\n",
            "Step 15740 : Loss : 2.00639892\n",
            "Step 15750 : Loss : 2.00535369\n",
            "Step 15760 : Loss : 2.00420904\n",
            "Step 15770 : Loss : 2.0031116\n",
            "Step 15780 : Loss : 2.00201702\n",
            "Step 15790 : Loss : 2.0008812\n",
            "Step 15800 : Loss : 1.99972236\n",
            "Step 15810 : Loss : 1.99859941\n",
            "Step 15820 : Loss : 1.99744773\n",
            "Step 15830 : Loss : 1.99633706\n",
            "Step 15840 : Loss : 1.99523568\n",
            "Step 15850 : Loss : 1.99413276\n",
            "Step 15860 : Loss : 1.99306488\n",
            "Step 15870 : Loss : 1.99192226\n",
            "Step 15880 : Loss : 1.99083567\n",
            "Step 15890 : Loss : 1.98972094\n",
            "Step 15900 : Loss : 1.98860848\n",
            "Step 15910 : Loss : 1.9874649\n",
            "Step 15920 : Loss : 1.98630857\n",
            "Step 15930 : Loss : 1.98516572\n",
            "Step 15940 : Loss : 1.98403978\n",
            "Step 15950 : Loss : 1.98293388\n",
            "Step 15960 : Loss : 1.98182666\n",
            "Step 15970 : Loss : 1.98071575\n",
            "Step 15980 : Loss : 1.9795841\n",
            "Step 15990 : Loss : 1.97851169\n",
            "Step 16000 : Loss : 1.97738349\n",
            "Step 16010 : Loss : 1.97626841\n",
            "Step 16011 : Loss :  1.9761554\n",
            "Epoch 28\n",
            "Step 16020 : Loss : 1.97516906\n",
            "Step 16030 : Loss : 1.97410798\n",
            "Step 16040 : Loss : 1.97301805\n",
            "Step 16050 : Loss : 1.97196972\n",
            "Step 16060 : Loss : 1.97087526\n",
            "Step 16070 : Loss : 1.96976435\n",
            "Step 16080 : Loss : 1.96867824\n",
            "Step 16090 : Loss : 1.96757162\n",
            "Step 16100 : Loss : 1.9664582\n",
            "Step 16110 : Loss : 1.96545827\n",
            "Step 16120 : Loss : 1.96435559\n",
            "Step 16130 : Loss : 1.96331525\n",
            "Step 16140 : Loss : 1.96219432\n",
            "Step 16150 : Loss : 1.96108544\n",
            "Step 16160 : Loss : 1.95999992\n",
            "Step 16170 : Loss : 1.95894885\n",
            "Step 16180 : Loss : 1.95782292\n",
            "Step 16190 : Loss : 1.95672882\n",
            "Step 16200 : Loss : 1.9556284\n",
            "Step 16210 : Loss : 1.95453012\n",
            "Step 16220 : Loss : 1.95345175\n",
            "Step 16230 : Loss : 1.95238507\n",
            "Step 16240 : Loss : 1.95128\n",
            "Step 16250 : Loss : 1.95015979\n",
            "Step 16260 : Loss : 1.94907486\n",
            "Step 16270 : Loss : 1.94800544\n",
            "Step 16280 : Loss : 1.94691896\n",
            "Step 16290 : Loss : 1.94580197\n",
            "Step 16300 : Loss : 1.94469535\n",
            "Step 16310 : Loss : 1.94360483\n",
            "Step 16320 : Loss : 1.94252825\n",
            "Step 16330 : Loss : 1.94152308\n",
            "Step 16340 : Loss : 1.9404397\n",
            "Step 16350 : Loss : 1.93933558\n",
            "Step 16360 : Loss : 1.93824232\n",
            "Step 16370 : Loss : 1.93713129\n",
            "Step 16380 : Loss : 1.93604839\n",
            "Step 16390 : Loss : 1.93496013\n",
            "Step 16400 : Loss : 1.93388271\n",
            "Step 16410 : Loss : 1.93278575\n",
            "Step 16420 : Loss : 1.93171525\n",
            "Step 16430 : Loss : 1.93061507\n",
            "Step 16440 : Loss : 1.92956042\n",
            "Step 16450 : Loss : 1.92849278\n",
            "Step 16460 : Loss : 1.92742229\n",
            "Step 16470 : Loss : 1.92640817\n",
            "Step 16480 : Loss : 1.92535365\n",
            "Step 16490 : Loss : 1.92430329\n",
            "Step 16500 : Loss : 1.92324305\n",
            "Step 16510 : Loss : 1.92219138\n",
            "Step 16520 : Loss : 1.92110801\n",
            "Step 16530 : Loss : 1.92004299\n",
            "Step 16540 : Loss : 1.91897154\n",
            "Step 16550 : Loss : 1.91791\n",
            "Step 16560 : Loss : 1.91684163\n",
            "Step 16570 : Loss : 1.91580236\n",
            "Step 16580 : Loss : 1.91473699\n",
            "Step 16590 : Loss : 1.91368747\n",
            "Step 16600 : Loss : 1.91262436\n",
            "Step 16604 : Loss :  1.91220629\n",
            "Epoch 29\n",
            "Step 16610 : Loss : 1.91157949\n",
            "Step 16620 : Loss : 1.91050816\n",
            "Step 16630 : Loss : 1.9094398\n",
            "Step 16640 : Loss : 1.90843761\n",
            "Step 16650 : Loss : 1.90740442\n",
            "Step 16660 : Loss : 1.90640163\n",
            "Step 16670 : Loss : 1.90539622\n",
            "Step 16680 : Loss : 1.90435982\n",
            "Step 16690 : Loss : 1.90334272\n",
            "Step 16700 : Loss : 1.90231276\n",
            "Step 16710 : Loss : 1.90130663\n",
            "Step 16720 : Loss : 1.90026951\n",
            "Step 16730 : Loss : 1.89933372\n",
            "Step 16740 : Loss : 1.89830685\n",
            "Step 16750 : Loss : 1.89729953\n",
            "Step 16760 : Loss : 1.89629352\n",
            "Step 16770 : Loss : 1.89526987\n",
            "Step 16780 : Loss : 1.89422488\n",
            "Step 16790 : Loss : 1.89318705\n",
            "Step 16800 : Loss : 1.8921361\n",
            "Step 16810 : Loss : 1.89110446\n",
            "Step 16820 : Loss : 1.89008665\n",
            "Step 16830 : Loss : 1.88908458\n",
            "Step 16840 : Loss : 1.88808811\n",
            "Step 16850 : Loss : 1.88708079\n",
            "Step 16860 : Loss : 1.88609028\n",
            "Step 16870 : Loss : 1.88512838\n",
            "Step 16880 : Loss : 1.8841728\n",
            "Step 16890 : Loss : 1.88321567\n",
            "Step 16900 : Loss : 1.88227761\n",
            "Step 16910 : Loss : 1.88128364\n",
            "Step 16920 : Loss : 1.88031602\n",
            "Step 16930 : Loss : 1.87932682\n",
            "Step 16940 : Loss : 1.87834239\n",
            "Step 16950 : Loss : 1.87735868\n",
            "Step 16960 : Loss : 1.87636554\n",
            "Step 16970 : Loss : 1.87540138\n",
            "Step 16980 : Loss : 1.87442982\n",
            "Step 16990 : Loss : 1.87345088\n",
            "Step 17000 : Loss : 1.87248588\n",
            "Step 17010 : Loss : 1.87149966\n",
            "Step 17020 : Loss : 1.87052464\n",
            "Step 17030 : Loss : 1.86952984\n",
            "Step 17040 : Loss : 1.86852753\n",
            "Step 17050 : Loss : 1.86755109\n",
            "Step 17060 : Loss : 1.86657178\n",
            "Step 17070 : Loss : 1.86565423\n",
            "Step 17080 : Loss : 1.86468291\n",
            "Step 17090 : Loss : 1.86371851\n",
            "Step 17100 : Loss : 1.86274195\n",
            "Step 17110 : Loss : 1.86176443\n",
            "Step 17120 : Loss : 1.86085188\n",
            "Step 17130 : Loss : 1.85987377\n",
            "Step 17140 : Loss : 1.85892069\n",
            "Step 17150 : Loss : 1.85795665\n",
            "Step 17160 : Loss : 1.85699534\n",
            "Step 17170 : Loss : 1.8560313\n",
            "Step 17180 : Loss : 1.85504651\n",
            "Step 17190 : Loss : 1.85406888\n",
            "Step 17197 : Loss :  1.85337901\n",
            "Epoch 30\n",
            "Step 17200 : Loss : 1.85307479\n",
            "Step 17210 : Loss : 1.85206354\n",
            "Step 17220 : Loss : 1.85107255\n",
            "Step 17230 : Loss : 1.85007441\n",
            "Step 17240 : Loss : 1.84910679\n",
            "Step 17250 : Loss : 1.84813416\n",
            "Step 17260 : Loss : 1.84722829\n",
            "Step 17270 : Loss : 1.84626734\n",
            "Step 17280 : Loss : 1.84530497\n",
            "Step 17290 : Loss : 1.8443799\n",
            "Step 17300 : Loss : 1.84348023\n",
            "Step 17310 : Loss : 1.84255517\n",
            "Step 17320 : Loss : 1.84166431\n",
            "Step 17330 : Loss : 1.84073412\n",
            "Step 17340 : Loss : 1.83987403\n",
            "Step 17350 : Loss : 1.83893192\n",
            "Step 17360 : Loss : 1.83798075\n",
            "Step 17370 : Loss : 1.83701074\n",
            "Step 17380 : Loss : 1.83605647\n",
            "Step 17390 : Loss : 1.83509266\n",
            "Step 17400 : Loss : 1.83413613\n",
            "Step 17410 : Loss : 1.8331579\n",
            "Step 17420 : Loss : 1.83220804\n",
            "Step 17430 : Loss : 1.83122253\n",
            "Step 17440 : Loss : 1.83027375\n",
            "Step 17450 : Loss : 1.82928681\n",
            "Step 17460 : Loss : 1.82833898\n",
            "Step 17470 : Loss : 1.82739711\n",
            "Step 17480 : Loss : 1.8264147\n",
            "Step 17490 : Loss : 1.82545626\n",
            "Step 17500 : Loss : 1.82451415\n",
            "Step 17510 : Loss : 1.823578\n",
            "Step 17520 : Loss : 1.82264841\n",
            "Step 17530 : Loss : 1.82171202\n",
            "Step 17540 : Loss : 1.8207798\n",
            "Step 17550 : Loss : 1.81986427\n",
            "Step 17560 : Loss : 1.81894648\n",
            "Step 17570 : Loss : 1.81800401\n",
            "Step 17580 : Loss : 1.8170867\n",
            "Step 17590 : Loss : 1.81616282\n",
            "Step 17600 : Loss : 1.81521487\n",
            "Step 17610 : Loss : 1.81430197\n",
            "Step 17620 : Loss : 1.81337357\n",
            "Step 17630 : Loss : 1.81241858\n",
            "Step 17640 : Loss : 1.81148779\n",
            "Step 17650 : Loss : 1.81055176\n",
            "Step 17660 : Loss : 1.80959392\n",
            "Step 17670 : Loss : 1.80866885\n",
            "Step 17680 : Loss : 1.80775452\n",
            "Step 17690 : Loss : 1.80683506\n",
            "Step 17700 : Loss : 1.80591106\n",
            "Step 17710 : Loss : 1.80497706\n",
            "Step 17720 : Loss : 1.80406904\n",
            "Step 17730 : Loss : 1.80319202\n",
            "Step 17740 : Loss : 1.80227792\n",
            "Step 17750 : Loss : 1.8013593\n",
            "Step 17760 : Loss : 1.80045033\n",
            "Step 17770 : Loss : 1.79952919\n",
            "Step 17780 : Loss : 1.79858482\n",
            "Step 17790 : Loss : 1.79768348\n",
            "Step 17790 : Loss :  1.79768348\n",
            "Epoch 31\n",
            "Step 17800 : Loss : 1.79674923\n",
            "Step 17810 : Loss : 1.79580438\n",
            "Step 17820 : Loss : 1.79491019\n",
            "Step 17830 : Loss : 1.79399514\n",
            "Step 17840 : Loss : 1.79304552\n",
            "Step 17850 : Loss : 1.79214466\n",
            "Step 17860 : Loss : 1.79121673\n",
            "Step 17870 : Loss : 1.7902782\n",
            "Step 17880 : Loss : 1.78938043\n",
            "Step 17890 : Loss : 1.78847396\n",
            "Step 17900 : Loss : 1.78754544\n",
            "Step 17910 : Loss : 1.78662658\n",
            "Step 17920 : Loss : 1.78572273\n",
            "Step 17930 : Loss : 1.78478932\n",
            "Step 17940 : Loss : 1.78386652\n",
            "Step 17950 : Loss : 1.78294837\n",
            "Step 17960 : Loss : 1.78203261\n",
            "Step 17970 : Loss : 1.78110945\n",
            "Step 17980 : Loss : 1.78017724\n",
            "Step 17990 : Loss : 1.77930021\n",
            "Step 18000 : Loss : 1.77838743\n",
            "Step 18010 : Loss : 1.77748489\n",
            "Step 18020 : Loss : 1.77656007\n",
            "Step 18030 : Loss : 1.77564466\n",
            "Step 18040 : Loss : 1.77471626\n",
            "Step 18050 : Loss : 1.77386355\n",
            "Step 18060 : Loss : 1.77295828\n",
            "Step 18070 : Loss : 1.7720561\n",
            "Step 18080 : Loss : 1.77120066\n",
            "Step 18090 : Loss : 1.77030551\n",
            "Step 18100 : Loss : 1.76940775\n",
            "Step 18110 : Loss : 1.76853597\n",
            "Step 18120 : Loss : 1.76764178\n",
            "Step 18130 : Loss : 1.76674306\n",
            "Step 18140 : Loss : 1.76582336\n",
            "Step 18150 : Loss : 1.76494873\n",
            "Step 18160 : Loss : 1.76409471\n",
            "Step 18170 : Loss : 1.76319063\n",
            "Step 18180 : Loss : 1.76230514\n",
            "Step 18190 : Loss : 1.76140416\n",
            "Step 18200 : Loss : 1.76052356\n",
            "Step 18210 : Loss : 1.75962782\n",
            "Step 18220 : Loss : 1.75875473\n",
            "Step 18230 : Loss : 1.75788856\n",
            "Step 18240 : Loss : 1.75702345\n",
            "Step 18250 : Loss : 1.75618279\n",
            "Step 18260 : Loss : 1.75529301\n",
            "Step 18270 : Loss : 1.75443316\n",
            "Step 18280 : Loss : 1.75361013\n",
            "Step 18290 : Loss : 1.75279295\n",
            "Step 18300 : Loss : 1.75196981\n",
            "Step 18310 : Loss : 1.75114572\n",
            "Step 18320 : Loss : 1.75031841\n",
            "Step 18330 : Loss : 1.74948359\n",
            "Step 18340 : Loss : 1.74863732\n",
            "Step 18350 : Loss : 1.74784529\n",
            "Step 18360 : Loss : 1.74704742\n",
            "Step 18370 : Loss : 1.74623597\n",
            "Step 18380 : Loss : 1.74542511\n",
            "Step 18383 : Loss :  1.7451663\n",
            "Epoch 32\n",
            "Step 18390 : Loss : 1.74457467\n",
            "Step 18400 : Loss : 1.74376667\n",
            "Step 18410 : Loss : 1.74295235\n",
            "Step 18420 : Loss : 1.74214864\n",
            "Step 18430 : Loss : 1.74130261\n",
            "Step 18440 : Loss : 1.74050069\n",
            "Step 18450 : Loss : 1.73965287\n",
            "Step 18460 : Loss : 1.73878694\n",
            "Step 18470 : Loss : 1.73794794\n",
            "Step 18480 : Loss : 1.73707962\n",
            "Step 18490 : Loss : 1.73624504\n",
            "Step 18500 : Loss : 1.73542416\n",
            "Step 18510 : Loss : 1.73463702\n",
            "Step 18520 : Loss : 1.73378277\n",
            "Step 18530 : Loss : 1.7329551\n",
            "Step 18540 : Loss : 1.73211217\n",
            "Step 18550 : Loss : 1.73129344\n",
            "Step 18560 : Loss : 1.73046768\n",
            "Step 18570 : Loss : 1.72972\n",
            "Step 18580 : Loss : 1.72894311\n",
            "Step 18590 : Loss : 1.72813594\n",
            "Step 18600 : Loss : 1.72737682\n",
            "Step 18610 : Loss : 1.72657943\n",
            "Step 18620 : Loss : 1.72577691\n",
            "Step 18630 : Loss : 1.72495854\n",
            "Step 18640 : Loss : 1.72414696\n",
            "Step 18650 : Loss : 1.72337127\n",
            "Step 18660 : Loss : 1.72255945\n",
            "Step 18670 : Loss : 1.72172713\n",
            "Step 18680 : Loss : 1.72090745\n",
            "Step 18690 : Loss : 1.72010899\n",
            "Step 18700 : Loss : 1.71931362\n",
            "Step 18710 : Loss : 1.71851289\n",
            "Step 18720 : Loss : 1.71769214\n",
            "Step 18730 : Loss : 1.716869\n",
            "Step 18740 : Loss : 1.71604145\n",
            "Step 18750 : Loss : 1.71522427\n",
            "Step 18760 : Loss : 1.71441829\n",
            "Step 18770 : Loss : 1.71358502\n",
            "Step 18780 : Loss : 1.71276224\n",
            "Step 18790 : Loss : 1.71194923\n",
            "Step 18800 : Loss : 1.71113586\n",
            "Step 18810 : Loss : 1.71033514\n",
            "Step 18820 : Loss : 1.70958805\n",
            "Step 18830 : Loss : 1.70877588\n",
            "Step 18840 : Loss : 1.70802236\n",
            "Step 18850 : Loss : 1.70725799\n",
            "Step 18860 : Loss : 1.70644164\n",
            "Step 18870 : Loss : 1.70566332\n",
            "Step 18880 : Loss : 1.70487309\n",
            "Step 18890 : Loss : 1.70406795\n",
            "Step 18900 : Loss : 1.70329726\n",
            "Step 18910 : Loss : 1.70251453\n",
            "Step 18920 : Loss : 1.7017144\n",
            "Step 18930 : Loss : 1.70091593\n",
            "Step 18940 : Loss : 1.70011091\n",
            "Step 18950 : Loss : 1.69929755\n",
            "Step 18960 : Loss : 1.69852304\n",
            "Step 18970 : Loss : 1.69772768\n",
            "Step 18976 : Loss :  1.69725275\n",
            "Epoch 33\n",
            "Step 18980 : Loss : 1.69691408\n",
            "Step 18990 : Loss : 1.69609106\n",
            "Step 19000 : Loss : 1.69528162\n",
            "Step 19010 : Loss : 1.69448626\n",
            "Step 19020 : Loss : 1.69367588\n",
            "Step 19030 : Loss : 1.6928575\n",
            "Step 19040 : Loss : 1.69207478\n",
            "Step 19050 : Loss : 1.69125199\n",
            "Step 19060 : Loss : 1.69043446\n",
            "Step 19070 : Loss : 1.68961942\n",
            "Step 19080 : Loss : 1.68884575\n",
            "Step 19090 : Loss : 1.68801308\n",
            "Step 19100 : Loss : 1.68718791\n",
            "Step 19110 : Loss : 1.68639493\n",
            "Step 19120 : Loss : 1.68559873\n",
            "Step 19130 : Loss : 1.68477523\n",
            "Step 19140 : Loss : 1.68396091\n",
            "Step 19150 : Loss : 1.68312752\n",
            "Step 19160 : Loss : 1.68229961\n",
            "Step 19170 : Loss : 1.68150556\n",
            "Step 19180 : Loss : 1.6807183\n",
            "Step 19190 : Loss : 1.67993104\n",
            "Step 19200 : Loss : 1.67916155\n",
            "Step 19210 : Loss : 1.67837417\n",
            "Step 19220 : Loss : 1.67757523\n",
            "Step 19230 : Loss : 1.676741\n",
            "Step 19240 : Loss : 1.67591357\n",
            "Step 19250 : Loss : 1.67512357\n",
            "Step 19260 : Loss : 1.67433643\n",
            "Step 19270 : Loss : 1.67359436\n",
            "Step 19280 : Loss : 1.67279267\n",
            "Step 19290 : Loss : 1.67199314\n",
            "Step 19300 : Loss : 1.67119265\n",
            "Step 19310 : Loss : 1.6703819\n",
            "Step 19320 : Loss : 1.66957438\n",
            "Step 19330 : Loss : 1.66874671\n",
            "Step 19340 : Loss : 1.66794038\n",
            "Step 19350 : Loss : 1.66714132\n",
            "Step 19360 : Loss : 1.66634595\n",
            "Step 19370 : Loss : 1.66553771\n",
            "Step 19380 : Loss : 1.66476691\n",
            "Step 19390 : Loss : 1.66396129\n",
            "Step 19400 : Loss : 1.66315436\n",
            "Step 19410 : Loss : 1.66236937\n",
            "Step 19420 : Loss : 1.66160619\n",
            "Step 19430 : Loss : 1.66085267\n",
            "Step 19440 : Loss : 1.66006625\n",
            "Step 19450 : Loss : 1.65929019\n",
            "Step 19460 : Loss : 1.65855551\n",
            "Step 19470 : Loss : 1.65778\n",
            "Step 19480 : Loss : 1.65701294\n",
            "Step 19490 : Loss : 1.6562283\n",
            "Step 19500 : Loss : 1.65544498\n",
            "Step 19510 : Loss : 1.65469205\n",
            "Step 19520 : Loss : 1.65392721\n",
            "Step 19530 : Loss : 1.65317369\n",
            "Step 19540 : Loss : 1.65239882\n",
            "Step 19550 : Loss : 1.65159667\n",
            "Step 19560 : Loss : 1.65082383\n",
            "Step 19569 : Loss :  1.65011489\n",
            "Epoch 34\n",
            "Step 19570 : Loss : 1.65003657\n",
            "Step 19580 : Loss : 1.64926279\n",
            "Step 19590 : Loss : 1.64850235\n",
            "Step 19600 : Loss : 1.64771247\n",
            "Step 19610 : Loss : 1.64698339\n",
            "Step 19620 : Loss : 1.64622796\n",
            "Step 19630 : Loss : 1.64548934\n",
            "Step 19640 : Loss : 1.64471924\n",
            "Step 19650 : Loss : 1.64396214\n",
            "Step 19660 : Loss : 1.64321136\n",
            "Step 19670 : Loss : 1.64245629\n",
            "Step 19680 : Loss : 1.64171278\n",
            "Step 19690 : Loss : 1.64097106\n",
            "Step 19700 : Loss : 1.64022112\n",
            "Step 19710 : Loss : 1.63947403\n",
            "Step 19720 : Loss : 1.63872504\n",
            "Step 19730 : Loss : 1.63795102\n",
            "Step 19740 : Loss : 1.63717818\n",
            "Step 19750 : Loss : 1.63646245\n",
            "Step 19760 : Loss : 1.63570893\n",
            "Step 19770 : Loss : 1.63497472\n",
            "Step 19780 : Loss : 1.63421893\n",
            "Step 19790 : Loss : 1.63348365\n",
            "Step 19800 : Loss : 1.63277674\n",
            "Step 19810 : Loss : 1.6320349\n",
            "Step 19820 : Loss : 1.63128138\n",
            "Step 19830 : Loss : 1.63055813\n",
            "Step 19840 : Loss : 1.62981427\n",
            "Step 19850 : Loss : 1.6290648\n",
            "Step 19860 : Loss : 1.62832057\n",
            "Step 19870 : Loss : 1.62757051\n",
            "Step 19880 : Loss : 1.62687337\n",
            "Step 19890 : Loss : 1.6261636\n",
            "Step 19900 : Loss : 1.6254214\n",
            "Step 19910 : Loss : 1.62471616\n",
            "Step 19920 : Loss : 1.6239959\n",
            "Step 19930 : Loss : 1.62325287\n",
            "Step 19940 : Loss : 1.622509\n",
            "Step 19950 : Loss : 1.62173271\n",
            "Step 19960 : Loss : 1.62098503\n",
            "Step 19970 : Loss : 1.62025523\n",
            "Step 19980 : Loss : 1.61955571\n",
            "Step 19990 : Loss : 1.61882114\n",
            "Step 20000 : Loss : 1.61807\n",
            "Step 20010 : Loss : 1.61732554\n",
            "Step 20020 : Loss : 1.61660957\n",
            "Step 20030 : Loss : 1.61586463\n",
            "Step 20040 : Loss : 1.61515021\n",
            "Step 20050 : Loss : 1.61443746\n",
            "Step 20060 : Loss : 1.61372793\n",
            "Step 20070 : Loss : 1.61300385\n",
            "Step 20080 : Loss : 1.61226535\n",
            "Step 20090 : Loss : 1.61157298\n",
            "Step 20100 : Loss : 1.61089051\n",
            "Step 20110 : Loss : 1.61019933\n",
            "Step 20120 : Loss : 1.60949886\n",
            "Step 20130 : Loss : 1.60879266\n",
            "Step 20140 : Loss : 1.60808599\n",
            "Step 20150 : Loss : 1.60736179\n",
            "Step 20160 : Loss : 1.60668945\n",
            "Step 20162 : Loss :  1.60653853\n",
            "Epoch 35\n",
            "Step 20170 : Loss : 1.60600305\n",
            "Step 20180 : Loss : 1.60531294\n",
            "Step 20190 : Loss : 1.60460961\n",
            "Step 20200 : Loss : 1.6039077\n",
            "Step 20210 : Loss : 1.60324526\n",
            "Step 20220 : Loss : 1.6025846\n",
            "Step 20230 : Loss : 1.60193038\n",
            "Step 20240 : Loss : 1.60120773\n",
            "Step 20250 : Loss : 1.6005168\n",
            "Step 20260 : Loss : 1.59984314\n",
            "Step 20270 : Loss : 1.59915602\n",
            "Step 20280 : Loss : 1.59852433\n",
            "Step 20290 : Loss : 1.59783757\n",
            "Step 20300 : Loss : 1.59713674\n",
            "Step 20310 : Loss : 1.59646356\n",
            "Step 20320 : Loss : 1.59580183\n",
            "Step 20330 : Loss : 1.59508646\n",
            "Step 20340 : Loss : 1.5944041\n",
            "Step 20350 : Loss : 1.59371233\n",
            "Step 20360 : Loss : 1.59302628\n",
            "Step 20370 : Loss : 1.59233618\n",
            "Step 20380 : Loss : 1.59164917\n",
            "Step 20390 : Loss : 1.59094584\n",
            "Step 20400 : Loss : 1.59022236\n",
            "Step 20410 : Loss : 1.58950007\n",
            "Step 20420 : Loss : 1.58880091\n",
            "Step 20430 : Loss : 1.58812737\n",
            "Step 20440 : Loss : 1.58741319\n",
            "Step 20450 : Loss : 1.58674395\n",
            "Step 20460 : Loss : 1.58607817\n",
            "Step 20470 : Loss : 1.58542216\n",
            "Step 20480 : Loss : 1.58477426\n",
            "Step 20490 : Loss : 1.58408391\n",
            "Step 20500 : Loss : 1.58340204\n",
            "Step 20510 : Loss : 1.58274174\n",
            "Step 20520 : Loss : 1.58207703\n",
            "Step 20530 : Loss : 1.58141029\n",
            "Step 20540 : Loss : 1.58077502\n",
            "Step 20550 : Loss : 1.58007658\n",
            "Step 20560 : Loss : 1.57939589\n",
            "Step 20570 : Loss : 1.57873666\n",
            "Step 20580 : Loss : 1.57805777\n",
            "Step 20590 : Loss : 1.57741046\n",
            "Step 20600 : Loss : 1.57673502\n",
            "Step 20610 : Loss : 1.57603836\n",
            "Step 20620 : Loss : 1.5753603\n",
            "Step 20630 : Loss : 1.57466304\n",
            "Step 20640 : Loss : 1.57397866\n",
            "Step 20650 : Loss : 1.57331336\n",
            "Step 20660 : Loss : 1.57262993\n",
            "Step 20670 : Loss : 1.57194805\n",
            "Step 20680 : Loss : 1.57125771\n",
            "Step 20690 : Loss : 1.57056272\n",
            "Step 20700 : Loss : 1.56986952\n",
            "Step 20710 : Loss : 1.56917381\n",
            "Step 20720 : Loss : 1.56849122\n",
            "Step 20730 : Loss : 1.56780946\n",
            "Step 20740 : Loss : 1.56713426\n",
            "Step 20750 : Loss : 1.56645215\n",
            "Step 20755 : Loss :  1.56612706\n",
            "Epoch 36\n",
            "Step 20760 : Loss : 1.56580281\n",
            "Step 20770 : Loss : 1.56517041\n",
            "Step 20780 : Loss : 1.56451428\n",
            "Step 20790 : Loss : 1.56385171\n",
            "Step 20800 : Loss : 1.56318688\n",
            "Step 20810 : Loss : 1.56252408\n",
            "Step 20820 : Loss : 1.56188881\n",
            "Step 20830 : Loss : 1.56120443\n",
            "Step 20840 : Loss : 1.56051338\n",
            "Step 20850 : Loss : 1.55983579\n",
            "Step 20860 : Loss : 1.55913591\n",
            "Step 20870 : Loss : 1.55845463\n",
            "Step 20880 : Loss : 1.55776465\n",
            "Step 20890 : Loss : 1.55709016\n",
            "Step 20900 : Loss : 1.55641186\n",
            "Step 20910 : Loss : 1.55571568\n",
            "Step 20920 : Loss : 1.55503428\n",
            "Step 20930 : Loss : 1.55435383\n",
            "Step 20940 : Loss : 1.55367684\n",
            "Step 20950 : Loss : 1.55299282\n",
            "Step 20960 : Loss : 1.55231082\n",
            "Step 20970 : Loss : 1.55162024\n",
            "Step 20980 : Loss : 1.5509398\n",
            "Step 20990 : Loss : 1.55024803\n",
            "Step 21000 : Loss : 1.54955208\n",
            "Step 21010 : Loss : 1.54885793\n",
            "Step 21020 : Loss : 1.54816449\n",
            "Step 21030 : Loss : 1.54751146\n",
            "Step 21040 : Loss : 1.54682887\n",
            "Step 21050 : Loss : 1.54616272\n",
            "Step 21060 : Loss : 1.54549134\n",
            "Step 21070 : Loss : 1.54482579\n",
            "Step 21080 : Loss : 1.54417145\n",
            "Step 21090 : Loss : 1.54348946\n",
            "Step 21100 : Loss : 1.54284668\n",
            "Step 21110 : Loss : 1.54216897\n",
            "Step 21120 : Loss : 1.5415566\n",
            "Step 21130 : Loss : 1.54087269\n",
            "Step 21140 : Loss : 1.54021955\n",
            "Step 21150 : Loss : 1.53956735\n",
            "Step 21160 : Loss : 1.53894126\n",
            "Step 21170 : Loss : 1.53826034\n",
            "Step 21180 : Loss : 1.53762031\n",
            "Step 21190 : Loss : 1.5369817\n",
            "Step 21200 : Loss : 1.5363313\n",
            "Step 21210 : Loss : 1.53566432\n",
            "Step 21220 : Loss : 1.53499019\n",
            "Step 21230 : Loss : 1.53433084\n",
            "Step 21240 : Loss : 1.53365779\n",
            "Step 21250 : Loss : 1.53301847\n",
            "Step 21260 : Loss : 1.53235579\n",
            "Step 21270 : Loss : 1.53170228\n",
            "Step 21280 : Loss : 1.5310328\n",
            "Step 21290 : Loss : 1.53036869\n",
            "Step 21300 : Loss : 1.529706\n",
            "Step 21310 : Loss : 1.52902091\n",
            "Step 21320 : Loss : 1.52837038\n",
            "Step 21330 : Loss : 1.52769434\n",
            "Step 21340 : Loss : 1.52703333\n",
            "Step 21348 : Loss :  1.52652669\n",
            "Epoch 37\n",
            "Step 21350 : Loss : 1.52639353\n",
            "Step 21360 : Loss : 1.52577639\n",
            "Step 21370 : Loss : 1.5251205\n",
            "Step 21380 : Loss : 1.52447295\n",
            "Step 21390 : Loss : 1.52384341\n",
            "Step 21400 : Loss : 1.52318597\n",
            "Step 21410 : Loss : 1.52256775\n",
            "Step 21420 : Loss : 1.52193618\n",
            "Step 21430 : Loss : 1.52131724\n",
            "Step 21440 : Loss : 1.52070105\n",
            "Step 21450 : Loss : 1.52002847\n",
            "Step 21460 : Loss : 1.51942611\n",
            "Step 21470 : Loss : 1.51878548\n",
            "Step 21480 : Loss : 1.51812565\n",
            "Step 21490 : Loss : 1.51749814\n",
            "Step 21500 : Loss : 1.51687694\n",
            "Step 21510 : Loss : 1.51622736\n",
            "Step 21520 : Loss : 1.51561379\n",
            "Step 21530 : Loss : 1.51499248\n",
            "Step 21540 : Loss : 1.51436448\n",
            "Step 21550 : Loss : 1.513762\n",
            "Step 21560 : Loss : 1.51313794\n",
            "Step 21570 : Loss : 1.51250327\n",
            "Step 21580 : Loss : 1.5119065\n",
            "Step 21590 : Loss : 1.5113014\n",
            "Step 21600 : Loss : 1.51068604\n",
            "Step 21610 : Loss : 1.51009381\n",
            "Step 21620 : Loss : 1.50949597\n",
            "Step 21630 : Loss : 1.50886464\n",
            "Step 21640 : Loss : 1.50826919\n",
            "Step 21650 : Loss : 1.50764692\n",
            "Step 21660 : Loss : 1.50701618\n",
            "Step 21670 : Loss : 1.50639343\n",
            "Step 21680 : Loss : 1.50575924\n",
            "Step 21690 : Loss : 1.50513184\n",
            "Step 21700 : Loss : 1.50449371\n",
            "Step 21710 : Loss : 1.50387466\n",
            "Step 21720 : Loss : 1.50323665\n",
            "Step 21730 : Loss : 1.50260377\n",
            "Step 21740 : Loss : 1.501984\n",
            "Step 21750 : Loss : 1.50140238\n",
            "Step 21760 : Loss : 1.50077558\n",
            "Step 21770 : Loss : 1.50019503\n",
            "Step 21780 : Loss : 1.49957204\n",
            "Step 21790 : Loss : 1.49893475\n",
            "Step 21800 : Loss : 1.49829304\n",
            "Step 21810 : Loss : 1.49766839\n",
            "Step 21820 : Loss : 1.49702609\n",
            "Step 21830 : Loss : 1.49641049\n",
            "Step 21840 : Loss : 1.49578035\n",
            "Step 21850 : Loss : 1.49514639\n",
            "Step 21860 : Loss : 1.49453175\n",
            "Step 21870 : Loss : 1.49388337\n",
            "Step 21880 : Loss : 1.49324703\n",
            "Step 21890 : Loss : 1.49260807\n",
            "Step 21900 : Loss : 1.49197066\n",
            "Step 21910 : Loss : 1.491328\n",
            "Step 21920 : Loss : 1.49072015\n",
            "Step 21930 : Loss : 1.49009478\n",
            "Step 21940 : Loss : 1.48946953\n",
            "Step 21941 : Loss :  1.48940468\n",
            "Epoch 38\n",
            "Step 21950 : Loss : 1.48885036\n",
            "Step 21960 : Loss : 1.48821902\n",
            "Step 21970 : Loss : 1.4876132\n",
            "Step 21980 : Loss : 1.4870019\n",
            "Step 21990 : Loss : 1.4863863\n",
            "Step 22000 : Loss : 1.48580444\n",
            "Step 22010 : Loss : 1.4852246\n",
            "Step 22020 : Loss : 1.48460817\n",
            "Step 22030 : Loss : 1.48400712\n",
            "Step 22040 : Loss : 1.48338962\n",
            "Step 22050 : Loss : 1.48278737\n",
            "Step 22060 : Loss : 1.48219979\n",
            "Step 22070 : Loss : 1.4815805\n",
            "Step 22080 : Loss : 1.48098874\n",
            "Step 22090 : Loss : 1.48041618\n",
            "Step 22100 : Loss : 1.47980511\n",
            "Step 22110 : Loss : 1.47919977\n",
            "Step 22120 : Loss : 1.47859895\n",
            "Step 22130 : Loss : 1.47799504\n",
            "Step 22140 : Loss : 1.47741878\n",
            "Step 22150 : Loss : 1.47683525\n",
            "Step 22160 : Loss : 1.47627902\n",
            "Step 22170 : Loss : 1.47566843\n",
            "Step 22180 : Loss : 1.47506177\n",
            "Step 22190 : Loss : 1.47444105\n",
            "Step 22200 : Loss : 1.4738946\n",
            "Step 22210 : Loss : 1.47329795\n",
            "Step 22220 : Loss : 1.47269416\n",
            "Step 22230 : Loss : 1.47211778\n",
            "Step 22240 : Loss : 1.47155166\n",
            "Step 22250 : Loss : 1.47100294\n",
            "Step 22260 : Loss : 1.47053599\n",
            "Step 22270 : Loss : 1.46994936\n",
            "Step 22280 : Loss : 1.46935439\n",
            "Step 22290 : Loss : 1.46880054\n",
            "Step 22300 : Loss : 1.46823812\n",
            "Step 22310 : Loss : 1.46764207\n",
            "Step 22320 : Loss : 1.4670527\n",
            "Step 22330 : Loss : 1.46647382\n",
            "Step 22340 : Loss : 1.46589613\n",
            "Step 22350 : Loss : 1.4653337\n",
            "Step 22360 : Loss : 1.46474886\n",
            "Step 22370 : Loss : 1.46414435\n",
            "Step 22380 : Loss : 1.46356893\n",
            "Step 22390 : Loss : 1.46298921\n",
            "Step 22400 : Loss : 1.46242416\n",
            "Step 22410 : Loss : 1.46182799\n",
            "Step 22420 : Loss : 1.46125448\n",
            "Step 22430 : Loss : 1.46069694\n",
            "Step 22440 : Loss : 1.46010232\n",
            "Step 22450 : Loss : 1.45953739\n",
            "Step 22460 : Loss : 1.45894301\n",
            "Step 22470 : Loss : 1.45835364\n",
            "Step 22480 : Loss : 1.45775843\n",
            "Step 22490 : Loss : 1.45714688\n",
            "Step 22500 : Loss : 1.45658123\n",
            "Step 22510 : Loss : 1.45599687\n",
            "Step 22520 : Loss : 1.45543635\n",
            "Step 22530 : Loss : 1.45486665\n",
            "Step 22534 : Loss :  1.45462668\n",
            "Epoch 39\n",
            "Step 22540 : Loss : 1.45428264\n",
            "Step 22550 : Loss : 1.45370293\n",
            "Step 22560 : Loss : 1.45312572\n",
            "Step 22570 : Loss : 1.45256\n",
            "Step 22580 : Loss : 1.45198083\n",
            "Step 22590 : Loss : 1.45141518\n",
            "Step 22600 : Loss : 1.45084405\n",
            "Step 22610 : Loss : 1.45028746\n",
            "Step 22620 : Loss : 1.44972956\n",
            "Step 22630 : Loss : 1.44915795\n",
            "Step 22640 : Loss : 1.44859207\n",
            "Step 22650 : Loss : 1.44802392\n",
            "Step 22660 : Loss : 1.4474349\n",
            "Step 22670 : Loss : 1.44687116\n",
            "Step 22680 : Loss : 1.44631422\n",
            "Step 22690 : Loss : 1.44572175\n",
            "Step 22700 : Loss : 1.44514608\n",
            "Step 22710 : Loss : 1.44457352\n",
            "Step 22720 : Loss : 1.44401681\n",
            "Step 22730 : Loss : 1.44348931\n",
            "Step 22740 : Loss : 1.44297409\n",
            "Step 22750 : Loss : 1.44240248\n",
            "Step 22760 : Loss : 1.44184566\n",
            "Step 22770 : Loss : 1.44126844\n",
            "Step 22780 : Loss : 1.44070065\n",
            "Step 22790 : Loss : 1.44013202\n",
            "Step 22800 : Loss : 1.43956709\n",
            "Step 22810 : Loss : 1.43899953\n",
            "Step 22820 : Loss : 1.43845737\n",
            "Step 22830 : Loss : 1.43788469\n",
            "Step 22840 : Loss : 1.43731439\n",
            "Step 22850 : Loss : 1.43675673\n",
            "Step 22860 : Loss : 1.4361887\n",
            "Step 22870 : Loss : 1.43561745\n",
            "Step 22880 : Loss : 1.43503737\n",
            "Step 22890 : Loss : 1.43447828\n",
            "Step 22900 : Loss : 1.43390834\n",
            "Step 22910 : Loss : 1.43338728\n",
            "Step 22920 : Loss : 1.4328357\n",
            "Step 22930 : Loss : 1.43230164\n",
            "Step 22940 : Loss : 1.43176293\n",
            "Step 22950 : Loss : 1.43122399\n",
            "Step 22960 : Loss : 1.43067\n",
            "Step 22970 : Loss : 1.43011844\n",
            "Step 22980 : Loss : 1.42955697\n",
            "Step 22990 : Loss : 1.42899132\n",
            "Step 23000 : Loss : 1.42842829\n",
            "Step 23010 : Loss : 1.42791045\n",
            "Step 23020 : Loss : 1.42738223\n",
            "Step 23030 : Loss : 1.42682993\n",
            "Step 23040 : Loss : 1.42627561\n",
            "Step 23050 : Loss : 1.42574513\n",
            "Step 23060 : Loss : 1.42521179\n",
            "Step 23070 : Loss : 1.42464471\n",
            "Step 23080 : Loss : 1.42411125\n",
            "Step 23090 : Loss : 1.42355573\n",
            "Step 23100 : Loss : 1.42299938\n",
            "Step 23110 : Loss : 1.42242146\n",
            "Step 23120 : Loss : 1.4218576\n",
            "Step 23127 : Loss :  1.42145562\n",
            "Epoch 40\n",
            "Step 23130 : Loss : 1.42130017\n",
            "Step 23140 : Loss : 1.42074597\n",
            "Step 23150 : Loss : 1.42020857\n",
            "Step 23160 : Loss : 1.41965365\n",
            "Step 23170 : Loss : 1.41909\n",
            "Step 23180 : Loss : 1.41855979\n",
            "Step 23190 : Loss : 1.41803169\n",
            "Step 23200 : Loss : 1.41746414\n",
            "Step 23210 : Loss : 1.41689396\n",
            "Step 23220 : Loss : 1.41634989\n",
            "Step 23230 : Loss : 1.4158181\n",
            "Step 23240 : Loss : 1.41528058\n",
            "Step 23250 : Loss : 1.41472459\n",
            "Step 23260 : Loss : 1.41418779\n",
            "Step 23270 : Loss : 1.41365445\n",
            "Step 23280 : Loss : 1.41309965\n",
            "Step 23290 : Loss : 1.41255236\n",
            "Step 23300 : Loss : 1.41202235\n",
            "Step 23310 : Loss : 1.41147053\n",
            "Step 23320 : Loss : 1.41092908\n",
            "Step 23330 : Loss : 1.41039169\n",
            "Step 23340 : Loss : 1.40983272\n",
            "Step 23350 : Loss : 1.40928245\n",
            "Step 23360 : Loss : 1.40871298\n",
            "Step 23370 : Loss : 1.40817976\n",
            "Step 23380 : Loss : 1.4076128\n",
            "Step 23390 : Loss : 1.40708518\n",
            "Step 23400 : Loss : 1.40655363\n",
            "Step 23410 : Loss : 1.40604615\n",
            "Step 23420 : Loss : 1.4054991\n",
            "Step 23430 : Loss : 1.40494597\n",
            "Step 23440 : Loss : 1.40439558\n",
            "Step 23450 : Loss : 1.40387297\n",
            "Step 23460 : Loss : 1.40330982\n",
            "Step 23470 : Loss : 1.40276468\n",
            "Step 23480 : Loss : 1.40220499\n",
            "Step 23490 : Loss : 1.40165675\n",
            "Step 23500 : Loss : 1.40109646\n",
            "Step 23510 : Loss : 1.40053117\n",
            "Step 23520 : Loss : 1.39998507\n",
            "Step 23530 : Loss : 1.39941096\n",
            "Step 23540 : Loss : 1.3988899\n",
            "Step 23550 : Loss : 1.39833283\n",
            "Step 23560 : Loss : 1.39778709\n",
            "Step 23570 : Loss : 1.39722848\n",
            "Step 23580 : Loss : 1.39667821\n",
            "Step 23590 : Loss : 1.3961153\n",
            "Step 23600 : Loss : 1.39556754\n",
            "Step 23610 : Loss : 1.39504766\n",
            "Step 23620 : Loss : 1.39452016\n",
            "Step 23630 : Loss : 1.39398444\n",
            "Step 23640 : Loss : 1.39342415\n",
            "Step 23650 : Loss : 1.39287078\n",
            "Step 23660 : Loss : 1.39234436\n",
            "Step 23670 : Loss : 1.39180386\n",
            "Step 23680 : Loss : 1.39125907\n",
            "Step 23690 : Loss : 1.39072216\n",
            "Step 23700 : Loss : 1.39017129\n",
            "Step 23710 : Loss : 1.38961077\n",
            "Step 23720 : Loss : 1.38905299\n",
            "Step 23720 : Loss :  1.38905299\n",
            "Epoch 41\n",
            "Step 23730 : Loss : 1.38850558\n",
            "Step 23740 : Loss : 1.38795674\n",
            "Step 23750 : Loss : 1.38743389\n",
            "Step 23760 : Loss : 1.38688624\n",
            "Step 23770 : Loss : 1.38635063\n",
            "Step 23780 : Loss : 1.38583767\n",
            "Step 23790 : Loss : 1.38530958\n",
            "Step 23800 : Loss : 1.38476086\n",
            "Step 23810 : Loss : 1.38421619\n",
            "Step 23820 : Loss : 1.38368225\n",
            "Step 23830 : Loss : 1.38316751\n",
            "Step 23840 : Loss : 1.38262582\n",
            "Step 23850 : Loss : 1.38208854\n",
            "Step 23860 : Loss : 1.38156939\n",
            "Step 23870 : Loss : 1.38103545\n",
            "Step 23880 : Loss : 1.38050342\n",
            "Step 23890 : Loss : 1.37997282\n",
            "Step 23900 : Loss : 1.37945294\n",
            "Step 23910 : Loss : 1.37892652\n",
            "Step 23920 : Loss : 1.37839735\n",
            "Step 23930 : Loss : 1.37789989\n",
            "Step 23940 : Loss : 1.37737715\n",
            "Step 23950 : Loss : 1.37688279\n",
            "Step 23960 : Loss : 1.37640417\n",
            "Step 23970 : Loss : 1.37591457\n",
            "Step 23980 : Loss : 1.37544\n",
            "Step 23990 : Loss : 1.37490344\n",
            "Step 24000 : Loss : 1.37440228\n",
            "Step 24010 : Loss : 1.37388659\n",
            "Step 24020 : Loss : 1.37340903\n",
            "Step 24030 : Loss : 1.37290502\n",
            "Step 24040 : Loss : 1.37242305\n",
            "Step 24050 : Loss : 1.37190938\n",
            "Step 24060 : Loss : 1.37139463\n",
            "Step 24070 : Loss : 1.3708725\n",
            "Step 24080 : Loss : 1.37036347\n",
            "Step 24090 : Loss : 1.36984646\n",
            "Step 24100 : Loss : 1.36935771\n",
            "Step 24110 : Loss : 1.36884224\n",
            "Step 24120 : Loss : 1.36831415\n",
            "Step 24130 : Loss : 1.36780334\n",
            "Step 24140 : Loss : 1.36729717\n",
            "Step 24150 : Loss : 1.36678731\n",
            "Step 24160 : Loss : 1.36628079\n",
            "Step 24170 : Loss : 1.36577189\n",
            "Step 24180 : Loss : 1.3652544\n",
            "Step 24190 : Loss : 1.36472809\n",
            "Step 24200 : Loss : 1.36424959\n",
            "Step 24210 : Loss : 1.36374211\n",
            "Step 24220 : Loss : 1.3632412\n",
            "Step 24230 : Loss : 1.36272728\n",
            "Step 24240 : Loss : 1.36225474\n",
            "Step 24250 : Loss : 1.36178255\n",
            "Step 24260 : Loss : 1.36128032\n",
            "Step 24270 : Loss : 1.36079025\n",
            "Step 24280 : Loss : 1.36029029\n",
            "Step 24290 : Loss : 1.35980642\n",
            "Step 24300 : Loss : 1.35930037\n",
            "Step 24310 : Loss : 1.35881186\n",
            "Step 24313 : Loss :  1.35865581\n",
            "Epoch 42\n",
            "Step 24320 : Loss : 1.35831952\n",
            "Step 24330 : Loss : 1.35783398\n",
            "Step 24340 : Loss : 1.35733533\n",
            "Step 24350 : Loss : 1.35687745\n",
            "Step 24360 : Loss : 1.35641134\n",
            "Step 24370 : Loss : 1.35593903\n",
            "Step 24380 : Loss : 1.35545433\n",
            "Step 24390 : Loss : 1.35496235\n",
            "Step 24400 : Loss : 1.35446835\n",
            "Step 24410 : Loss : 1.35397732\n",
            "Step 24420 : Loss : 1.3535161\n",
            "Step 24430 : Loss : 1.35302055\n",
            "Step 24440 : Loss : 1.35251653\n",
            "Step 24450 : Loss : 1.35201645\n",
            "Step 24460 : Loss : 1.35153866\n",
            "Step 24470 : Loss : 1.35103071\n",
            "Step 24480 : Loss : 1.35054886\n",
            "Step 24490 : Loss : 1.35006177\n",
            "Step 24500 : Loss : 1.3495729\n",
            "Step 24510 : Loss : 1.34910035\n",
            "Step 24520 : Loss : 1.34861135\n",
            "Step 24530 : Loss : 1.34811854\n",
            "Step 24540 : Loss : 1.34763\n",
            "Step 24550 : Loss : 1.34714806\n",
            "Step 24560 : Loss : 1.34665298\n",
            "Step 24570 : Loss : 1.34614623\n",
            "Step 24580 : Loss : 1.34566486\n",
            "Step 24590 : Loss : 1.3451575\n",
            "Step 24600 : Loss : 1.3446722\n",
            "Step 24610 : Loss : 1.34418952\n",
            "Step 24620 : Loss : 1.34373713\n",
            "Step 24630 : Loss : 1.34329259\n",
            "Step 24640 : Loss : 1.3427937\n",
            "Step 24650 : Loss : 1.34228969\n",
            "Step 24660 : Loss : 1.34179592\n",
            "Step 24670 : Loss : 1.34130979\n",
            "Step 24680 : Loss : 1.34085572\n",
            "Step 24690 : Loss : 1.34036946\n",
            "Step 24700 : Loss : 1.33988571\n",
            "Step 24710 : Loss : 1.33941483\n",
            "Step 24720 : Loss : 1.33894336\n",
            "Step 24730 : Loss : 1.33844423\n",
            "Step 24740 : Loss : 1.33797979\n",
            "Step 24750 : Loss : 1.33753538\n",
            "Step 24760 : Loss : 1.33709514\n",
            "Step 24770 : Loss : 1.33662021\n",
            "Step 24780 : Loss : 1.3361336\n",
            "Step 24790 : Loss : 1.33567715\n",
            "Step 24800 : Loss : 1.3351953\n",
            "Step 24810 : Loss : 1.33472693\n",
            "Step 24820 : Loss : 1.33428919\n",
            "Step 24830 : Loss : 1.33381939\n",
            "Step 24840 : Loss : 1.33336198\n",
            "Step 24850 : Loss : 1.33291531\n",
            "Step 24860 : Loss : 1.3324542\n",
            "Step 24870 : Loss : 1.33197498\n",
            "Step 24880 : Loss : 1.3314817\n",
            "Step 24890 : Loss : 1.33102357\n",
            "Step 24900 : Loss : 1.33054578\n",
            "Step 24906 : Loss :  1.33025658\n",
            "Epoch 43\n",
            "Step 24910 : Loss : 1.33006632\n",
            "Step 24920 : Loss : 1.32960546\n",
            "Step 24930 : Loss : 1.32914329\n",
            "Step 24940 : Loss : 1.32868028\n",
            "Step 24950 : Loss : 1.32822335\n",
            "Step 24960 : Loss : 1.32774568\n",
            "Step 24970 : Loss : 1.3272866\n",
            "Step 24980 : Loss : 1.32682157\n",
            "Step 24990 : Loss : 1.32633162\n",
            "Step 25000 : Loss : 1.32584739\n",
            "Step 25010 : Loss : 1.32536542\n",
            "Step 25020 : Loss : 1.32488966\n",
            "Step 25030 : Loss : 1.32441628\n",
            "Step 25040 : Loss : 1.32395124\n",
            "Step 25050 : Loss : 1.32348704\n",
            "Step 25060 : Loss : 1.32304239\n",
            "Step 25070 : Loss : 1.32255876\n",
            "Step 25080 : Loss : 1.32208645\n",
            "Step 25090 : Loss : 1.32161307\n",
            "Step 25100 : Loss : 1.32112741\n",
            "Step 25110 : Loss : 1.32065213\n",
            "Step 25120 : Loss : 1.3201735\n",
            "Step 25130 : Loss : 1.31969666\n",
            "Step 25140 : Loss : 1.3192122\n",
            "Step 25150 : Loss : 1.31871259\n",
            "Step 25160 : Loss : 1.31823254\n",
            "Step 25170 : Loss : 1.31775415\n",
            "Step 25180 : Loss : 1.31728649\n",
            "Step 25190 : Loss : 1.31679296\n",
            "Step 25200 : Loss : 1.31631\n",
            "Step 25210 : Loss : 1.31585217\n",
            "Step 25220 : Loss : 1.31538355\n",
            "Step 25230 : Loss : 1.314888\n",
            "Step 25240 : Loss : 1.31438863\n",
            "Step 25250 : Loss : 1.31389391\n",
            "Step 25260 : Loss : 1.31340635\n",
            "Step 25270 : Loss : 1.31291103\n",
            "Step 25280 : Loss : 1.31242228\n",
            "Step 25290 : Loss : 1.3119483\n",
            "Step 25300 : Loss : 1.31145394\n",
            "Step 25310 : Loss : 1.3109951\n",
            "Step 25320 : Loss : 1.31051862\n",
            "Step 25330 : Loss : 1.31008625\n",
            "Step 25340 : Loss : 1.30963433\n",
            "Step 25350 : Loss : 1.30916774\n",
            "Step 25360 : Loss : 1.30869019\n",
            "Step 25370 : Loss : 1.30823\n",
            "Step 25380 : Loss : 1.30773878\n",
            "Step 25390 : Loss : 1.30728805\n",
            "Step 25400 : Loss : 1.30685794\n",
            "Step 25410 : Loss : 1.30640185\n",
            "Step 25420 : Loss : 1.30595315\n",
            "Step 25430 : Loss : 1.30550563\n",
            "Step 25440 : Loss : 1.30505705\n",
            "Step 25450 : Loss : 1.30460548\n",
            "Step 25460 : Loss : 1.3041333\n",
            "Step 25470 : Loss : 1.30367553\n",
            "Step 25480 : Loss : 1.30325162\n",
            "Step 25490 : Loss : 1.30279386\n",
            "Step 25499 : Loss :  1.3023814\n",
            "Epoch 44\n",
            "Step 25500 : Loss : 1.30234\n",
            "Step 25510 : Loss : 1.30190611\n",
            "Step 25520 : Loss : 1.30145371\n",
            "Step 25530 : Loss : 1.30101013\n",
            "Step 25540 : Loss : 1.30055463\n",
            "Step 25550 : Loss : 1.30010164\n",
            "Step 25560 : Loss : 1.29968178\n",
            "Step 25570 : Loss : 1.2992059\n",
            "Step 25580 : Loss : 1.2987361\n",
            "Step 25590 : Loss : 1.29829335\n",
            "Step 25600 : Loss : 1.29783106\n",
            "Step 25610 : Loss : 1.29736269\n",
            "Step 25620 : Loss : 1.29694235\n",
            "Step 25630 : Loss : 1.29650724\n",
            "Step 25640 : Loss : 1.29607058\n",
            "Step 25650 : Loss : 1.29563093\n",
            "Step 25660 : Loss : 1.2951988\n",
            "Step 25670 : Loss : 1.29477882\n",
            "Step 25680 : Loss : 1.29437065\n",
            "Step 25690 : Loss : 1.29394782\n",
            "Step 25700 : Loss : 1.29350662\n",
            "Step 25710 : Loss : 1.29307759\n",
            "Step 25720 : Loss : 1.29264235\n",
            "Step 25730 : Loss : 1.29219937\n",
            "Step 25740 : Loss : 1.29177773\n",
            "Step 25750 : Loss : 1.29132104\n",
            "Step 25760 : Loss : 1.29090917\n",
            "Step 25770 : Loss : 1.29048097\n",
            "Step 25780 : Loss : 1.29004419\n",
            "Step 25790 : Loss : 1.28958523\n",
            "Step 25800 : Loss : 1.28913033\n",
            "Step 25810 : Loss : 1.28869188\n",
            "Step 25820 : Loss : 1.28824115\n",
            "Step 25830 : Loss : 1.28778684\n",
            "Step 25840 : Loss : 1.2873162\n",
            "Step 25850 : Loss : 1.28686976\n",
            "Step 25860 : Loss : 1.28642559\n",
            "Step 25870 : Loss : 1.28596437\n",
            "Step 25880 : Loss : 1.28551126\n",
            "Step 25890 : Loss : 1.28504944\n",
            "Step 25900 : Loss : 1.28457916\n",
            "Step 25910 : Loss : 1.28412926\n",
            "Step 25920 : Loss : 1.28368616\n",
            "Step 25930 : Loss : 1.28323162\n",
            "Step 25940 : Loss : 1.2828083\n",
            "Step 25950 : Loss : 1.28237402\n",
            "Step 25960 : Loss : 1.28193796\n",
            "Step 25970 : Loss : 1.28148162\n",
            "Step 25980 : Loss : 1.28105938\n",
            "Step 25990 : Loss : 1.28062165\n",
            "Step 26000 : Loss : 1.28016484\n",
            "Step 26010 : Loss : 1.27974021\n",
            "Step 26020 : Loss : 1.27932394\n",
            "Step 26030 : Loss : 1.278862\n",
            "Step 26040 : Loss : 1.27843249\n",
            "Step 26050 : Loss : 1.27799428\n",
            "Step 26060 : Loss : 1.27755725\n",
            "Step 26070 : Loss : 1.27711785\n",
            "Step 26080 : Loss : 1.27670074\n",
            "Step 26090 : Loss : 1.27627563\n",
            "Step 26092 : Loss :  1.27618754\n",
            "Epoch 45\n",
            "Step 26100 : Loss : 1.27584827\n",
            "Step 26110 : Loss : 1.27542567\n",
            "Step 26120 : Loss : 1.27500987\n",
            "Step 26130 : Loss : 1.27456856\n",
            "Step 26140 : Loss : 1.27412784\n",
            "Step 26150 : Loss : 1.27371955\n",
            "Step 26160 : Loss : 1.27327538\n",
            "Step 26170 : Loss : 1.27282345\n",
            "Step 26180 : Loss : 1.27237475\n",
            "Step 26190 : Loss : 1.27193904\n",
            "Step 26200 : Loss : 1.27150524\n",
            "Step 26210 : Loss : 1.27107525\n",
            "Step 26220 : Loss : 1.27063179\n",
            "Step 26230 : Loss : 1.27018607\n",
            "Step 26240 : Loss : 1.26975703\n",
            "Step 26250 : Loss : 1.26932263\n",
            "Step 26260 : Loss : 1.26887333\n",
            "Step 26270 : Loss : 1.26843417\n",
            "Step 26280 : Loss : 1.26799643\n",
            "Step 26290 : Loss : 1.26755512\n",
            "Step 26300 : Loss : 1.26711714\n",
            "Step 26310 : Loss : 1.26666307\n",
            "Step 26320 : Loss : 1.26623726\n",
            "Step 26330 : Loss : 1.26578259\n",
            "Step 26340 : Loss : 1.26535153\n",
            "Step 26350 : Loss : 1.26490247\n",
            "Step 26360 : Loss : 1.26446366\n",
            "Step 26370 : Loss : 1.2640152\n",
            "Step 26380 : Loss : 1.263574\n",
            "Step 26390 : Loss : 1.26312518\n",
            "Step 26400 : Loss : 1.26267219\n",
            "Step 26410 : Loss : 1.26223683\n",
            "Step 26420 : Loss : 1.26179385\n",
            "Step 26430 : Loss : 1.2613498\n",
            "Step 26440 : Loss : 1.26091945\n",
            "Step 26450 : Loss : 1.2604723\n",
            "Step 26460 : Loss : 1.2600311\n",
            "Step 26470 : Loss : 1.25959182\n",
            "Step 26480 : Loss : 1.25915086\n",
            "Step 26490 : Loss : 1.25871539\n",
            "Step 26500 : Loss : 1.25827289\n",
            "Step 26510 : Loss : 1.25783074\n",
            "Step 26520 : Loss : 1.25738788\n",
            "Step 26530 : Loss : 1.2569387\n",
            "Step 26540 : Loss : 1.25650966\n",
            "Step 26550 : Loss : 1.25605786\n",
            "Step 26560 : Loss : 1.2556138\n",
            "Step 26570 : Loss : 1.25516665\n",
            "Step 26580 : Loss : 1.2547555\n",
            "Step 26590 : Loss : 1.25431705\n",
            "Step 26600 : Loss : 1.25389683\n",
            "Step 26610 : Loss : 1.25345981\n",
            "Step 26620 : Loss : 1.25303113\n",
            "Step 26630 : Loss : 1.25260544\n",
            "Step 26640 : Loss : 1.25217426\n",
            "Step 26650 : Loss : 1.25173986\n",
            "Step 26660 : Loss : 1.25130701\n",
            "Step 26670 : Loss : 1.25087798\n",
            "Step 26680 : Loss : 1.25045669\n",
            "Step 26685 : Loss :  1.25023675\n",
            "Epoch 46\n",
            "Step 26690 : Loss : 1.25001621\n",
            "Step 26700 : Loss : 1.24958599\n",
            "Step 26710 : Loss : 1.24915516\n",
            "Step 26720 : Loss : 1.24873161\n",
            "Step 26730 : Loss : 1.24829006\n",
            "Step 26740 : Loss : 1.24785447\n",
            "Step 26750 : Loss : 1.2474525\n",
            "Step 26760 : Loss : 1.24702919\n",
            "Step 26770 : Loss : 1.24659\n",
            "Step 26780 : Loss : 1.24617338\n",
            "Step 26790 : Loss : 1.24574196\n",
            "Step 26800 : Loss : 1.2453351\n",
            "Step 26810 : Loss : 1.24490988\n",
            "Step 26820 : Loss : 1.24447632\n",
            "Step 26830 : Loss : 1.24405456\n",
            "Step 26840 : Loss : 1.24362361\n",
            "Step 26850 : Loss : 1.2432102\n",
            "Step 26860 : Loss : 1.24278343\n",
            "Step 26870 : Loss : 1.24236441\n",
            "Step 26880 : Loss : 1.24194\n",
            "Step 26890 : Loss : 1.24154174\n",
            "Step 26900 : Loss : 1.24111116\n",
            "Step 26910 : Loss : 1.24072111\n",
            "Step 26920 : Loss : 1.24030101\n",
            "Step 26930 : Loss : 1.23989642\n",
            "Step 26940 : Loss : 1.23947966\n",
            "Step 26950 : Loss : 1.23907673\n",
            "Step 26960 : Loss : 1.23867214\n",
            "Step 26970 : Loss : 1.23825824\n",
            "Step 26980 : Loss : 1.23785496\n",
            "Step 26990 : Loss : 1.2374444\n",
            "Step 27000 : Loss : 1.23704278\n",
            "Step 27010 : Loss : 1.23663473\n",
            "Step 27020 : Loss : 1.23622191\n",
            "Step 27030 : Loss : 1.23583043\n",
            "Step 27040 : Loss : 1.23540187\n",
            "Step 27050 : Loss : 1.23500264\n",
            "Step 27060 : Loss : 1.23458779\n",
            "Step 27070 : Loss : 1.23419333\n",
            "Step 27080 : Loss : 1.23379648\n",
            "Step 27090 : Loss : 1.23338735\n",
            "Step 27100 : Loss : 1.23300469\n",
            "Step 27110 : Loss : 1.23259282\n",
            "Step 27120 : Loss : 1.23218346\n",
            "Step 27130 : Loss : 1.23178577\n",
            "Step 27140 : Loss : 1.23139\n",
            "Step 27150 : Loss : 1.23097777\n",
            "Step 27160 : Loss : 1.23056066\n",
            "Step 27170 : Loss : 1.23014367\n",
            "Step 27180 : Loss : 1.22974181\n",
            "Step 27190 : Loss : 1.22932422\n",
            "Step 27200 : Loss : 1.2289\n",
            "Step 27210 : Loss : 1.22848988\n",
            "Step 27220 : Loss : 1.22806776\n",
            "Step 27230 : Loss : 1.22766197\n",
            "Step 27240 : Loss : 1.22724521\n",
            "Step 27250 : Loss : 1.22683132\n",
            "Step 27260 : Loss : 1.22640872\n",
            "Step 27270 : Loss : 1.226\n",
            "Step 27278 : Loss :  1.22567165\n",
            "Epoch 47\n",
            "Step 27280 : Loss : 1.22558618\n",
            "Step 27290 : Loss : 1.22520137\n",
            "Step 27300 : Loss : 1.22482467\n",
            "Step 27310 : Loss : 1.22445428\n",
            "Step 27320 : Loss : 1.22407031\n",
            "Step 27330 : Loss : 1.2236563\n",
            "Step 27340 : Loss : 1.22329533\n",
            "Step 27350 : Loss : 1.22291195\n",
            "Step 27360 : Loss : 1.22251844\n",
            "Step 27370 : Loss : 1.22213829\n",
            "Step 27380 : Loss : 1.22177613\n",
            "Step 27390 : Loss : 1.22137928\n",
            "Step 27400 : Loss : 1.22101\n",
            "Step 27410 : Loss : 1.22062504\n",
            "Step 27420 : Loss : 1.2202512\n",
            "Step 27430 : Loss : 1.21986878\n",
            "Step 27440 : Loss : 1.21945763\n",
            "Step 27450 : Loss : 1.21907055\n",
            "Step 27460 : Loss : 1.21869731\n",
            "Step 27470 : Loss : 1.21829343\n",
            "Step 27480 : Loss : 1.21791732\n",
            "Step 27490 : Loss : 1.21753573\n",
            "Step 27500 : Loss : 1.21714222\n",
            "Step 27510 : Loss : 1.21672571\n",
            "Step 27520 : Loss : 1.21631873\n",
            "Step 27530 : Loss : 1.21593642\n",
            "Step 27540 : Loss : 1.2155391\n",
            "Step 27550 : Loss : 1.21515369\n",
            "Step 27560 : Loss : 1.2147485\n",
            "Step 27570 : Loss : 1.21436656\n",
            "Step 27580 : Loss : 1.21399164\n",
            "Step 27590 : Loss : 1.2136091\n",
            "Step 27600 : Loss : 1.21322906\n",
            "Step 27610 : Loss : 1.21284795\n",
            "Step 27620 : Loss : 1.21246946\n",
            "Step 27630 : Loss : 1.21208262\n",
            "Step 27640 : Loss : 1.211707\n",
            "Step 27650 : Loss : 1.2113266\n",
            "Step 27660 : Loss : 1.21095145\n",
            "Step 27670 : Loss : 1.21058202\n",
            "Step 27680 : Loss : 1.21020412\n",
            "Step 27690 : Loss : 1.20981681\n",
            "Step 27700 : Loss : 1.20942092\n",
            "Step 27710 : Loss : 1.20904648\n",
            "Step 27720 : Loss : 1.20866871\n",
            "Step 27730 : Loss : 1.20827711\n",
            "Step 27740 : Loss : 1.2078898\n",
            "Step 27750 : Loss : 1.20750427\n",
            "Step 27760 : Loss : 1.2071116\n",
            "Step 27770 : Loss : 1.20672143\n",
            "Step 27780 : Loss : 1.2063458\n",
            "Step 27790 : Loss : 1.20597088\n",
            "Step 27800 : Loss : 1.20557809\n",
            "Step 27810 : Loss : 1.2051903\n",
            "Step 27820 : Loss : 1.20481372\n",
            "Step 27830 : Loss : 1.20444703\n",
            "Step 27840 : Loss : 1.20405376\n",
            "Step 27850 : Loss : 1.20366454\n",
            "Step 27860 : Loss : 1.20326328\n",
            "Step 27870 : Loss : 1.20287287\n",
            "Step 27871 : Loss :  1.20283067\n",
            "Epoch 48\n",
            "Step 27880 : Loss : 1.20246661\n",
            "Step 27890 : Loss : 1.20209324\n",
            "Step 27900 : Loss : 1.20169568\n",
            "Step 27910 : Loss : 1.20131791\n",
            "Step 27920 : Loss : 1.2009176\n",
            "Step 27930 : Loss : 1.20053637\n",
            "Step 27940 : Loss : 1.20013368\n",
            "Step 27950 : Loss : 1.19974232\n",
            "Step 27960 : Loss : 1.19933748\n",
            "Step 27970 : Loss : 1.19894207\n",
            "Step 27980 : Loss : 1.1985544\n",
            "Step 27990 : Loss : 1.19818783\n",
            "Step 28000 : Loss : 1.19781613\n",
            "Step 28010 : Loss : 1.19744027\n",
            "Step 28020 : Loss : 1.19704485\n",
            "Step 28030 : Loss : 1.19665718\n",
            "Step 28040 : Loss : 1.19625735\n",
            "Step 28050 : Loss : 1.19587743\n",
            "Step 28060 : Loss : 1.19548321\n",
            "Step 28070 : Loss : 1.19509637\n",
            "Step 28080 : Loss : 1.19470155\n",
            "Step 28090 : Loss : 1.19430542\n",
            "Step 28100 : Loss : 1.19390428\n",
            "Step 28110 : Loss : 1.19350421\n",
            "Step 28120 : Loss : 1.19310474\n",
            "Step 28130 : Loss : 1.19274342\n",
            "Step 28140 : Loss : 1.19234729\n",
            "Step 28150 : Loss : 1.19197106\n",
            "Step 28160 : Loss : 1.19157565\n",
            "Step 28170 : Loss : 1.19118452\n",
            "Step 28180 : Loss : 1.19077587\n",
            "Step 28190 : Loss : 1.19038737\n",
            "Step 28200 : Loss : 1.19000065\n",
            "Step 28210 : Loss : 1.18960857\n",
            "Step 28220 : Loss : 1.18921554\n",
            "Step 28230 : Loss : 1.18881464\n",
            "Step 28240 : Loss : 1.18841767\n",
            "Step 28250 : Loss : 1.1880343\n",
            "Step 28260 : Loss : 1.18763697\n",
            "Step 28270 : Loss : 1.18723345\n",
            "Step 28280 : Loss : 1.18683219\n",
            "Step 28290 : Loss : 1.18643332\n",
            "Step 28300 : Loss : 1.18604708\n",
            "Step 28310 : Loss : 1.18567121\n",
            "Step 28320 : Loss : 1.18526757\n",
            "Step 28330 : Loss : 1.18487942\n",
            "Step 28340 : Loss : 1.18449235\n",
            "Step 28350 : Loss : 1.18411124\n",
            "Step 28360 : Loss : 1.18374681\n",
            "Step 28370 : Loss : 1.18336189\n",
            "Step 28380 : Loss : 1.18297875\n",
            "Step 28390 : Loss : 1.18257761\n",
            "Step 28400 : Loss : 1.18217909\n",
            "Step 28410 : Loss : 1.18178976\n",
            "Step 28420 : Loss : 1.18140864\n",
            "Step 28430 : Loss : 1.18103099\n",
            "Step 28440 : Loss : 1.18064308\n",
            "Step 28450 : Loss : 1.18026805\n",
            "Step 28460 : Loss : 1.17989707\n",
            "Step 28464 : Loss :  1.17974234\n",
            "Epoch 49\n",
            "Step 28470 : Loss : 1.17951691\n",
            "Step 28480 : Loss : 1.17913485\n",
            "Step 28490 : Loss : 1.17875433\n",
            "Step 28500 : Loss : 1.17837846\n",
            "Step 28510 : Loss : 1.17799342\n",
            "Step 28520 : Loss : 1.17763531\n",
            "Step 28530 : Loss : 1.17724621\n",
            "Step 28540 : Loss : 1.17686152\n",
            "Step 28550 : Loss : 1.17648578\n",
            "Step 28560 : Loss : 1.17612112\n",
            "Step 28570 : Loss : 1.17577183\n",
            "Step 28580 : Loss : 1.17541027\n",
            "Step 28590 : Loss : 1.17502\n",
            "Step 28600 : Loss : 1.17464256\n",
            "Step 28610 : Loss : 1.17425358\n",
            "Step 28620 : Loss : 1.17386079\n",
            "Step 28630 : Loss : 1.17350233\n",
            "Step 28640 : Loss : 1.17310882\n",
            "Step 28650 : Loss : 1.17274547\n",
            "Step 28660 : Loss : 1.17235649\n",
            "Step 28670 : Loss : 1.17197764\n",
            "Step 28680 : Loss : 1.17160368\n",
            "Step 28690 : Loss : 1.17124057\n",
            "Step 28700 : Loss : 1.17086577\n",
            "Step 28710 : Loss : 1.17048621\n",
            "Step 28720 : Loss : 1.17013252\n",
            "Step 28730 : Loss : 1.16977978\n",
            "Step 28740 : Loss : 1.16941404\n",
            "Step 28750 : Loss : 1.16906404\n",
            "Step 28760 : Loss : 1.16873384\n",
            "Step 28770 : Loss : 1.16837835\n",
            "Step 28780 : Loss : 1.16804814\n",
            "Step 28790 : Loss : 1.16770077\n",
            "Step 28800 : Loss : 1.16735482\n",
            "Step 28810 : Loss : 1.16700149\n",
            "Step 28820 : Loss : 1.16665256\n",
            "Step 28830 : Loss : 1.16629016\n",
            "Step 28840 : Loss : 1.16594172\n",
            "Step 28850 : Loss : 1.1655997\n",
            "Step 28860 : Loss : 1.16525674\n",
            "Step 28870 : Loss : 1.16492426\n",
            "Step 28880 : Loss : 1.16458964\n",
            "Step 28890 : Loss : 1.16424882\n",
            "Step 28900 : Loss : 1.16392303\n",
            "Step 28910 : Loss : 1.16358721\n",
            "Step 28920 : Loss : 1.16323149\n",
            "Step 28930 : Loss : 1.16286969\n",
            "Step 28940 : Loss : 1.16251051\n",
            "Step 28950 : Loss : 1.16214693\n",
            "Step 28960 : Loss : 1.16179228\n",
            "Step 28970 : Loss : 1.1614598\n",
            "Step 28980 : Loss : 1.16111434\n",
            "Step 28990 : Loss : 1.16079104\n",
            "Step 29000 : Loss : 1.16044486\n",
            "Step 29010 : Loss : 1.16009116\n",
            "Step 29020 : Loss : 1.15974355\n",
            "Step 29030 : Loss : 1.15938795\n",
            "Step 29040 : Loss : 1.15903211\n",
            "Step 29050 : Loss : 1.15868628\n",
            "Step 29057 : Loss :  1.15843141\n",
            "Epoch 50\n",
            "Step 29060 : Loss : 1.15832102\n",
            "Step 29070 : Loss : 1.15796733\n",
            "Step 29080 : Loss : 1.15762\n",
            "Step 29090 : Loss : 1.15727019\n",
            "Step 29100 : Loss : 1.15693009\n",
            "Step 29110 : Loss : 1.15657032\n",
            "Step 29120 : Loss : 1.15623689\n",
            "Step 29130 : Loss : 1.15587342\n",
            "Step 29140 : Loss : 1.1555171\n",
            "Step 29150 : Loss : 1.15517044\n",
            "Step 29160 : Loss : 1.15483749\n",
            "Step 29170 : Loss : 1.15451765\n",
            "Step 29180 : Loss : 1.15420353\n",
            "Step 29190 : Loss : 1.15390646\n",
            "Step 29200 : Loss : 1.15356982\n",
            "Step 29210 : Loss : 1.15325248\n",
            "Step 29220 : Loss : 1.15293634\n",
            "Step 29230 : Loss : 1.15260243\n",
            "Step 29240 : Loss : 1.15227711\n",
            "Step 29250 : Loss : 1.15193224\n",
            "Step 29260 : Loss : 1.15158331\n",
            "Step 29270 : Loss : 1.15122306\n",
            "Step 29280 : Loss : 1.15086401\n",
            "Step 29290 : Loss : 1.15051496\n",
            "Step 29300 : Loss : 1.15015984\n",
            "Step 29310 : Loss : 1.14980352\n",
            "Step 29320 : Loss : 1.14945555\n",
            "Step 29330 : Loss : 1.14909899\n",
            "Step 29340 : Loss : 1.14875793\n",
            "Step 29350 : Loss : 1.14841056\n",
            "Step 29360 : Loss : 1.14806628\n",
            "Step 29370 : Loss : 1.14770138\n",
            "Step 29380 : Loss : 1.14735973\n",
            "Step 29390 : Loss : 1.14699125\n",
            "Step 29400 : Loss : 1.14664161\n",
            "Step 29410 : Loss : 1.14628851\n",
            "Step 29420 : Loss : 1.14594543\n",
            "Step 29430 : Loss : 1.14560235\n",
            "Step 29440 : Loss : 1.14526725\n",
            "Step 29450 : Loss : 1.14491737\n",
            "Step 29460 : Loss : 1.14458334\n",
            "Step 29470 : Loss : 1.14422584\n",
            "Step 29480 : Loss : 1.14388156\n",
            "Step 29490 : Loss : 1.14351606\n",
            "Step 29500 : Loss : 1.14316273\n",
            "Step 29510 : Loss : 1.14281476\n",
            "Step 29520 : Loss : 1.14245152\n",
            "Step 29530 : Loss : 1.14209771\n",
            "Step 29540 : Loss : 1.14177608\n",
            "Step 29550 : Loss : 1.14142144\n",
            "Step 29560 : Loss : 1.1410799\n",
            "Step 29570 : Loss : 1.14073122\n",
            "Step 29580 : Loss : 1.14039147\n",
            "Step 29590 : Loss : 1.14004624\n",
            "Step 29600 : Loss : 1.13968635\n",
            "Step 29610 : Loss : 1.13932633\n",
            "Step 29620 : Loss : 1.13897\n",
            "Step 29630 : Loss : 1.13862014\n",
            "Step 29640 : Loss : 1.13826704\n",
            "Step 29650 : Loss : 1.13793397\n",
            "Step 29650 : Loss :  1.13793397\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqo8N4K8x8j_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp bin/model_weights.h5 /content/gdrive/My\\ Drive/AppliedML/model_weights.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOHQsJkhi8Ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}