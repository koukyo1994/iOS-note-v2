\section{Related Work}
\label{section:related-work}

\subsection{Handwriting Recognition Taxonomy}

HWR can be roughly devided into two appproaches:
online approach and offline approach~\cite{plamondon2000online}.
While online approach uses information on the trajectory of the pen tip
obtained from a special pen for classification, offline method uses
optically scanned images as input and performs recognition using computer vision techniques.
In this work, for simplisity of the pipeline, we focuses only on offline approach.

Offline approach of HWR can be positioned as one variant of Scene Text Detection / Recognition, which
is a technology to extract and recognize text information written in natural images.
Due to the recent development of Neural Networks technology,
much research has been done in this field to this day.
Except for few methods\cite{liu2018fots}\cite{lyu2018mask}, most approaches of Scene Text Detection / Recognition
separate text detection and text recognition and perform stepwise inference.

\subsection{Detection}

Scene Text Detection can be subsumed under general object detection, therefore those methods usually follow
the same procedure of object detection, which is dichotomized as one-stage methods and two-stage ones~\cite{liu2018deep}.
After the emergence of FasterR-CNN\cite{ren2015faster}, most of the modern text detection algorithms
are based on FasterR-CNN, YOLO\cite{redmon2016you}, SSD\cite{liu2016ssd}.

In addition to the general object detection model, text detection models are devised to detect
tilted bounding boxes\cite{zhou2017east}\cite{jiang2017r2cnn} and character regions of arbitrary shapes\cite{zhang2019look},
or to simplify the pipeline\cite{he2017single}, since pipeline of text detection tends to be complicated\cite{liu2018deep}.

\subsection{Recognition}

Some text recognition algorithms devide the task into character segmentation and character recognition~\cite{bissacco2013photoocr}\cite{phan2011gradient}.
Character segmentation is considered as the most challenging part of scene text recognition, and may affect
overall accuracy. It is especially difficult to segment connected characters such as cursive.
Therefore some techniques which do not rely on character segmentation have been developped so far.
This report introduces a method called Connectionist Temporal Classification (CTC)~\cite{graves2006connectionist}.

CTC was first proposed to handle sequence labeling of arbitrary length,
requiring no pre-segmented training data. A CTC network outputs probabilities for each label
at each time step. Time step length can be any length longer than label length.
The output at each time step is the probability of the classess to be recognized plus
the extra class representing "blank". Let this output probabilities be
$\mathbf{y}=(y_1, y_2, \cdots, y_w)$ and denote by $y_{k}^{t}$ the activation of
label $k$ at time step $t$. Given this probability distribution, the conditional
probability of the sequence is calculated as follows.

\begin{equation}
    p(\pi |\mathbf{y}) = \prod_{t=1}^{w}y_{\pi_t}^{t}
\end{equation}

Then a many-to-one mapping $\mathcal{B}$ is defined to transform the sequence
$\pi$ to a shorter sequence. The final predicted label is obtained by this mapping.
This mapping removes all blanks and repeated continuous labels from the sequence.
For example, $\mathcal{B}$ maps the predicted sequence "aa-p-pl----ee" to "apple",
where "-" represents the "blank". Since this mapping is many-to-one mapping, different
sequences may be mapped to the same sequence. Therefore the probability of the final
output sequence is the sum of all possible conditional probabilities of all $\pi$ corresponding
to that final sequence.

\begin{equation}
    p(l|\mathbf{y}) = \sum_{\mathbf{\pi}} p(\pi | \mathbf{y})
\end{equation}

where $\mathbf{\pi}$ represents all $\pi$ which produces $l = \mathcal{B}(\pi)$.

The output of the classifier should be the most probable labeling for the input sequence.

\begin{equation}
    h(\mathbf{y}) = \arg\max p(l|\mathbf{y})
\end{equation}

In general, there are a large number of mapping paths for a give sequence, thus
calculation of $\arg\max$ requires heavy computation. In practice, following two approximate
methods are known to give us a good result.

The first method is based on the assumption that the most probable path can be approximated
by the sequence of most probable labeling

\begin{equation}
    h(\mathbf{y})\approx\mathcal{B}(\pi^*)
\end{equation}

where $\pi^{*}$ is a set of labels which get the highest probabilities at each time step.
Although it works well, it is not guaranteed to get the most probable labeling.

The second method is to use forward-backward algorithm to efficiently search for the most
probable sequence. With enough time, this approach can always find the most probable labeling
from the input sequence, but the amount of computation increases exponentially with respect to
the sequence length, it is not practical to find the exact solution.

To train the network with the dataset $\mathcal{D} = \{I_i, l_i\}$, where $I_i$ represents
the input image and $l_i$ represents the corresponding label, maximum likelihood approach
it utilized. The objective function of this can be negative log-likelihood

\begin{equation}
    \mathcal{O} = -\sum_{(I_i, l_i)\in\mathcal{D}} \log p(l_i|\mathbf{y}_i)
    \label{eq:loss}
\end{equation}

where $\mathbf{y}_i = f(I_i)$ and $f(\cdot)$ represents the classifier. To minimize negative
log-likelihood, Stochastic Gradient Descent (SGD) can be used. To summarize,
a model that performs text recognition with CTC can be obtained by defining a network
that outputs a sequence longer than the required label length and
training the network so as to minimize the loss function (\ref{eq:loss}). After the fitting,
model outputs a sequence of probabilities of labels for a given input. Each time step of the sequence
corresponds to, if the input is an image, image patches arranged in the direction of writing.
We then get final prediction by putting the output sequence to a many-to-one mapping.
There are several options for this many-to-one mapping, but the one to get highest probabilities
at each time step and the one which use forward-backward algorithm is considered to achieve
good performance in practice.
